{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/06_BigGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEjGkSNWedet"
   },
   "source": [
    "# BigGAN\n",
    "## 目的\n",
    "高解像度な画像生成ができるBigGANを動かすことによって，最先端のGANの凄さを実感する．\n",
    "\n",
    "## 必要となるモジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EGoVIH5wedeu"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DkkOBvBedew"
   },
   "source": [
    "# ネットワークの構築\n",
    "BigGANは，膨大なマシンリソースをフル活用して高解像度化に成功した，cGANの一種として知られています．\n",
    "BigGANは，Miyatoらが提案したSpectral Normalization GAN (SN-GAN)とZhangらが提案したSelf-Attention GAN (SA-GAN)をベースにネットワークを構成しています．\n",
    "\n",
    "そこで，まずSpectral Normalizationと呼ばれるテクニックの構築をします．\n",
    "Spectral Normalization (SN)は，ネットワーク内のリプシッツ連続を保証するために必要な正規化です．\n",
    "GANの学習安定化及び高解像度化に最適化されたテクニックと捉えることもできます．\n",
    "\n",
    "GANの学習は，単純に2つのネットワークの誤差が競合するように設計されており，プログラム上ではbianry corss entropyなどで実装できる反面で，非常に不安定であることが広く知られています．\n",
    "そのため，SNが提案される以前は，Wasserstein距離をGANの誤差として学習するWasserstein GAN (WGAN)や，WGANをさらに応用したWGAN Gradient Penallty (WGAN-GP)などが頻繁に使用されていました．\n",
    "WGANでは，Discriminatorの重みを一定の範囲に収まるように無理矢理クリップしている（リプシッツ連続を保証するため）ため，非常に強い制約がネットワーク全体に科されていました．\n",
    "この強い制約を打破した手法がWGAN-GPです．\n",
    "WGAN-GPでは，Discriminatorの出力値の勾配のl1ノルムを誤差関数に含めて学習することによってWGANと同様のモチベーションを実現しました．\n",
    "しかし，WGAN-GPでもまだ制約は強く，2回微分を求めることが必要なため，計算コストが嵩んでしまいます．\n",
    "\n",
    "これらを踏まえてMiyatoらは，何かしらの方法によってDiscriminator内部のリプシッツ連続を保証すれば十分であることを発見して，SNが提案されました．\n",
    "SNは，Discriminator内の全ての層に適用することで効果を発揮します．\n",
    "\n",
    "### Spectral Normalizationの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XOPD_Ep_edex"
   },
   "outputs": [],
   "source": [
    "def proj(x, y):\n",
    "    return torch.mm(y, x.t()) * y / torch.mm(y, y.t())\n",
    "\n",
    "def gram_schmidt(x, ys):\n",
    "    for y in ys:\n",
    "        x = x - proj(x, y)\n",
    "    return x\n",
    "\n",
    "def power_iteration(W, u_, update=True, eps=1e-12):\n",
    "    us, vs, svs = [], [], []\n",
    "    for i, u in enumerate(u_):\n",
    "        with torch.no_grad():\n",
    "            v = torch.matmul(u, W)\n",
    "            v = F.normalize(gram_schmidt(v, vs), eps=eps)\n",
    "            vs += [v]\n",
    "            u = torch.matmul(v, W.t())\n",
    "            u = F.normalize(gram_schmidt(u, us), eps=eps)\n",
    "            us += [u]\n",
    "            if update:\n",
    "                u_[i][:] = u\n",
    "        svs += [torch.squeeze(torch.matmul(torch.matmul(v, W.t()), u.t()))]\n",
    "    return svs, us, vs\n",
    "\n",
    "class SpectralNorm(object):\n",
    "    def __init__(self, num_svs, num_itrs, num_outputs, transpose=False, eps=1e-12):\n",
    "        self.num_itrs = num_itrs\n",
    "        self.num_svs = num_svs\n",
    "        self.transpose = transpose\n",
    "        self.eps = eps\n",
    "        for i in range(self.num_svs):\n",
    "            self.register_buffer('u%d' % i, torch.randn(1, num_outputs))\n",
    "            self.register_buffer('sv%d' % i, torch.ones(1))\n",
    "  \n",
    "    @property\n",
    "    def u(self):\n",
    "        return [getattr(self, 'u%d' % i) for i in range(self.num_svs)]\n",
    "\n",
    "    @property\n",
    "    def sv(self):\n",
    "        return [getattr(self, 'sv%d' % i) for i in range(self.num_svs)]\n",
    "   \n",
    "    def W_(self):\n",
    "        W_mat = self.weight.view(self.weight.size(0), -1)\n",
    "        if self.transpose:\n",
    "            W_mat = W_mat.t()\n",
    "        for _ in range(self.num_itrs):\n",
    "            svs, us, vs = power_iteration(W_mat, self.u, update=self.training, eps=self.eps) \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                for i, sv in enumerate(svs):\n",
    "                    self.sv[i][:] = sv     \n",
    "        return self.weight / svs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pejwiD-edez"
   },
   "source": [
    "### Self Attention layerの構築\n",
    "SA-GANは，名前の通りSelf Attentionを取り入れたGANです．\n",
    "GANに初めてAttentionという考えを導入した最も初めの手法です．\n",
    "\n",
    "一般的にGANの学習において，CIFAR-10やCIFAR-10などの自然画像のようにデータ全体を通して姿勢が一定でない（様々な姿勢を取りうる）データの生成は崩れた画像になることが知られています．\n",
    "Generator及びDiscriminatorで，特徴マップのサイズがなるべく大きい時にSelf attentionを施すことによって先に述べた問題を解決します．\n",
    "\n",
    "Self Attentionの作成方法は，とても単純で特徴マップに3種類の畳み込みを施してkey，value，queryとなる特徴マップを作成ます．\n",
    "3つの特徴マップを，self attentionの計算方法に則りattention mapを作成しています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN0Uk2NHkloJ"
   },
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/0s4eo5p84r01yt6/self-attention.png\" width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jzSV9Y5aedez"
   },
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, in_dim, act):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.act = act\n",
    "        \n",
    "        self.query_conv = SpectralNormConv2d(in_dim, in_dim//8, kernel_size=1)\n",
    "        self.key_conv = SpectralNormConv2d(in_dim, in_dim//8, kernel_size=1)\n",
    "        self.value_conv = SpectralNormConv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.gamma = Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, w, h = x.size()\n",
    "        ## (B, Ch, W, H) --> (B, Ch, WxH) --> (B, WxH, Ch)\n",
    "        query = self.query_conv(x).view(b, -1, w*h).permute(0, 2, 1)\n",
    "        ## (B, Ch, W, H) --> (B, Ch, WxH)\n",
    "        key = self.key_conv(x).view(b, -1, w*h)\n",
    "        ## query @ key\n",
    "        energy = torch.bmm(query, key)\n",
    "        attn = self.softmax(energy)\n",
    "        ## (B, Ch, W, H) --> (B, Ch, WxH)\n",
    "        value = self.value_conv(x).view(b, -1, w*h)\n",
    "        \n",
    "        ## value @ attn:(B, WxH, Ch)\n",
    "        out = torch.bmm(value, attn.permute(0, 2, 1))\n",
    "        out = out.view(b, c, w, h)\n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRYSgE8Mede2"
   },
   "source": [
    "### Conditional Batch Normalizationの構築\n",
    "BigGANは，入力した潜在変数と条件からGenerator内の全層のBatch Normalizationのアフィンパラメータを生成します．\n",
    "Pytorchなどで実装されているBatch Normalizationに少し手を加えるだけで実現できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UqOcHpktede2"
   },
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channel, n_condition=148):\n",
    "        super(ConditionalBatchNorm, self).__init__()\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(in_channel, affine=False)\n",
    "\n",
    "        self.embed = nn.Linear(n_condition, in_channel* 2)\n",
    "        self.embed.weight.data[:, :in_channel] = 1\n",
    "        self.embed.weight.data[:, in_channel:] = 0\n",
    "\n",
    "    def forward(self, input, class_id):\n",
    "        out = self.bn(input)\n",
    "        embed = self.embed(class_id)\n",
    "        gamma, beta = embed.chunk(2, 1)\n",
    "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
    "        beta = beta.unsqueeze(2).unsqueeze(3)\n",
    "        return gamma * out + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-daenjKede5"
   },
   "source": [
    "### Cross Replica Batch Normalizationの構築\n",
    "Generatorの最後のBatch Normalizationは，Conditional Batch Normalizationではなく異なるものが使用されています．\n",
    "それがCross Replica Batch Normalizationというものです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d3gj0c1jede5"
   },
   "outputs": [],
   "source": [
    "class _BatchNorm(nn.Module):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-4, momentum=0.1, affine=True,\n",
    "                 track_running_stats=True):\n",
    "        super(_BatchNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        if self.affine:\n",
    "            self.weight = Parameter(torch.Tensor(num_features))\n",
    "            self.bias = Parameter(torch.Tensor(num_features))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var', torch.ones(num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_parameter('running_mean', None)\n",
    "            self.register_parameter('running_var', None)\n",
    "            self.register_parameter('num_batches_tracked', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self):\n",
    "        if self.track_running_stats:\n",
    "            self.running_mean.zero_()\n",
    "            self.running_var.fill_(1)\n",
    "            self.num_batches_tracked.zero_()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            self.weight.data.uniform_()\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "            if self.momentum is None:  # use cumulative moving average\n",
    "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
    "            else:  # use exponential moving average\n",
    "                exponential_average_factor = self.momentum\n",
    "\n",
    "        return F.batch_norm(\n",
    "            input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "            self.training or not self.track_running_stats,\n",
    "            exponential_average_factor, self.eps)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
    "               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        version = metadata.get('version', None)\n",
    "\n",
    "        if (version is None or version < 2) and self.track_running_stats:\n",
    "            # at version 2: added num_batches_tracked buffer\n",
    "            #               this should have a default value of 0\n",
    "            num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "            if num_batches_tracked_key not in state_dict:\n",
    "                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        super(_BatchNorm, self)._load_from_state_dict(\n",
    "            state_dict, prefix, metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "class ScaledCrossReplicaBatchNorm2d(_BatchNorm):\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3FF2cF0ede8"
   },
   "source": [
    "### ネットワーク構築に必要なモジュールの改造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BXO7y3FAede8"
   },
   "outputs": [],
   "source": [
    "class SpectralNormConv2d(nn.Conv2d, SpectralNorm):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "             padding=0, dilation=1, groups=1, bias=True, \n",
    "             num_svs=1, num_itrs=1, eps=1e-12):\n",
    "        nn.Conv2d.__init__(self, in_channels, out_channels, kernel_size, stride, \n",
    "                     padding, dilation, groups, bias)\n",
    "        SpectralNorm.__init__(self, num_svs, num_itrs, out_channels, eps=eps)    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.W_(), self.bias, self.stride, \n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "class SpectralNormLinear(nn.Linear, SpectralNorm):\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 num_svs=1, num_itrs=1, eps=1e-12):\n",
    "        nn.Linear.__init__(self, in_features, out_features, bias)\n",
    "        SpectralNorm.__init__(self, num_svs, num_itrs, out_features, eps=eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.W_(), self.bias)\n",
    "    \n",
    "class SpectralNormEmbedding(nn.Embedding, SpectralNorm):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, \n",
    "                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n",
    "                 sparse=False, _weight=None,\n",
    "                 num_svs=1, num_itrs=1, eps=1e-12):\n",
    "        nn.Embedding.__init__(self, num_embeddings, embedding_dim, padding_idx,\n",
    "                              max_norm, norm_type, scale_grad_by_freq, \n",
    "                              sparse, _weight)\n",
    "        SpectralNorm.__init__(self, num_svs, num_itrs, num_embeddings, eps=eps)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return F.embedding(x, self.W_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FP0FdoHede_"
   },
   "source": [
    "### Generatorの構築\n",
    "BigGANは，Residual Networkをベースにネットワークが構築されています．\n",
    "先ほどまでに定義したレイヤやモジュールを用いてネットワークの構築をします．\n",
    "\n",
    "また，SN-GANやSA-GANでは，SNをDiscriminatorのみ使用してGeneratorは通常と同じように構築されています．\n",
    "しかしながら，BigGANではSNをGeneratorに対しても使用してリプシッツ連続を満たすようにすることで，学習が安定化して高解像度な画像の生成が可能となっています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nRI8ZM1qZtp"
   },
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/gmw05zwla6hoeoo/BigGAN_G.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g9Npk2Tx6TA"
   },
   "source": [
    "図中のSplitは，潜在変数を任意の次元で分割する処理を表しています．Concatは，分割した潜在変数及びクラスラベルを結合する処理を表しています．図中には，単純にConv.と記載しましたが，実際にはSpectral normalizationを施したConvolutionであることに注意してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Z5OBjvqede_"
   },
   "outputs": [],
   "source": [
    "class GResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, \n",
    "                 padding=1, n_cls=None, bn=True, act='relu', up=True):\n",
    "        super(GResBlock, self).__init__()\n",
    "        self.upsample = up\n",
    "        self.bn = bn\n",
    "        \n",
    "        self.c1 = SpectralNormConv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.c2 = SpectralNormConv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.c1x1 = SpectralNormConv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        self.upsample_layer = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.cbn1 = ConditionalBatchNorm(in_channel, 128+20)\n",
    "        self.cbn2 = ConditionalBatchNorm(out_channel, 128+20)\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif act == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif act == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % act\n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        x1 = self.c1x1(self.upsample_layer(x))\n",
    "        \n",
    "        h1 = self.cbn1(x, y)\n",
    "        h1 = self.activation(h1)\n",
    "        h1 = self.upsample_layer(h1)\n",
    "        h1 = self.c1(h1)\n",
    "        \n",
    "        h2 = self.cbn2(h1, y)\n",
    "        h2 = self.activation(h2)\n",
    "        h2 = self.c2(h2)\n",
    "        \n",
    "        return x1 + h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6EkbFiDhedfB"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_latent=120, n_ch=64, n_cls=100, n_block=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_ch = n_ch\n",
    "        self.linear_latent = SpectralNormLinear(20, 4*4*16*n_ch)\n",
    "        self.linear_condition = SpectralNormLinear(n_cls, 128)\n",
    "            \n",
    "        self.blocks = nn.ModuleList([\n",
    "            GResBlock(16*n_ch, 16*n_ch, n_cls=n_cls),\n",
    "            GResBlock(16*n_ch, 8*n_ch, n_cls=n_cls),\n",
    "            GResBlock(8*n_ch, 4*n_ch, n_cls=n_cls),\n",
    "            GResBlock(4*n_ch, 2*n_ch, n_cls=n_cls),\n",
    "            Self_Attention(2*n_ch, nn.Softmax()),\n",
    "            GResBlock(2*n_ch, 1*n_ch, n_cls=n_cls),\n",
    "        ])\n",
    "        \n",
    "        self.bn = ScaledCrossReplicaBatchNorm2d(1*n_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = SpectralNormConv2d(1*n_ch, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.param_count = 0\n",
    "        for module in self.modules():\n",
    "            if (isinstance(module, nn.Conv2d) \n",
    "             or isinstance(module, nn.Linear) \n",
    "             or isinstance(module, nn.Embedding)):\n",
    "                init.orthogonal_(module.weight)\n",
    "                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n",
    "        print('Param count for G''s initialized parameters: %d' % self.param_count)\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        split_latent = torch.split(z, split_size_or_sections=20, dim=1)\n",
    "        proj_cond = self.linear_condition(y)\n",
    "        \n",
    "        z_idx = 0\n",
    "        out = self.linear_latent(split_latent[z_idx])\n",
    "        out = out.view(-1, 16*self.n_ch, 4, 4)\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            if idx == len(self.blocks) - 2:\n",
    "                out = block(out)\n",
    "            else:\n",
    "                z_idx += 1\n",
    "                cond = torch.cat((split_latent[z_idx], proj_cond), dim=1)\n",
    "                out = block(out, cond)\n",
    "        \n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv(out)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD8-89zHedfF"
   },
   "source": [
    "### Discriminatorの構築\n",
    "Discriminatorは，SN-GANやSA-GANと同じように構築します．\n",
    "また，cGANと同じようにDiscriminatorの入力層に直接条件を与えるのではなく，Miyatoらが提案したprojection discriminatorのアイデアを用いてネットワークへ条件を提供します．\n",
    "\n",
    "projection discriminatorは，条件を任意の次元のベクトル（discriminatorの出力値と同じ次元数）へ投影し，discriminatorの出力値に足し合わせるだけの簡単な与え方です．\n",
    "このように与えることによって，条件を考慮した画像生成の促進をすることが可能です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCt9JY0r6XTv"
   },
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/dmy1ve51gup60u8/BigGAN_D.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wVYcLpyDedfF"
   },
   "outputs": [],
   "source": [
    "class DResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, padding=1, act='relu', down=True):\n",
    "        super(DResBlock, self).__init__()\n",
    "        self.c1x1 = SpectralNormConv2d(in_channel, out_channel, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.c1 = SpectralNormConv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        self.c2 = SpectralNormConv2d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif act == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif act == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % act\n",
    "            \n",
    "        self.down = down\n",
    "        if down:\n",
    "            self.avg = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.down:\n",
    "            x1 = self.avg(self.c1x1(x))\n",
    "        else:\n",
    "            x1 = self.c1x1(x)\n",
    "        \n",
    "        h = self.c1(self.activation(x))\n",
    "        h = self.c2(self.activation(h))\n",
    "        \n",
    "        if self.down:\n",
    "            h = self.avg(h)\n",
    "        \n",
    "        return x1 + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Pvo2Ho75edfH"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_ch=64, n_cls=100):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.first_layer = nn.Sequential(\n",
    "            SpectralNormConv2d(3, 1*n_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "            SpectralNormConv2d(1*n_ch, 1*n_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.AvgPool2d(2, 2))\n",
    "        \n",
    "        self.first_skip = SpectralNormConv2d(3, 1*n_ch, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.avgpool = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            DResBlock(1*n_ch, 2*n_ch, down=True),\n",
    "            Self_Attention(2*n_ch, nn.Softmax()),\n",
    "            DResBlock(2*n_ch, 4*n_ch, down=True),\n",
    "            DResBlock(4*n_ch, 8*n_ch, down=True),\n",
    "            DResBlock(8*n_ch, 16*n_ch, down=True),\n",
    "            DResBlock(16*n_ch, 16*n_ch, down=True),\n",
    "            DResBlock(16*n_ch, 16*n_ch, down=False))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear = SpectralNormLinear(16*n_ch, 1)\n",
    "        self.emb_cls = SpectralNormEmbedding(n_cls, 16*n_ch)\n",
    "        #self.embed_cls.weight.data.uniform_(-0.1, 0.1)\n",
    "        #self.emb_cls = spectral_norm(self.embed_cls)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.param_count = 0\n",
    "        for module in self.modules():\n",
    "            if (isinstance(module, nn.Conv2d) \n",
    "             or isinstance(module, nn.Linear) \n",
    "             or isinstance(module, nn.Embedding)):\n",
    "                init.orthogonal_(module.weight)\n",
    "                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n",
    "        print('Param count for D''s initialized parameters: %d' % self.param_count)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = self.first_layer(x)\n",
    "        out_skip = self.avgpool(self.first_skip(x))\n",
    "        out = out + out_skip\n",
    "        \n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            out = block(out)\n",
    "        out = self.relu(out)\n",
    "        out = out.view(out.size(0), out.size(1), -1)\n",
    "        out = out.sum(dim=2)\n",
    "        \n",
    "        out_linear = self.linear(out).squeeze(1)\n",
    "        embed = self.emb_cls(y)\n",
    "        proj = (out * embed).sum(1)\n",
    "        \n",
    "        return out_linear + proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GsRik29edfJ"
   },
   "source": [
    "### BigGANの学習\n",
    "本来であれば，構築したネットワークを任意のデータセットで学習するのですが，Colabolatoryでは，学習が終わらないので事前に学習したネットワークの重みを用いて画像を生成します．\n",
    "\n",
    "今回ダウンロードするモデルは，LSUNというデータセットの中から10クラスのみ用いて学習したものです．\n",
    "画像サイズは128×128です．\n",
    "以下のリンクからpretrainモデルのzipファイルをダウンロードし，解凍をします．\n",
    "中にはGeneratorのパラメータ**gen**と，Discriminatorのパラメータ**dis**が入っています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bGL0sjWaedfK"
   },
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/xanox9atls6kuxa/BigGAN_pretrained_models.zip -O BigGAN_pretrained_models.zip\n",
    "!unzip -q -o BigGAN_pretrained_models.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdFEnz6CedfM"
   },
   "source": [
    "### パラメータの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5kYQA7-VedfM",
    "outputId": "2fb0efcf-9711-4dd6-a1de-4b13c2a3ea6e"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-37ca79d3673b>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-37ca79d3673b>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    G.load_state_dict(torch.load(modelpath))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "modelpath = './BigGAN_pretrained_models/gen'\n",
    "categories = ['airplane', 'bird', 'bottle', 'bus', 'car', \n",
    "              'cat', 'dog', 'horse', 'motorbike', 'sheep']\n",
    "cls_idx = None\n",
    "resample_module = False\n",
    "threshold = 1\n",
    "n_latent = 120\n",
    "n_ch = 64\n",
    "n_cls = len(categories)\n",
    "n_block = 6\n",
    "n_img = 100\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = Generator(n_latent, n_ch, n_cls, n_block).to(device)\n",
    "D = Discriminator(n_channel=64, n_cls\n",
    "G.load_state_dict(torch.load(modelpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAWcUsg1DefL"
   },
   "source": [
    "### BigGANを学習するためのソースコード\n",
    "BigGANは，あまりにもネットワークが精密に作ってあるためColablatry上では到底，学習が終わりません．一応ソースコードを載せておくので，学習リソースを用意できる人は自分のPCへダウンロードして，好きなデータセットを用いて学習してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP-je9K8HWg0"
   },
   "outputs": [],
   "source": [
    "def ortho(model, strength=1e-4, blacklist=[]):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            # Only apply this to parameters with at least 2 axes, and not in the blacklist\n",
    "            if len(param.shape) < 2 or any([param is item for item in blacklist]):\n",
    "                continue\n",
    "            w = param.view(param.shape[0], -1)\n",
    "            grad = (2 * torch.mm(torch.mm(w, w.t()) \n",
    "                    * (1. - torch.eye(w.shape[0], device=w.device)), w))\n",
    "            param.grad.data += strength * grad.view(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCAAakotIryC"
   },
   "source": [
    "BigGANを学習する際の誤差関数は，hinge lossかwgan-gpの誤差関数を利用します．hinge lossの方が安定した学習で高解像な画像が生成される傾向にあるので，そちらを使用することをお勧めします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeC1_rRoDdjD"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "beta1 = 0\n",
    "beta2 = 0.999\n",
    "lr_G, lr_D = 5e-5, 2e-4\n",
    "G_ortho, D_ortho = 1e-4, 0.0\n",
    "lambda_gp = 10\n",
    "loss_func = 'hinge'\n",
    "G_opt = optim.Adam(G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "D_opt = optim.Adam(D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(1, epochs):\n",
    "    print('Training networks for a epoch.')\n",
    "    for idx, (img, tgt) in enumerate(training_dataset):\n",
    "        G.train()\n",
    "        D.train()\n",
    "        real_img = img.cuda()\n",
    "        onehot = torch.eye(conf.n_cls)[tgt].type_as(real_img)\n",
    "        \n",
    "        # ====================== Update Discriminator ======================\n",
    "        D.zero_grad()\n",
    "        dis_real_out = D(real_img, tgt.cuda())\n",
    "        if loss_func == 'wgan-gp':\n",
    "            dis_real = -torch.mean(dis_real_out)\n",
    "        elif loss_func == 'hinge'\n",
    "            dis_real = nn.ReLU()(1.0 - dis_real_out).mean()\n",
    "        \n",
    "        z = torch.randn(conf.batch_size, conf.n_latent).cuda()\n",
    "        fake_img = G(z, onehot)\n",
    "        dis_fake_out = D(fake_img, tgt.cuda())\n",
    "        if loss_func == 'wgan-gp':\n",
    "            dis_fake = torch.mean(dis_fake_out)\n",
    "        elif loss_func == 'hinge':\n",
    "            dis_fake = nn.ReLU()(1.0 + dis_fake_out).mean()\n",
    "        dis_loss = dis_real + dis_fake\n",
    "        dis_loss.backward()\n",
    "        \n",
    "        if D_ortho > 0.0:\n",
    "            ortho(D, D_ortho)\n",
    "            \n",
    "        D_opt.step()\n",
    "        \n",
    "        if loss_func == 'wgan-gp':\n",
    "            eps = torch.rand(real_img.size(0), 1, 1, 1).cuda().expand_as(real_img)\n",
    "            interpolated = torch.autograd.Variable(eps * real_img.data + (1 - eps) * fake_img.data, \n",
    "                                                   requires_grad=True)\n",
    "            out = D(interpolated, tgt.cuda())\n",
    "\n",
    "            grad = torch.autograd.grad(outputs=out,\n",
    "                                       inputs=interpolated,\n",
    "                                       grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                       retain_graph=True,\n",
    "                                       create_graph=True,\n",
    "                                       only_inputs=True)[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "            d_loss = lambda_gp * d_loss_gp\n",
    "        \n",
    "        # ====================== Update Generator ======================\n",
    "        G.zero_grad()\n",
    "        z = torch.randn(conf.batch_size, conf.n_latent).cuda()\n",
    "        fake_img = G(z, onehot)\n",
    "        gen_out = D(fake_img, tgt.cuda())\n",
    "        gen_loss = - gen_out.mean()\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        if G_ortho > 0.0:\n",
    "            ortho(G, G_ortho, blacklist=[param for param in G.shared.parameters()])\n",
    "            \n",
    "        G_opt.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if idx %  100 == 0:\n",
    "            print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss : {:.6f} | G loss: {:.6f} |'\\\n",
    "                  .format(epoch, idx * len(img), len(training_dataset.dataset),\n",
    "                  100. * idx / len(training_dataset), dis_loss.item(), gen_loss.item()))\n",
    "            tb.add_scalars('prediction loss',\n",
    "                           {'D': dis_loss.item(),\n",
    "                            'G': gen_loss.item()},\n",
    "                            iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaazxvJqedfP"
   },
   "source": [
    "### 画像の生成\n",
    "実際に画像を生成してみましょう．\n",
    "\n",
    "生成するクラスは，任意に設定することができます．\n",
    "特に何も指定をしなければランダムに決定したクラスを生成することになります．\n",
    "自分の好きなクラスを生成したいときは，**cls_idx**を適当なintの数字にしてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9DoRiAiedfP"
   },
   "outputs": [],
   "source": [
    "def resampling_module(org_noise, n_batch=30, n_latent=120, th=2):\n",
    "    resample_noise = torch.empty(n_batch, n_latent)\n",
    "    for d1_idx, z in enumerate(org_noise):\n",
    "        for d2_idx, v in enumerate(z):\n",
    "            while True:\n",
    "                if v > - th and v < th:break\n",
    "                v = torch.rand(1)\n",
    "            resample_noise[d1_idx, d2_idx] = v\n",
    "\n",
    "    return resample_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhYdZ5ooedfR",
    "outputId": "ae8269db-5963-41f7-99a1-f194af59f2e7"
   },
   "outputs": [],
   "source": [
    "if cls_idx:\n",
    "    cls_idx = cls_idx\n",
    "else:\n",
    "    cls_idx = np.random.randint(n_cls)\n",
    "    \n",
    "latent = torch.randn(n_img, n_latent).to(device)\n",
    "onehot = torch.eye(n_cls)[cls_idx].unsqueeze(0).expand(n_img, n_cls).to(device)\n",
    "if resample_module:\n",
    "    latent = resampling_module(latent, n_batch=n_img, n_latent=n_latent, th=threhould).to(device)\n",
    "else:\n",
    "    latent = latent\n",
    "\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    imgs = G(latent, onehot)\n",
    "\n",
    "print('Generated class: %s' % categories[cls_idx])\n",
    "\n",
    "row = 10\n",
    "col = 10\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "num = 0\n",
    "while num < row * col:\n",
    "    img = (imgs[num] * 256.).permute(1,2,0).clamp(min=0., max=255.).data.cpu().numpy().astype(np.uint8)\n",
    "    num += 1\n",
    "    plt.subplot(row, col, num)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc-iwdeledfU"
   },
   "source": [
    "## 課題\n",
    "1. resampling_moduleの閾値(th)を変更したらどのようになっていくか確認してみましょう．\n",
    "2. 任意の2つの潜在変数の間を線形補間した場合に，どのような画像が生成されるか確認してみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFxhRdmDsuHj"
   },
   "source": [
    "# 参考文献\n",
    "[1] Andrew Brock, Jeff Donahue and Karen Simonyan, Large Scale GAN Training for High Fidelity Natural Image Synthesis, ICLR, 2019.\\\n",
    "[2] Han Zhang, Ian Goodfellow, Dimitris Metaxas and Augustus Odena, Self-Attention Generative Adversarial Networks, ICML, 2019.\\\n",
    "[3] Takeru Miyato, Toshiki Kataoka, Masanori Koyama and Yuichi Yoshida, Spectral Normalization for Generative Adversarial Networks, ICLR, 2018.\\\n",
    "[4] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin and Aaron Courville, Improved Training of Wasserstein GANs, NIPS, 2017.\\\n",
    "[5] Martin Arjovsky, Soumith Chintala and Léon Bottou, Wasserstein Generative Adversarial Networks, ICML, 2017.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BK0bBgpedfX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "06_BigGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
