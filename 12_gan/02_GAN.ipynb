{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "02_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/02_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMtdu1IbpZL"
      },
      "source": [
        "# Generative Adversarial Networks (GAN)\n",
        "## 目的\n",
        "GANによって画像の生成をして動作を理解する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-n2P--PtOHr"
      },
      "source": [
        "## 必要なモジュールのインポート\n",
        "Pytorchで学習するときに必要となるモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOIgRbOGbpZM"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbo56TMkbpZP"
      },
      "source": [
        "## ネットワークの構築\n",
        "GANは，Generator (生成器)とDiscriminator (識別器)と呼ばれる２つのネットワークによって構成されている生成モデルです．\n",
        "そのため，2つのネットワークを別々に構築します． <br>\n",
        "Generatorは，$N(0, 1)$や$U[-1, 1]$からサンプリングした潜在変数を入力して画像を生成するネットワークです．Generatorは，綺麗な画像を生成することでDiscriminatorを欺くことを目的としています．<br>\n",
        "一方で，Discriminatorは実画像 (訓練画像)またはGeneratorが生成した画像のどちらかを入力して，入力されたデータを正確に判別するネットワークです．Discriminatorは，入力画像の中からGeneratorの生成した画像を見破ることを目的としています．<br>\n",
        "GANの学習は，一般的な事象に置き換えて，しばしば，「偽札製造者と警察官」を例に出して説明することがあります．この場合Generatorが偽札製造者でDiscriminatorが警察官です．<br>\n",
        "GANのネットワークを簡易的に表現したものを，以下に示します．<br>\n",
        "<img src=\"https://dl.dropboxusercontent.com/s/deek34es6dqu4lb/gan.png\" width=50%>\n",
        "\n",
        "GANの最終的な目的は，実画像$x$の確率分布$p_{data}(x)$と実画像$x$をGenerator上の分布で見た時の確率分布$p_{g}(x)$が一致することです．つまり，$p_{data}(x)=p_{g}(x)$が成立した時にDiscriminatorが完全にRealなのかFakeなのかわからなくなっていると言えます．<br>\n",
        "Generator及びDiscriminatorの構造は非常にシンプルで，全結合とReLUによって構築します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQrlp9UbpZP"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 784))\n",
        "        \n",
        "    def forward(self, z):\n",
        "        return self.layer(z)\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRtlFoWxbpZR"
      },
      "source": [
        "## データセットと最適化関数\n",
        "データセットにはMNISTを使用します．\n",
        "最適化関数はAdam optimizer使用し，学習率$2\\times 10^4$，betaの値を$0.5, 0.999$として学習します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muwbLKFzbpZS"
      },
      "source": [
        "mnist_data = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "training_data = DataLoader(mnist_data, batch_size=100, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "latent_dim = 100\n",
        "G = Generator(latent_dim=latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "opt_g = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "opt_d = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuiMK-hUbpZU"
      },
      "source": [
        "## GANの学習\n",
        "ここでは，Generator及びDiscriminatorを用いてGANの学習をします．<br>\n",
        "GANの最適化式は以下に示すとおりです．\n",
        "$$\n",
        "\\min_{G}\\max_{D}V(D, G) = \\mathbb{E}_{x\\sim P_{data}(x)}\\left[\\log\\left(D(x)\\right)\\right] + \\mathbb{E}_{z\\sim P(z)}\\left[\\log\\left(1 - D(\\hat{x})\\right)\\right]\n",
        "$$\n",
        "ここで，$x$は実画像で$\\hat{x}$がGeneratorの生成した画像G(z)に対応します．<br>\n",
        "GANを学習する際は，binary cross entopyを用いて，実画像は1に，生成画像は0に近似するように学習をします．\n",
        "Discriminatorは，実画像は1生成画像は0と識別するとように学習をしますが，Generatorは生成した画像を実画像であるとDiscriminatorに誤識別をさせたいので，1と識別されるように学習をします．\n",
        "\n",
        "これによりGANの醍醐味である敵対学習を完成させることができます．<br>\n",
        "ここで，n_epochは学習回数です．n_criticはdiscriminatorを1 iterationで何回更新するかの数となっています．<br>\n",
        "Discriminatorを複数回更新した後にGeneratorを1回更新する理由は，モード崩壊を防止するためです．モード崩壊とは，GANの学習では深刻な問題で，Generatorがある一定の画像しか生成できなることや全く画像が生成できなくなることを指します．<br>\n",
        "Discriminatorは，おバカすぎてもダメで，賢すぎてもダメなのでいい塩梅をn_criticで指定しましょう．n_criticが多いと簡単にモード崩壊します．　（理想は2回ぐらいだと思います．）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fan3aF6zbpZU",
        "outputId": "6dc5152c-29d9-467c-d6b2-68fe8cbe306f"
      },
      "source": [
        "n_epoch = 10\n",
        "n_critic = 2\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "    for idx, (real_x, _) in enumerate(training_data):\n",
        "        real_x = real_x.cuda()\n",
        "        batch = real_x.size(0)\n",
        "        flag_real = Tensor(batch, 1).fill_(1.0)\n",
        "        flag_fake = Tensor(batch, 1).fill_(0.0)\n",
        "        \n",
        "        for _ in range(n_critic):\n",
        "            D.zero_grad()\n",
        "            z = torch.randn(batch, latent_dim).to(device)\n",
        "            fake_x = G(z)\n",
        "            out_real = D(real_x.view(batch, -1))\n",
        "            out_fake = D(fake_x.detach().view(batch, -1))\n",
        "            loss_real = criterion(out_real, flag_real)\n",
        "            loss_fake = criterion(out_fake, flag_fake)\n",
        "            dis_loss = loss_real + loss_fake\n",
        "            dis_loss.backward()\n",
        "            opt_d.step()\n",
        "            \n",
        "        G.zero_grad()\n",
        "        z = torch.randn(batch, latent_dim).to(device)\n",
        "        fake_x = G(z)\n",
        "        out_gen = D(fake_x)\n",
        "        gen_loss = criterion(out_gen, flag_real)\n",
        "        gen_loss.backward()\n",
        "        opt_g.step()\n",
        "        \n",
        "        if idx % 100 == 0:\n",
        "            print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss: {:.6f} | G loss: {:.6f} |'\\\n",
        "                  .format(epoch, idx * len(real_x), len(training_data.dataset),\n",
        "                  100. * idx / len(training_data), dis_loss.item(), gen_loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 1 [0/60000 (0%)] | D loss: 0.139158 | G loss: 6.747652 |\n",
            "Training epoch: 1 [10000/60000 (17%)] | D loss: 0.046877 | G loss: 5.843255 |\n",
            "Training epoch: 1 [20000/60000 (33%)] | D loss: 0.225689 | G loss: 5.125803 |\n",
            "Training epoch: 1 [30000/60000 (50%)] | D loss: 0.297474 | G loss: 4.759229 |\n",
            "Training epoch: 1 [40000/60000 (67%)] | D loss: 0.135520 | G loss: 4.716477 |\n",
            "Training epoch: 1 [50000/60000 (83%)] | D loss: 0.241813 | G loss: 4.291683 |\n",
            "Training epoch: 2 [0/60000 (0%)] | D loss: 0.132298 | G loss: 4.690893 |\n",
            "Training epoch: 2 [10000/60000 (17%)] | D loss: 0.175080 | G loss: 3.642710 |\n",
            "Training epoch: 2 [20000/60000 (33%)] | D loss: 0.233343 | G loss: 4.049376 |\n",
            "Training epoch: 2 [30000/60000 (50%)] | D loss: 0.401531 | G loss: 3.880386 |\n",
            "Training epoch: 2 [40000/60000 (67%)] | D loss: 0.360896 | G loss: 3.374158 |\n",
            "Training epoch: 2 [50000/60000 (83%)] | D loss: 0.315911 | G loss: 4.439030 |\n",
            "Training epoch: 3 [0/60000 (0%)] | D loss: 0.408877 | G loss: 3.867288 |\n",
            "Training epoch: 3 [10000/60000 (17%)] | D loss: 0.368686 | G loss: 4.389357 |\n",
            "Training epoch: 3 [20000/60000 (33%)] | D loss: 0.367120 | G loss: 3.418237 |\n",
            "Training epoch: 3 [30000/60000 (50%)] | D loss: 0.452225 | G loss: 3.656815 |\n",
            "Training epoch: 3 [40000/60000 (67%)] | D loss: 0.479825 | G loss: 3.836434 |\n",
            "Training epoch: 3 [50000/60000 (83%)] | D loss: 0.502892 | G loss: 3.162023 |\n",
            "Training epoch: 4 [0/60000 (0%)] | D loss: 0.365545 | G loss: 3.666163 |\n",
            "Training epoch: 4 [10000/60000 (17%)] | D loss: 0.512023 | G loss: 3.610920 |\n",
            "Training epoch: 4 [20000/60000 (33%)] | D loss: 0.319011 | G loss: 3.539586 |\n",
            "Training epoch: 4 [30000/60000 (50%)] | D loss: 0.377729 | G loss: 3.619189 |\n",
            "Training epoch: 4 [40000/60000 (67%)] | D loss: 0.497894 | G loss: 3.391917 |\n",
            "Training epoch: 4 [50000/60000 (83%)] | D loss: 0.499292 | G loss: 2.763221 |\n",
            "Training epoch: 5 [0/60000 (0%)] | D loss: 0.549643 | G loss: 3.398707 |\n",
            "Training epoch: 5 [10000/60000 (17%)] | D loss: 0.503225 | G loss: 3.191890 |\n",
            "Training epoch: 5 [20000/60000 (33%)] | D loss: 0.498607 | G loss: 3.515576 |\n",
            "Training epoch: 5 [30000/60000 (50%)] | D loss: 0.410080 | G loss: 3.638770 |\n",
            "Training epoch: 5 [40000/60000 (67%)] | D loss: 0.401185 | G loss: 3.645909 |\n",
            "Training epoch: 5 [50000/60000 (83%)] | D loss: 0.418163 | G loss: 3.784169 |\n",
            "Training epoch: 6 [0/60000 (0%)] | D loss: 0.339724 | G loss: 3.728620 |\n",
            "Training epoch: 6 [10000/60000 (17%)] | D loss: 0.594614 | G loss: 3.882597 |\n",
            "Training epoch: 6 [20000/60000 (33%)] | D loss: 0.445549 | G loss: 3.257845 |\n",
            "Training epoch: 6 [30000/60000 (50%)] | D loss: 0.421676 | G loss: 3.555411 |\n",
            "Training epoch: 6 [40000/60000 (67%)] | D loss: 0.380448 | G loss: 3.630557 |\n",
            "Training epoch: 6 [50000/60000 (83%)] | D loss: 0.586770 | G loss: 3.780826 |\n",
            "Training epoch: 7 [0/60000 (0%)] | D loss: 0.377701 | G loss: 3.514519 |\n",
            "Training epoch: 7 [10000/60000 (17%)] | D loss: 0.575689 | G loss: 3.167253 |\n",
            "Training epoch: 7 [20000/60000 (33%)] | D loss: 0.346953 | G loss: 3.811909 |\n",
            "Training epoch: 7 [30000/60000 (50%)] | D loss: 0.489560 | G loss: 2.962102 |\n",
            "Training epoch: 7 [40000/60000 (67%)] | D loss: 0.356484 | G loss: 4.374479 |\n",
            "Training epoch: 7 [50000/60000 (83%)] | D loss: 0.303833 | G loss: 3.575810 |\n",
            "Training epoch: 8 [0/60000 (0%)] | D loss: 0.378757 | G loss: 3.488744 |\n",
            "Training epoch: 8 [10000/60000 (17%)] | D loss: 0.283000 | G loss: 3.611646 |\n",
            "Training epoch: 8 [20000/60000 (33%)] | D loss: 0.342287 | G loss: 3.621671 |\n",
            "Training epoch: 8 [30000/60000 (50%)] | D loss: 0.398181 | G loss: 3.167782 |\n",
            "Training epoch: 8 [40000/60000 (67%)] | D loss: 0.415795 | G loss: 3.183191 |\n",
            "Training epoch: 8 [50000/60000 (83%)] | D loss: 0.518203 | G loss: 3.222573 |\n",
            "Training epoch: 9 [0/60000 (0%)] | D loss: 0.402239 | G loss: 3.409310 |\n",
            "Training epoch: 9 [10000/60000 (17%)] | D loss: 0.481363 | G loss: 3.450401 |\n",
            "Training epoch: 9 [20000/60000 (33%)] | D loss: 0.276712 | G loss: 3.440969 |\n",
            "Training epoch: 9 [30000/60000 (50%)] | D loss: 0.425895 | G loss: 3.574426 |\n",
            "Training epoch: 9 [40000/60000 (67%)] | D loss: 0.354665 | G loss: 4.050847 |\n",
            "Training epoch: 9 [50000/60000 (83%)] | D loss: 0.405068 | G loss: 3.477289 |\n",
            "Training epoch: 10 [0/60000 (0%)] | D loss: 0.410332 | G loss: 3.267435 |\n",
            "Training epoch: 10 [10000/60000 (17%)] | D loss: 0.554152 | G loss: 3.324025 |\n",
            "Training epoch: 10 [20000/60000 (33%)] | D loss: 0.313368 | G loss: 3.615428 |\n",
            "Training epoch: 10 [30000/60000 (50%)] | D loss: 0.489414 | G loss: 3.501924 |\n",
            "Training epoch: 10 [40000/60000 (67%)] | D loss: 0.309643 | G loss: 3.370232 |\n",
            "Training epoch: 10 [50000/60000 (83%)] | D loss: 0.415101 | G loss: 3.418372 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvfYDd_0H-lT"
      },
      "source": [
        "## 学習したGeneratorによる画像生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5WW2f9HbpZX",
        "outputId": "bd4ac73c-581d-4489-c570-c64721c6b9c4"
      },
      "source": [
        "z = torch.randn(100, 100).to(device)\n",
        "test_img = G(z)\n",
        "test_img_array = (test_img * 256.).clamp(min=0., max=255.).data.cpu().numpy().reshape(28, 28)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i, im in enumerate(test_img_array):\n",
        "    ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(im, 'gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACXklEQVR4nDWSTUjTAQDFf9t/OvW/OW3pnNsKEzfXEt1a20r7RJZW06QPC5U69CVFt4gOXYo+iA5ZRCGBh6APqQyqU3Rwp4KCKELo42SGJVYaWFT4Orje9fEe7z0eSQJAhx9rAgulACnAEbE0wMVOkpvDVK5oBWL4oh58MA98doACA8CZk9RYARxQiwvDXkaOrAZq1gb8OOEw5Ukw4lSBWYEJdJXcvn90ewondioAWFODudrBgRQXpanr73Q8Fr/QFrDZ2y1Q8t01Vfpt6YvNZ0PkcHMoz/XrQxYfwW1+wO3v+SNJ2vZVGnuwHtyQTMPirTspYmRIkk61rdWsZhy2ZdiAJIA9nzU3sjof37OJpRq+M1gIWEk8I83ezG/Tke7rebIh9JrnuLzZnwBYdwFgA86pP3P1lqTpRTYiIbARq/MUA+wlPqynWUkP0mfopQlcQMCNG+Ca3t6X9HD/krZuCjBYR2U91AOgQWlmuP+yBdw4TwF5xQY0U9asoX1H9FQ6/X+KALAHqGse19++LR81ozFoDUTNuYiYRUBakt5Ko1IvQEkTQQtA2EHHG2kZXH40IfWY+VVhJ+uC7XPuuwefDdiB45mD+pTZYI1hj0ERLaQWrbo0PVkGJ+9e6J3UgLeQCqCFKHjxLPj2QZFMQqMalyiHOnJfiQJds9J7SdK9/eUhduCvztXpXknwy5UjxyRNngiHDJYDFAIWAKLMf0z8c2zMHCbx6heERzZO5z8BGgIEW8vAA1AxtzI0AlZ4OWr7sXAKDnUC4415RPDAhLHci0EjALUxav1Eczm6MJ3wDzOezl/CzGMdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F9B302FEC50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWzsGnZxJYkg"
      },
      "source": [
        "# 課題\n",
        "1. 学習を増やすことで本物らしい画像になるか確認してみましょう．\n",
        "2.   潜在変数の次元数を100次元から減らした場合どうなるでしょうか．また，増やした場合はどのようになるでしょうか．\n",
        "3.   意図的にモード崩壊させてみてください．どのような画像が生成されるか確認しましょう．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3jn_GNko7JP"
      },
      "source": [
        "# 参考文献\n",
        "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio, Generative adversarial nets, NIPS, 2014."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80b8a5b_bpZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}