{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/05_CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r9qjTJxb6st"
   },
   "source": [
    "# CycleGAN\n",
    "## 目的\n",
    "Cycle GANのネットワークを構築して動作を理解する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPGo3zbkSCyN"
   },
   "source": [
    "# データセット\n",
    "りんごとオレンジが含まれるデータセットのzipファイルをダウンロードして，解凍します．\n",
    "もし違うデータを使いたい場合は，以下のURLから選択してURL先のデータをwgetで取得してください．<br>\n",
    "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MliglJq9Qg4k"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip -O apple2orange.zip\n",
    "# !unzip -q -o apple2orange.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGjewjTHTbch"
   },
   "source": [
    "# 各種モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ht4YxManb6sx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzVLYWOqTfki"
   },
   "source": [
    "# ネットワークの構築\n",
    "CycleGANは，「画像を入力→異なるスタイルへ変換→変換したものを元のスタイルへ戻す」という処理によって，対になる画像がない場合であってもスタイルを変換できる代物です．<br>\n",
    "以下に示す図は，CycleGANによる変換及び再構成を表しています．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGtp4p_FRVKL"
   },
   "source": [
    "<div style = \"background-color:#ffffff\">\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/7339ohcadohs6o7/Cyclegan.png\" width=40%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDHDDjFLRY_g"
   },
   "source": [
    "CycleGANが提案される前は，pix2pixと呼ばれるペア画像を必要とするスタイル変換が主流でした．\n",
    "ペア画像とは，例えばRGB画像とその画像に対するセグメンテーション画像のことを指します．<br>\n",
    "\n",
    "CycleGANのGeneratorはResidual networkをベースに設計します．\n",
    "Residual network (ResNet)は，残差機構を用いることで多層になった場合でも特徴を残すことが可能なネットワークです．\n",
    "ResBlockは，カーネルサイズ3×3，ストライド1，パディング1の2層の畳み込み層で構築します．\n",
    "以下にResBlockの図を示します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1M94GqluOIP"
   },
   "source": [
    "<div style = \"background-color:#ffffff\">\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/pow1wsxhc37gmts/ResBlock.png\" width=50%>\n",
    "</div>\n",
    "    \n",
    "ここで$f(x)$は，前層が出力した特徴マップを表しています．\n",
    "また$g(x)$は，残差を加算した特徴マップです．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_q0pI8tzb6sz"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResBlock, self).__init__()\n",
    "        block_list = []\n",
    "        self.block = self.make_block(block_list, features)\n",
    "        \n",
    "    def make_block(self, modules_list, features):\n",
    "        modules_list.append(nn.ReflectionPad2d(1))\n",
    "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
    "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
    "        modules_list.append(nn.ReLU(inplace=True))\n",
    "        modules_list.append(nn.ReflectionPad2d(1))\n",
    "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
    "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
    "        modules = nn.Sequential(*modules_list)\n",
    "        return modules\n",
    "        \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwdCasq1N7cb"
   },
   "source": [
    "CycleGANのGeneratorは，入力に潜在変数ではなく変換元となる画像を入力して，画像を出力します．そのため，GeneratorはEncoder-Decoder構造をしています．Encoderは，入力画像を表現するために有益な情報を残した特徴抽出をします．一方で，DecoderはEncoderがエンコードした特徴を用いて画像の変換をします．このときにEncoderとDecoderの間には，任意の数積み上げたResBlockを使用することで，さらに特徴を凝縮します．\n",
    "ここで，ResBlockは，残差を用いてネットワークを構築するResidual Networksで使用されているものです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ9N0ENevqmS"
   },
   "source": [
    "<div style = \"background-color:#ffffff\">\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/yha9im0bbb6p0t1/CycleGAN_G.png\" width=50%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "myUSGWPzb6s1"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_down, n_up, n_res, in_features):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        out_features = 64\n",
    "        first_conv = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            self.select_normalization(norm='instance', features=out_features),\n",
    "            nn.ReLU(inplace=True)]\n",
    "        \n",
    "        down_block = []\n",
    "        for _ in range(n_down):\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "            down_block += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                self.select_normalization(norm='instance', features=out_features),\n",
    "                nn.ReLU(inplace=True)]\n",
    "            \n",
    "        res_block = []\n",
    "        res_features = out_features\n",
    "        for _ in range(n_res):\n",
    "            res_block.append(ResBlock(res_features))\n",
    "            \n",
    "        up_block = []\n",
    "        in_features = res_features\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(n_up):\n",
    "            up_block += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "                self.select_normalization(norm='instance', features=out_features),\n",
    "                nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        last_conv = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_features, 3, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            nn.Tanh()]\n",
    "        \n",
    "        self.first_conv = nn.Sequential(*first_conv)\n",
    "        self.down_block = nn.Sequential(*down_block)\n",
    "        self.res_block = nn.Sequential(*res_block)\n",
    "        self.up_block = nn.Sequential(*up_block)\n",
    "        self.last_conv = nn.Sequential(*last_conv)\n",
    "        self.init_weights(self.first_conv)\n",
    "        self.init_weights(self.down_block)\n",
    "        self.init_weights(self.res_block)\n",
    "        self.init_weights(self.up_block)\n",
    "        self.init_weights(self.last_conv)\n",
    "\n",
    "    def init_weights(self, net):\n",
    "        classname = net.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
    "            if hasattr(net, 'bias') and net.bias is not None:\n",
    "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
    "    \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = self.first_conv(x)\n",
    "        h = self.down_block(h)\n",
    "        h = self.res_block(h)\n",
    "        h = self.up_block(h)\n",
    "        out = self.last_conv(h)   \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qta1yAAWCRoc"
   },
   "source": [
    "\n",
    "Discriminatorは，実画像であるかGeneratorによって変換された画像であるかの分類をするため，通常のGANと同じ振る舞いです．\n",
    "DC-GANは，画像1枚を入力して実画像であるか生成画像であるかを判定しています．\n",
    "つまり，Dicriminatorは画像全体を見て判断していると言っても良いでしょう．<br>\n",
    "ところが，CycleGANはPatchGANをベースにDiscriminatorを構築しているので，少し変わった方法で実画像か生成画像かの判定をしています．\n",
    "PatchGANは，画像全体ではなく画像をいくつかのパッチに区切って入力することで，局所領域ごとに実画像なのか生成画像なのかを判定します．\n",
    "これによって，画像全体で判定するよりも性能がよくなることが知られています．\n",
    "以下に通常のDiscriminatorとCycleGANのDsicriminatorを示します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQE5nsqA13fI"
   },
   "source": [
    "<div style=background-color:#FFFFFF>\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/yrud8444g26g0yz/CycleGAN_D.png\" width=50%>\n",
    "<div background-color:#99cc00></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgE-BTCT2CnG"
   },
   "source": [
    "図にも示したように，厳密には任意の数のPatchに分割することは手間なので，Discriminator内部の畳み込み処理のカーネルサイズによって受容野の広さを制御して，出力値をスカラー値ではなく特徴マップとすることによってPatchに分割したときと同様の処理を実現しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8M0h0cZQb6s3"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_layers=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        out_features = 64\n",
    "        modules = [nn.Conv2d(3, out_features, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                   nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "            if i == n_layers - 1:    stride=1\n",
    "            else:    stride=2\n",
    "            modules += [nn.Conv2d(in_features, out_features, kernel_size=4, stride=stride, padding=1, bias=True),\n",
    "                        self.select_normalization(norm='instance', features=out_features),\n",
    "                        nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
    "        \n",
    "        modules += [nn.Conv2d(out_features, 1, kernel_size=4, stride=1, padding=1, bias=True)]\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        self.init_weights(self.layers)\n",
    "\n",
    "    def init_weights(self, net):\n",
    "        classname = net.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
    "            if hasattr(net, 'bias') and net.bias is not None:\n",
    "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
    "    \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EZI5T5j-9Mn"
   },
   "source": [
    "# DataLoaderの定義\n",
    "pytorchに含まれているデータ以外のデータセットを用いて学習するときや特殊なデータ構造を利用して学習をする時は，DataLoaderを自分で定義する必要があります．今回使用するデータセットは，zipファイルを解凍すると，内部にtrainA，trainB，testA，testBという名前のディレクトリが含まれています．\n",
    "また，それぞれのディレクトリの中に画像が入っています．\n",
    "\n",
    "これらを考慮して以下に示すようにDataLoaderを定義します．\n",
    "\n",
    "datapath：zipファイルを解凍したときにできるディレクトリのパス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "42LbNtoTb6s5"
   },
   "outputs": [],
   "source": [
    "class CycleGAN_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.A_path = os.path.join(datapath, 'trainA')\n",
    "        self.B_path = os.path.join(datapath, 'trainB')\n",
    "        dataA_list = os.listdir(self.A_path)\n",
    "        dataB_list = os.listdir(self.B_path)\n",
    "        random.shuffle(dataA_list)\n",
    "        random.shuffle(dataB_list)\n",
    "        self.datalength = min(len(dataA_list), len(dataB_list))\n",
    "        self.dataA = dataA_list[:self.datalength]\n",
    "        self.dataB = dataB_list[:self.datalength]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.datalength\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        imgA = Image.open(os.path.join(self.A_path, self.dataA[i]))\n",
    "        imgB = Image.open(os.path.join(self.B_path, self.dataB[i]))\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgA = self.transforms(imgA)\n",
    "            imgB = self.transforms(imgB)\n",
    "        \n",
    "        return imgA, imgB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqq6NIWqBHsX"
   },
   "source": [
    "# Image History Buffer\n",
    "CycleGANでは，任意のサイズのBufferを定義します．\n",
    "これにより，過去にDiscriminatorをうまく騙せた画像に依存して変換のバリエーションが少なくなる問題を回避することができます．\n",
    "\n",
    "Image Bufferは，Generatorが変換した画像を溜め込みますが，任意のiteration数で中身の画像を最新の画像にアップデートします．このときに，全ての画像を破棄して新しいものへアップデートするのではなく，一部のみをアップデートします．つまり，異なるiteration数で変換した画像が混在したBufferとなっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EPOcEIJLb6s7"
   },
   "outputs": [],
   "source": [
    "class Image_History_Buffer:\n",
    "    def __init__(self, pool_size=50):\n",
    "        self.pool_size = pool_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def get_images(self,pre_images):\n",
    "        return_imgs = []\n",
    "        for img in pre_images:\n",
    "            img = torch.unsqueeze(img,0)\n",
    "            if len(self.buffer) < self.pool_size:\n",
    "                self.buffer.append(img)\n",
    "                return_imgs.append(img)\n",
    "            else:\n",
    "                if random.randint(0,1)>0.5:\n",
    "                    i = random.randint(0,self.pool_size-1)\n",
    "                    tmp = self.buffer[i].clone()\n",
    "                    self.buffer[i]=img\n",
    "                    return_imgs.append(tmp)\n",
    "                else:\n",
    "                    return_imgs.append(img)\n",
    "        return torch.cat(return_imgs,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "laJRoFEWb6s9"
   },
   "outputs": [],
   "source": [
    "class loss_scheduler():\n",
    "    def __init__(self, epoch_decay):\n",
    "        self.epoch_decay = epoch_decay\n",
    "\n",
    "    def f(self, epoch):\n",
    "        if epoch<=self.epoch_decay:\n",
    "            return 1\n",
    "        else:\n",
    "            scaling = 1 - (epoch-self.epoch_decay)/float(self.epoch_decay)\n",
    "            return scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Div2ag6vPtnr"
   },
   "source": [
    "# ネットワークの作成，学習に必要なパラメータの定義\n",
    "CycleGANは，ドメインAからBに変換するGeneratorとその逆の処理をするGeneratorの2つ作成する必要があります．また，Discriminatorも同様で，ドメインAのためのDiscriminator，ドメインBのためのDicriminatorも作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-mpFvV3_b6s_"
   },
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "img_size = 256\n",
    "betas = (0.5, 0.999)\n",
    "batchsize = 1\n",
    "imgsize = 256\n",
    "n_epochs = 200\n",
    "decay_epoch = 100\n",
    "lambda_val = 10\n",
    "lambda_id_val = 0\n",
    "datapath = 'apple2orange'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make training dataset\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "transform = transforms.Compose([transforms.Resize(img_size, Image.BICUBIC),\n",
    "                                   transforms.RandomCrop(imgsize),\n",
    "                                   transforms.RandomHorizontalFlip(),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean, std)])\n",
    "train_data = CycleGAN_Dataset(datapath=datapath, transforms=transform)\n",
    "training_dataset = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "# Define networks\n",
    "G_A2B = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
    "G_B2A = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
    "D_A = Discriminator(n_layers=3).to(device)\n",
    "D_B = Discriminator(n_layers=3).to(device)\n",
    "\n",
    "g_opt = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=betas)\n",
    "d_A_opt = optim.Adam(D_A.parameters(), lr=lr, betas=betas)\n",
    "d_B_opt = optim.Adam(D_B.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "g_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(g_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "d_a_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_A_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "d_b_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_B_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "\n",
    "adv_loss = nn.MSELoss()\n",
    "l1_norm = nn.L1Loss()\n",
    "criterion_idn = nn.L1Loss()\n",
    "\n",
    "buffer_for_fakeA = Image_History_Buffer()\n",
    "buffer_for_fakeB = Image_History_Buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5fMDxHRfY_R"
   },
   "source": [
    "# CycleGANの学習\n",
    "これまでに定義したネットワークを用いて学習します．\n",
    "\n",
    "CycleGANの誤差関数は，オリジナルのGANとは異なるものが使用されています．\n",
    "* Real/Fakeの誤差関数：GANを学習するために必要な誤差関数です．LS-GANで提案されているカイ2乗誤差を誤差関数とする方法を利用しています．\n",
    "$$\\mathcal{L}_{adv}^{A} = \\mathbb{E}\\left[\\left(D_{A}(x_{A}) - 1\\right)^{2}\\right] + \\mathbb{E}\\left[D_{A}\\left(G_{B\\rightarrow A}(x_{B})\\right)^{2}\\right]$$\n",
    "$$\\mathcal{L}_{adv}^{B} = \\mathbb{E}\\left[\\left(D_{B}(x_{B}) - 1\\right)^{2}\\right] + \\mathbb{E}\\left[D_{B}\\left(G_{A\\rightarrow B}(x_{A})\\right)^{2}\\right]$$\n",
    "* Cycle consistency loss：オリジナルの画像と再構成した画像の一貫性を保つために必要な誤差関数です．\n",
    "$$\n",
    "\\mathcal{L}_{cycle}^{A} = \\|x_{A} - G_{B\\rightarrow A}\\left(G_{A\\rightarrow B}\\left(x_{A}\\right)\\right)\\|_{1}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{cycle}^{B} = \\|x_{B} - G_{A\\rightarrow B}\\left(G_{B\\rightarrow A}\\left(x_{B}\\right)\\right)\\|_{1}\n",
    "$$\n",
    "* Identity loss：別のスタイルへ変換したときにオリジナル画像を大幅に変更してしまうことを抑制するために必要な誤差です．実際，この誤差を抜いても学習は可能で，スタイルの変換もすることはできます．この誤差関数が最大の効果を発揮する場面は，ある風景画を絵画風に変換するときです．\n",
    "$$\n",
    "\\mathcal{L}_{identity}^{A} = \\|x_{A} - G_{A\\rightarrow B}\\left(x_{A}\\right)\\|_{1}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{identity}^{B} = \\|x_{B} - G_{B\\rightarrow A}\\left(x_{B}\\right)\\|_{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t2r4i8xb6tE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    identity_loss = None\n",
    "    \n",
    "    G_B2A.train()\n",
    "    G_A2B.train()\n",
    "    D_A.train()\n",
    "    D_B.train()\n",
    "    for idx, (imgA, imgB) in enumerate(training_dataset):\n",
    "        imgA = Variable(imgA.to(device), requires_grad=True)\n",
    "        imgB = Variable(imgB.to(device), requires_grad=True)\n",
    "        imgA_fake, imgB_fake = G_B2A(imgB), G_A2B(imgA)\n",
    "        imgA_rec, imgB_rec = G_B2A(imgB_fake), G_A2B(imgA_fake)\n",
    "        if lambda_id_val > 0:\n",
    "            iden_imgA, iden_imgB = G_B2A(imgA), G_A2B(imgB)\n",
    "        \n",
    "        # Update the discriminator (D_A, D_B)\n",
    "        d_A_opt.zero_grad()\n",
    "        disA_out_real = D_A(imgA)\n",
    "        imgA_fake_ = buffer_for_fakeA.get_images(imgA_fake)\n",
    "        disA_out_fake = D_A(imgA_fake_.detach())\n",
    "        d_lossA_real = adv_loss(disA_out_real, torch.tensor(1.0).expand_as(disA_out_real).to(device))\n",
    "        d_lossA_fake = adv_loss(disA_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
    "        disA_loss = (d_lossA_real + d_lossA_fake) * 0.5\n",
    "        disA_loss.backward()\n",
    "        d_A_opt.step()\n",
    "        \n",
    "        d_B_opt.zero_grad()\n",
    "        disB_out_real = D_B(imgB)\n",
    "        imgB_fake_ = buffer_for_fakeB.get_images(imgB_fake)\n",
    "        disB_out_fake = D_B(imgB_fake_.detach())\n",
    "        d_lossB_real = adv_loss(disB_out_real, torch.tensor(1.0).expand_as(disB_out_real).to(device))\n",
    "        d_lossB_fake = adv_loss(disB_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
    "        disB_loss = (d_lossB_real + d_lossB_fake) * 0.5\n",
    "        disB_loss.backward()\n",
    "        d_B_opt.step()\n",
    "        \n",
    "        # Update the generator (G)\n",
    "        g_opt.zero_grad()\n",
    "        disB_out_fake = D_B(imgB_fake)\n",
    "        disA_out_fake = D_A(imgA_fake)\n",
    "        g_lossA = adv_loss(disA_out_fake, torch.tensor(1.0).expand_as(disA_out_fake).to(device))\n",
    "        g_lossB = adv_loss(disB_out_fake, torch.tensor(1.0).expand_as(disB_out_fake).to(device))\n",
    "        gen_adv_loss = g_lossA + g_lossB\n",
    "        \n",
    "        cycle_consistency_loss = l1_norm(imgA_rec, imgA) + l1_norm(imgB_rec, imgB)\n",
    "        if lambda_id_val > 0:\n",
    "            identity_loss = criterion_idn(iden_imgA, imgA) + criterion_idn(iden_imgB, imgB)\n",
    "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss + lambda_id_val * identity_loss\n",
    "        else:\n",
    "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss\n",
    "        gen_loss.backward()\n",
    "        g_opt.step()\n",
    "        \n",
    "        if idx %100 == 0:\n",
    "            print(idx)\n",
    "        \n",
    "        if idx % 100 == 0 and identity_loss is not None:\n",
    "            print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss (A): {:.6f} | D loss (B): {:.6f} | G loss: {:.6f} | Consistency: {:.6f} | Identity: {:.6f} |'\\\n",
    "                  .format(epoch, idx * len(imgA), len(training_dataset.dataset),\n",
    "                  100. * idx / len(training_dataset), disA_loss.item(), disB_loss.item(), gen_loss.item(), cycle_consistency_loss.item(), identity_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUycupIgLODf"
   },
   "source": [
    "# 学習済みモデルのロード\n",
    "CycleGANはネットワークが深いため，綺麗な変換をするためには長時間の学習を必要とします．\n",
    "Colabratory上で学習できなくはないですが，手早くCycleGANが出力する結果を見たい方は，以下を動作させて学習済みモデルをダウンロードしてください．\n",
    "\n",
    "ディレクトリ内には，appleとorangeの相互変換をするモデルとhorseとzebraの相互変換するモデルが含まれています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzAKjd3rLWdl"
   },
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/99q0491u55moc04/CycleGAN_pretrained_model.zip?dl=1 -O result.zip\n",
    "!unzip -q -o result.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "O7ik4gPALZpQ",
    "outputId": "7edd1d8d-ce78-4657-c6e1-63e078cb83f6"
   },
   "outputs": [],
   "source": [
    "data_name = 'apple2orange'\n",
    "G_A2B_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'G_A2B')\n",
    "G_B2A_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'G_B2A')\n",
    "D_A_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'D_A')\n",
    "D_B_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'D_B')\n",
    "G_A2B.load_state_dict(torch.load(G_A2B_path))\n",
    "G_B2A.load_state_dict(torch.load(G_B2A_path))\n",
    "D_A.load_state_dict(torch.load(D_A_path))\n",
    "D_B.load_state_dict(torch.load(D_B_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "a3i4F1urb6tG",
    "outputId": "08322887-cb56-4d17-c66c-43771a6e1e0f"
   },
   "outputs": [],
   "source": [
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "to_tensor = transforms.ToTensor()\n",
    "transform = transforms.Compose([to_tensor, normalize])\n",
    "\n",
    "domainA_path = os.path.join(data_name, 'testA')\n",
    "domainB_path = os.path.join(data_name, 'testB')\n",
    "imgA_list, imgB_list = os.listdir(domainA_path), os.listdir(domainB_path)\n",
    "indexA, indexB = np.random.randint(len(imgA_list)), np.random.randint(len(imgB_list))\n",
    "imgA = Image.open(os.path.join(domainA_path, imgA_list[indexA]))\n",
    "imgB = Image.open(os.path.join(domainB_path, imgB_list[indexB]))\n",
    "imgA_tensor = transform(imgA).to(device)[None,:,:,:]\n",
    "imgB_tensor = transform(imgB).to(device)[None,:,:,:]\n",
    "G_A2B.eval()\n",
    "G_B2A.eval()\n",
    "with torch.no_grad():\n",
    "    fake_B = G_A2B(imgA_tensor)\n",
    "    fake_A = G_B2A(imgB_tensor)\n",
    "    rec_B = G_A2B(fake_A)\n",
    "    rec_A = G_B2A(fake_B)\n",
    "    mean = torch.tensor(mean, dtype=torch.float32)[None,:,None,None].to(device)\n",
    "    std = torch.tensor(std, dtype=torch.float32)[None,:,None,None].to(device)\n",
    "    fake_B = (fake_B * std) + mean\n",
    "    fake_A = (fake_A * std) + mean\n",
    "\n",
    "fake_imgA = Image.fromarray((fake_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
    "fake_imgB = Image.fromarray((fake_B * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
    "plt_items = [imgA, fake_imgB, imgB, fake_imgA]\n",
    "title_list = ['Real_A', 'Fake_B', 'Real_B', 'Fake_A']\n",
    "rows = 2\n",
    "cols = 2\n",
    "axes=[]\n",
    "fig=plt.figure(figsize=(16, 9))\n",
    "\n",
    "for i in range(rows*cols):\n",
    "    item = plt_items[i]\n",
    "    axes.append( fig.add_subplot(rows, cols, i+1) )\n",
    "    axes[-1].set_title(title_list[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(item)\n",
    "fig.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "ws3szHBNoR0V",
    "outputId": "6885bfd2-ef96-4810-aea6-089169664160"
   },
   "outputs": [],
   "source": [
    "rec_A = (rec_A * std) + mean\n",
    "Image.fromarray((rec_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtI7H0yR0ZOO"
   },
   "source": [
    "# 課題\n",
    "\n",
    "\n",
    "*   データセットを変更して学習してみましょう．\n",
    "*   学習する際に使用しているCycle consistency lossやIdentity lossを使用しない場合，どのような結果が得られるか確認してみましょう．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvWxlMQ2qwzW"
   },
   "source": [
    "# 参考文献\n",
    "[1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros, Image-to-Image Translation with Conditional Adversarial Nets, CVPR, 2017.\\\n",
    "[2] Jun-Yan Zhu, Taesung Park, Phillip Isola and Alexei A. Efros, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IAFsbPjb6tM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "05_CycleGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
