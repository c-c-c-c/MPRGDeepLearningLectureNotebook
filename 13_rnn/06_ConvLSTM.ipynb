{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_ConvLSTM",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFobkOyUzPSvhgWxaoTW1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/13_rnn/06_ConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV0l0NgYXAeB"
      },
      "source": [
        "#Google ColabでGoogleドライブをマウントする\n",
        "* 以下のコードを実行するとURLが出てくるので，クリック\n",
        "* Googleアカウントに許可を求めたらcodeが出るのでコピー\n",
        "* コピーしたcodeを[空白]にペースト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPFFG2xhW_oD",
        "outputId": "7c5e1f0c-4d76-4548-e363-5502df4a2905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2BhrFGqFx_M"
      },
      "source": [
        "#ConvLSTM\n",
        "ConvLSTMはLSTMの内部構造に畳み込み層を導入した再帰型のネットワークである．LSTMの内部に畳み込み層を導入することで，動画像を順伝播逆伝播できるネットワークとなっている．このネットワークは，時系列データを入力として受け取り，中間層のユニットの出力が自分自身に戻る「帰還路」を持つため，過去の情報を保持しやすい構造となる．今回はMoving Mnistを使って動画予測を行います．\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11YcP1HesFdLG5tEsDNkdGdrb4ExR_l5U\" width = 70%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfWe3l_76ZFQ"
      },
      "source": [
        "# モジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfImhoulTMI"
      },
      "source": [
        "import gzip\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import nn\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYa9zXLXNFC"
      },
      "source": [
        "# Data loader\n",
        "Moving Mnistの読み込みを行います．このときバッチ毎にシーケンス数が同じになるように“class MovingMNIST\"内で処理を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxXx6PLGWB8h"
      },
      "source": [
        "def load_mnist(root):\n",
        "    path = os.path.join(root)\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        mnist = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        mnist = mnist.reshape(-1, 28, 28)\n",
        "    return mnist\n",
        "\n",
        "\n",
        "def load_fixed_set(root, is_train):\n",
        "    filename = 'mnist_test_seq.npy'\n",
        "    path = os.path.join(root, filename)\n",
        "    dataset = np.load(path)\n",
        "    dataset = dataset[..., np.newaxis]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class MovingMNIST(data.Dataset):\n",
        "    def __init__(self, root, is_train, n_frames_input, n_frames_output, num_objects,\n",
        "                 transform=None):\n",
        "        '''\n",
        "        param num_objects: a list of number of possible objects.\n",
        "        '''\n",
        "        super(MovingMNIST, self).__init__()\n",
        "\n",
        "        self.dataset = None\n",
        "        if is_train:\n",
        "            self.mnist = load_mnist(root)\n",
        "        else:\n",
        "            if num_objects[0] != 2:\n",
        "                self.mnist = load_mnist(root)\n",
        "            else:\n",
        "                self.dataset = load_fixed_set(root, False)\n",
        "        self.length = int(1e4) if self.dataset is None else self.dataset.shape[1]\n",
        "\n",
        "        self.is_train = is_train\n",
        "        self.num_objects = num_objects\n",
        "        self.n_frames_input = n_frames_input\n",
        "        self.n_frames_output = n_frames_output\n",
        "        self.n_frames_total = self.n_frames_input + self.n_frames_output\n",
        "        self.transform = transform\n",
        "        self.image_size_ = 64\n",
        "        self.digit_size_ = 28\n",
        "        self.step_length_ = 0.1\n",
        "\n",
        "    def get_random_trajectory(self, seq_length):\n",
        "        ''' Generate a random sequence of a MNIST digit '''\n",
        "        canvas_size = self.image_size_ - self.digit_size_\n",
        "        x = random.random()\n",
        "        y = random.random()\n",
        "        theta = random.random() * 2 * np.pi\n",
        "        v_y = np.sin(theta)\n",
        "        v_x = np.cos(theta)\n",
        "\n",
        "        start_y = np.zeros(seq_length)\n",
        "        start_x = np.zeros(seq_length)\n",
        "        for i in range(seq_length):\n",
        "            y += v_y * self.step_length_\n",
        "            x += v_x * self.step_length_\n",
        "\n",
        "            if x <= 0:\n",
        "                x = 0\n",
        "                v_x = -v_x\n",
        "            if x >= 1.0:\n",
        "                x = 1.0\n",
        "                v_x = -v_x\n",
        "            if y <= 0:\n",
        "                y = 0\n",
        "                v_y = -v_y\n",
        "            if y >= 1.0:\n",
        "                y = 1.0\n",
        "                v_y = -v_y\n",
        "            start_y[i] = y\n",
        "            start_x[i] = x\n",
        "\n",
        "        start_y = (canvas_size * start_y).astype(np.int32)\n",
        "        start_x = (canvas_size * start_x).astype(np.int32)\n",
        "        return start_y, start_x\n",
        "\n",
        "    def generate_moving_mnist(self, num_digits=2):\n",
        "        '''\n",
        "        Get random trajectories for the digits and generate a video.\n",
        "        '''\n",
        "        data = np.zeros((self.n_frames_total, self.image_size_, self.image_size_), dtype=np.float32)\n",
        "        for n in range(num_digits):\n",
        "            # Trajectory\n",
        "            start_y, start_x = self.get_random_trajectory(self.n_frames_total)\n",
        "            ind = random.randint(0, self.mnist.shape[0] - 1)\n",
        "            digit_image = self.mnist[ind]\n",
        "            for i in range(self.n_frames_total):\n",
        "                top = start_y[i]\n",
        "                left = start_x[i]\n",
        "                bottom = top + self.digit_size_\n",
        "                right = left + self.digit_size_\n",
        "                data[i, top:bottom, left:right] = np.maximum(data[i, top:bottom, left:right], digit_image)\n",
        "\n",
        "        data = data[..., np.newaxis]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        length = self.n_frames_input + self.n_frames_output\n",
        "        if self.is_train or self.num_objects[0] != 2:\n",
        "            num_digits = random.choice(self.num_objects)\n",
        "            images = self.generate_moving_mnist(num_digits)\n",
        "        else:\n",
        "            images = self.dataset[:, idx, ...]\n",
        "\n",
        "\n",
        "        r = 1\n",
        "        w = int(64 / r)\n",
        "        images = images.reshape((length, w, r, w, r)).transpose(0, 2, 4, 1, 3).reshape((length, r * r, w, w))\n",
        "\n",
        "        input = images[:self.n_frames_input]\n",
        "        if self.n_frames_output > 0:\n",
        "            output = images[self.n_frames_input:length]\n",
        "        else:\n",
        "            output = []\n",
        "\n",
        "        frozen = input[-1]\n",
        "\n",
        "        output = torch.from_numpy(output / 255.0).contiguous().float()\n",
        "        input = torch.from_numpy(input / 255.0).contiguous().float()\n",
        "\n",
        "        out = [idx, output, input, frozen, np.zeros(1)]\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqxw5Fg253Mb"
      },
      "source": [
        "# 学習パラメータの設定やデータの読み込み\n",
        " 各パラメータを設定します．本実験では入出力を10フレームとし，観測10フレームの動画像を入力し，その後の10フレームの予測動画像を出力します．MovingMNIST内のrootで，driveに置いてあるファイルパスを指定します．このとき，学習用と評価用にsplitしたファイルが各Folderです．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHySon9BVa2S"
      },
      "source": [
        "frame_input = 10\n",
        "frame_output = 10\n",
        "batch = 64\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "\n",
        "trainFolder = MovingMNIST(is_train=True,\n",
        "                          root='/content/drive/My Drive/Colab_Notebooks/train-images-idx3-ubyte.gz',\n",
        "                          n_frames_input=frame_input,\n",
        "                          n_frames_output=frame_output,\n",
        "                          num_objects=[3])\n",
        "validFolder = MovingMNIST(is_train=False,\n",
        "                          root='/content/drive/My Drive/Colab_Notebooks/train-images-idx3-ubyte.gz',\n",
        "                          n_frames_input=frame_input,\n",
        "                          n_frames_output=frame_output,\n",
        "                          num_objects=[3])\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainFolder,\n",
        "                                          batch_size=batch,\n",
        "                                          shuffle=False)\n",
        "validLoader = torch.utils.data.DataLoader(validFolder,\n",
        "                                          batch_size=batch,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkaZX-pJw9g"
      },
      "source": [
        "# Layer作成\n",
        "Encoder，Decoderで指定されたlayer name毎に処理を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECQgo-DJkMSF"
      },
      "source": [
        "def make_layers(block):\n",
        "    layers = []\n",
        "    for layer_name, v in block.items():\n",
        "        if 'pool' in layer_name:\n",
        "            layer = nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2])\n",
        "            layers.append((layer_name, layer))\n",
        "        elif 'deconv' in layer_name:\n",
        "            transposeConv2d = nn.ConvTranspose2d(in_channels=v[0],\n",
        "                                                 out_channels=v[1],\n",
        "                                                 kernel_size=v[2],\n",
        "                                                 stride=v[3],\n",
        "                                                 padding=v[4])\n",
        "            layers.append((layer_name, transposeConv2d))\n",
        "            if 'relu' in layer_name:\n",
        "                layers.append(('relu_' + layer_name, nn.ReLU(inplace=True)))\n",
        "            elif 'leaky' in layer_name:\n",
        "                layers.append(('leaky_' + layer_name,\n",
        "                               nn.LeakyReLU(negative_slope=0.2, inplace=True)))\n",
        "        elif 'conv' in layer_name:\n",
        "            conv2d = nn.Conv2d(in_channels=v[0],\n",
        "                               out_channels=v[1],\n",
        "                               kernel_size=v[2],\n",
        "                               stride=v[3],\n",
        "                               padding=v[4])\n",
        "            layers.append((layer_name, conv2d))\n",
        "            if 'relu' in layer_name:\n",
        "                layers.append(('relu_' + layer_name, nn.ReLU(inplace=True)))\n",
        "            elif 'leaky' in layer_name:\n",
        "                layers.append(('leaky_' + layer_name,\n",
        "                               nn.LeakyReLU(negative_slope=0.2, inplace=True)))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    return nn.Sequential(OrderedDict(layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWjft0JKj6jS"
      },
      "source": [
        "#Encoder\n",
        "Moving Mnistの画像特徴量をConvolutionして捉えます．このとき，各時刻で得られる画像特徴量をConvolutional LSTMで時間方向に伝播することで，動画像における特徴量を求めます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF6_l1dxj6AA"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, subnets, rnns):\n",
        "        super().__init__()\n",
        "        assert len(subnets) == len(rnns)\n",
        "        self.blocks = len(subnets)\n",
        "\n",
        "        for index, (params, rnn) in enumerate(zip(subnets, rnns), 1):\n",
        "            setattr(self, 'stage' + str(index), make_layers(params))\n",
        "            setattr(self, 'rnn' + str(index), rnn)\n",
        "\n",
        "    def forward_by_stage(self, inputs, subnet, rnn):\n",
        "        seq_number, batch_size, input_channel, height, width = inputs.size()\n",
        "        inputs = torch.reshape(inputs, (-1, input_channel, height, width))\n",
        "        inputs = subnet(inputs)\n",
        "        inputs = torch.reshape(inputs, (seq_number, batch_size, inputs.size(1),\n",
        "                                        inputs.size(2), inputs.size(3)))\n",
        "        outputs_stage, state_stage = rnn(inputs, None)\n",
        "        return outputs_stage, state_stage\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.transpose(0, 1)  # to S,B,1,64,64\n",
        "        hidden_states = []\n",
        "        logging.debug(inputs.size())\n",
        "        for i in range(1, self.blocks + 1):\n",
        "            inputs, state_stage = self.forward_by_stage(\n",
        "                inputs, getattr(self, 'stage' + str(i)),\n",
        "                getattr(self, 'rnn' + str(i)))\n",
        "            hidden_states.append(state_stage)\n",
        "        return tuple(hidden_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVVJa1CPkAqf"
      },
      "source": [
        "#Decoder\n",
        "畳み込んだ特徴量をDeconvolutionします．このとき，各時刻で得られる画像特徴量をDeconvolutional LSTMで時間方向に伝播しつつ，元の動画像サイズまで復元します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHU9DokFkALg"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, subnets, rnns):\n",
        "        super().__init__()\n",
        "        assert len(subnets) == len(rnns)\n",
        "\n",
        "        self.blocks = len(subnets)\n",
        "\n",
        "        for index, (params, rnn) in enumerate(zip(subnets, rnns)):\n",
        "            setattr(self, 'rnn' + str(self.blocks - index), rnn)\n",
        "            setattr(self, 'stage' + str(self.blocks - index),\n",
        "                    make_layers(params))\n",
        "\n",
        "    def forward_by_stage(self, inputs, state, subnet, rnn):\n",
        "        inputs, state_stage = rnn(inputs, state, seq_len=10)\n",
        "        seq_number, batch_size, input_channel, height, width = inputs.size()\n",
        "        inputs = torch.reshape(inputs, (-1, input_channel, height, width))\n",
        "        inputs = subnet(inputs)\n",
        "        inputs = torch.reshape(inputs, (seq_number, batch_size, inputs.size(1),\n",
        "                                        inputs.size(2), inputs.size(3)))\n",
        "        return inputs\n",
        "\n",
        "        # input: 5D S*B*C*H*W\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        inputs = self.forward_by_stage(None, hidden_states[-1],\n",
        "                                       getattr(self, 'stage3'),\n",
        "                                       getattr(self, 'rnn3'))\n",
        "        for i in list(range(1, self.blocks))[::-1]:\n",
        "            inputs = self.forward_by_stage(inputs, hidden_states[i - 1],\n",
        "                                           getattr(self, 'stage' + str(i)),\n",
        "                                           getattr(self, 'rnn' + str(i)))\n",
        "        inputs = inputs.transpose(0, 1)  # to B,S,1,64,64\n",
        "        return inputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI2Q7j_rKnrP"
      },
      "source": [
        "# Encoder-Decoderに各入力を渡す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeT5G3ITdYRX"
      },
      "source": [
        "class activation():\n",
        "\n",
        "    def __init__(self, act_type, negative_slope=0.2, inplace=True):\n",
        "        super().__init__()\n",
        "        self._act_type = act_type\n",
        "        self.negative_slope = negative_slope\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def __call__(self, input):\n",
        "        if self._act_type == 'leaky':\n",
        "            return F.leaky_relu(input, negative_slope=self.negative_slope, inplace=self.inplace)\n",
        "        elif self._act_type == 'relu':\n",
        "            return F.relu(input, inplace=self.inplace)\n",
        "        elif self._act_type == 'sigmoid':\n",
        "            return torch.sigmoid(input)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "\n",
        "class ED(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input):\n",
        "        state = self.encoder(input)\n",
        "        output = self.decoder(state)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wenPUf1XRSb"
      },
      "source": [
        "# Convolution LSTM\n",
        "動画像における特徴量を求めます．Encoder-Decoderへの引数としてencoder_paramsとdecoder_paramsを定義します．Conv.LSTM層はは3つで，それぞれConvolutionまたはDeconvolution処理を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J584vKaXbp37"
      },
      "source": [
        "class CLSTM_cell(nn.Module):\n",
        "    \"\"\"ConvLSTMCell\n",
        "    \"\"\"\n",
        "    def __init__(self, shape, input_channels, filter_size, num_features):\n",
        "        super(CLSTM_cell, self).__init__()\n",
        "\n",
        "        self.shape = shape  # H, W\n",
        "        self.input_channels = input_channels\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self.padding = (filter_size - 1) // 2\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_channels + self.num_features,\n",
        "                      4 * self.num_features, self.filter_size, 1,\n",
        "                      self.padding),\n",
        "            nn.GroupNorm(4 * self.num_features // 32, 4 * self.num_features))\n",
        "\n",
        "    def forward(self, inputs=None, hidden_state=None, seq_len=10):\n",
        "        if hidden_state is None:\n",
        "            hx = torch.zeros(inputs.size(1), self.num_features, self.shape[0],\n",
        "                             self.shape[1]).cuda()\n",
        "            cx = torch.zeros(inputs.size(1), self.num_features, self.shape[0],\n",
        "                             self.shape[1]).cuda()\n",
        "        else:\n",
        "            hx, cx = hidden_state\n",
        "        output_inner = []\n",
        "        for index in range(seq_len):\n",
        "            if inputs is None:\n",
        "                x = torch.zeros(hx.size(0), self.input_channels, self.shape[0],\n",
        "                                self.shape[1]).cuda()\n",
        "            else:\n",
        "                x = inputs[index, ...]\n",
        "\n",
        "            combined = torch.cat((x, hx), 1)\n",
        "            gates = self.conv(combined)\n",
        "            ingate, forgetgate, cellgate, outgate = torch.split(\n",
        "                gates, self.num_features, dim=1)\n",
        "            ingate = torch.sigmoid(ingate)\n",
        "            forgetgate = torch.sigmoid(forgetgate)\n",
        "            cellgate = torch.tanh(cellgate)\n",
        "            outgate = torch.sigmoid(outgate)\n",
        "\n",
        "            cy = (forgetgate * cx) + (ingate * cellgate)\n",
        "            hy = outgate * torch.tanh(cy)\n",
        "            output_inner.append(hy)\n",
        "            hx = hy\n",
        "            cx = cy\n",
        "        return torch.stack(output_inner), (hy, cy)\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "encoder_params = [\n",
        "    [\n",
        "        OrderedDict({'conv1_leaky_1': [1, 16, 3, 1, 1]}),\n",
        "        OrderedDict({'conv2_leaky_1': [64, 64, 3, 2, 1]}),\n",
        "        OrderedDict({'conv3_leaky_1': [96, 96, 3, 2, 1]}),\n",
        "    ],\n",
        "\n",
        "    [\n",
        "        CLSTM_cell(shape=(64,64), input_channels=16, filter_size=5, num_features=64),\n",
        "        CLSTM_cell(shape=(32,32), input_channels=64, filter_size=5, num_features=96),\n",
        "        CLSTM_cell(shape=(16,16), input_channels=96, filter_size=5, num_features=96)\n",
        "    ]\n",
        "]\n",
        "\n",
        "decoder_params = [\n",
        "    [\n",
        "        OrderedDict({'deconv1_leaky_1': [96, 96, 4, 2, 1]}),\n",
        "        OrderedDict({'deconv2_leaky_1': [96, 96, 4, 2, 1]}),\n",
        "        OrderedDict({\n",
        "            'conv3_leaky_1': [64, 16, 3, 1, 1],\n",
        "            'conv4_leaky_1': [16, 1, 1, 1, 0]\n",
        "        }),\n",
        "    ],\n",
        "\n",
        "    [\n",
        "        CLSTM_cell(shape=(16,16), input_channels=96, filter_size=5, num_features=96),\n",
        "        CLSTM_cell(shape=(32,32), input_channels=96, filter_size=5, num_features=96),\n",
        "        CLSTM_cell(shape=(64,64), input_channels=96, filter_size=5, num_features=64),\n",
        "    ]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Y50zBJXVCo"
      },
      "source": [
        "# モデル定義\n",
        "EncoderとDecoderに各パラメータを引数として入れます．損失関数にはMSELossを用います．最適化手法にはAdamを使用します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQdkgeqLUE3z"
      },
      "source": [
        "encoder = Encoder(encoder_params[0], encoder_params[1]).cuda()\n",
        "decoder = Decoder(decoder_params[0], decoder_params[1]).cuda()\n",
        "net = ED(encoder, decoder)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    net = nn.DataParallel(net)\n",
        "net.to(device)\n",
        "lossfunction = nn.MSELoss().cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUumnymHb6lq"
      },
      "source": [
        "#学習\n",
        "学習と検証用データを回します．動画特徴量を捉えるため，エポック数を少々増やさないとうまく学習されないのと，学習に3日前後使うので，注意してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Gm99Q9bhNe"
      },
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "avg_train_losses = []\n",
        "avg_valid_losses = []\n",
        "for epoch in range(0, epochs + 1):\n",
        "    ######################\n",
        "    # 学習\n",
        "    ######################\n",
        "    t = tqdm(trainLoader, leave=False, total=len(trainLoader))\n",
        "    train_loss = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    for i, (idx, targetVar, inputVar, _, _) in enumerate(t):\n",
        "        inputs = inputVar.to(device)  # B,S,C,H,W\n",
        "        label = targetVar.to(device)  # B,S,C,H,W\n",
        "        optimizer.zero_grad()\n",
        "        net.train()\n",
        "        pred = net(inputs)  # B,S,C,H,W\n",
        "        loss = lossfunction(pred, label)\n",
        "        loss_aver = loss.item() / batch\n",
        "        train_loss += loss_aver\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(net.parameters(), clip_value=10.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        t.set_postfix({\n",
        "            'trainloss': '{:.6f}'.format(loss_aver),\n",
        "            'epoch': '{:02d}'.format(epoch)\n",
        "        })\n",
        "\n",
        "    ######################\n",
        "    # 検証\n",
        "    ######################\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        t = tqdm(validLoader, leave=False, total=len(validLoader))\n",
        "        for i, (idx, targetVar, inputVar, _, _) in enumerate(t):\n",
        "            if i == 3000:\n",
        "                break\n",
        "            inputs = inputVar.to(device)\n",
        "            label = targetVar.to(device)\n",
        "            pred = net(inputs)\n",
        "            loss = lossfunction(pred, label)\n",
        "            loss_aver = loss.item() / batch\n",
        "            test_loss += loss_aver\n",
        "            valid_losses.append(loss_aver)\n",
        "            t.set_postfix({\n",
        "                'validloss': '{:.6f}'.format(loss_aver),\n",
        "                'epoch': '{:02d}'.format(epoch)\n",
        "            })\n",
        "    print('%03d-epoch: train_loss %.5f : val_loss %.5f' %(epoch, train_loss/len(t), test_loss/len(t)))\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK36mAplESG2"
      },
      "source": [
        "# 可視化と評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXIdYSL3kU95"
      },
      "source": [
        "net.eval()\n",
        "t = tqdm(validLoader, leave=False, total=len(validLoader))\n",
        "for i, (idx, targetVar, inputVar, _, _) in enumerate(t):\n",
        "    if i == 3000:\n",
        "        break\n",
        "    inputs = inputVar.to(device)\n",
        "    label = targetVar.to(device)\n",
        "    pred = net(inputs)\n",
        "    plt.figure(figsize=[20, 6])\n",
        "    for i in range(10):\n",
        "        plt.subplot(2, 10, i+1)\n",
        "        plt.imshow(label[0, i, 0].detach().cpu().numpy())\n",
        "        plt.axis('off')\n",
        "        plt.title('gt # %2d' %i)\n",
        "        plt.subplot(2, 10, i+11)\n",
        "        plt.imshow(pred[0, i, 0].detach().cpu().numpy())\n",
        "        plt.axis('off')\n",
        "        plt.title('pred # %2d' %i)\n",
        "    loss = lossfunction(pred, label)\n",
        "    loss_aver = loss.item() / batch\n",
        "    test_loss += loss_aver\n",
        "    # record validation loss\n",
        "    valid_losses.append(loss_aver)\n",
        "    #print (\"validloss: {:.6f},  epoch : {:02d}\".format(loss_aver,epoch),end = '\\r', flush=True)\n",
        "    t.set_postfix({\n",
        "        'validloss': '{:.6f}'.format(loss_aver),\n",
        "        'epoch': '{:02d}'.format(epoch)\n",
        "    })\n",
        "print('loss:{:.6f}'.format(test_loss/len(t)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfgrm5jrF1BJ"
      },
      "source": [
        "可視化すると以下のようになります．\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1DV0WT6PBZ8BLn2_okuc4cbLI2f72Vs_2\" width = 100%>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1K1yqj3SWVptOfOURaNeOImQLlC-Uo9a6\" width = 100%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH2uB0bLTrAQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}