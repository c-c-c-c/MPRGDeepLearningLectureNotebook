{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "05_batchnorm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQEJLU_C3_sL"
      },
      "source": [
        "# 07：Batch Normalizationの導入\n",
        "\n",
        "---\n",
        "## 目的\n",
        "多層パーセプトロン (Multi Layer Perceptoron; MLP) を用いたMNISTデータセットの認識実験において，Batch Normalizationの効果を理解する．\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vZoiRR03_sL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VIul2gL3_sO"
      },
      "source": [
        "## データセットのダウンロードと読み込みとサンプル数の削減\n",
        "\n",
        "まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DDcpz6P3_sO"
      },
      "source": [
        "!wget -q http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXTzEns3_sQ"
      },
      "source": [
        "次に，ダウンロードしたファイルからデータを読み込みます．詳細は前回までのプログラムを確認してください．\n",
        "\n",
        "\n",
        "ここで，学習データを削減します．\n",
        "今回は1000サンプルになるように，先頭から1000個の学習データとラベルを取得します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGEMFDLI3_sR"
      },
      "source": [
        "# load images\n",
        "with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "x_train = x_train[0:1000, :]\n",
        "y_train = y_train[0:1000]\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlOaqBgd3_sV"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "次に，ニューラルネットワーク（多層パーセプトロン）を定義します．\n",
        "\n",
        "まずはじめに，ネットワークの定義に必要な関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVfJLTjw3_sV"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x.shape)\n",
        "    grad[x > 0] = 1\n",
        "    return grad\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "    else:\n",
        "        return np.exp(x) / np.sum(np.exp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkdkqRjq3_sX"
      },
      "source": [
        "次に，上で定義した関数を用いてネットワークを定義します． ここでは，Batch Normalizationを適用する場合としない場合の2種類の5層の多層パーセプトロンを定義します．\n",
        "\n",
        "まず，Batch Normaliztionを適用しないネットワーク`MLP`を定義します．詳細については前回までの資料を確認してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkujL9rz3_sX"
      },
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, w_std=0.01):\n",
        "        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = w_std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.b2 = np.zeros(hidden_size)\n",
        "        self.w3 = w_std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.b3 = np.zeros(hidden_size)\n",
        "        self.w4 = w_std * np.random.randn(hidden_size, output_size)\n",
        "        self.b4 = np.zeros(output_size)\n",
        "\n",
        "        self.act = relu\n",
        "        self.act_grad = relu_grad\n",
        "        \n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h1 = np.dot(x, self.w1) + self.b1\n",
        "        self.h2 = self.act(self.h1)\n",
        "        self.h3 = np.dot(self.h2, self.w2) + self.b2\n",
        "        self.h4 = self.act(self.h3)\n",
        "        self.h5 = np.dot(self.h4, self.w3) + self.b3\n",
        "        self.h6 = self.act(self.h5)\n",
        "        self.h7 = np.dot(self.h6, self.w4) + self.b4\n",
        "        self.y = softmax(self.h7)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "        self.grads = {}\n",
        "        \n",
        "        t = np.identity(10)[t]\n",
        "        dy = (self.y - t) / batch_size\n",
        "        self.grads['w4'] = np.dot(self.h6.T, dy)\n",
        "        self.grads['b4'] = np.sum(dy, axis=0)\n",
        "\n",
        "        d_h6 = np.dot(dy, self.w4.T)\n",
        "        d_h5 = self.act_grad(self.h5) * d_h6\n",
        "        self.grads['w3'] = np.dot(self.h4.T, d_h5)\n",
        "        self.grads['b3'] = np.sum(d_h5, axis=0)\n",
        "\n",
        "        d_h4 = np.dot(d_h5, self.w3.T)\n",
        "        d_h3 = self.act_grad(self.h3) * d_h4\n",
        "        self.grads['w2'] = np.dot(self.h2.T, d_h3)\n",
        "        self.grads['b2'] = np.sum(d_h3, axis=0)\n",
        "        \n",
        "        d_h2 = np.dot(d_h3, self.w2.T)\n",
        "        d_h1 = self.act_grad(self.h1) * d_h2\n",
        "        self.grads['w1'] = np.dot(x.T, d_h1)\n",
        "        self.grads['b1'] = np.sum(d_h1, axis=0)\n",
        "        \n",
        "    def update_parameters(self, lr=0.1):\n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']  \n",
        "        self.w3 -= lr * self.grads['w3']\n",
        "        self.b3 -= lr * self.grads['b3']\n",
        "        self.w4 -= lr * self.grads['w4']\n",
        "        self.b4 -= lr * self.grads['b4'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eepCrqz3wLTx"
      },
      "source": [
        "次に，Batch Normalizationを適用する場合のネットワーク`MLPBatchNorm`を定義します．\n",
        "\n",
        "\n",
        "`__init__`関数でネットワークの初期化を行う際に，各中間層（全結合層）後に適用するBatch Norm.のパラメータを定義し，初期化します．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DLJ8EPzpK-h"
      },
      "source": [
        "class MLPBatchNorm:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, w_std=0.01):\n",
        "        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = w_std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.b2 = np.zeros(hidden_size)\n",
        "        self.w3 = w_std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.b3 = np.zeros(hidden_size)\n",
        "        self.w4 = w_std * np.random.randn(hidden_size, output_size)\n",
        "        self.b4 = np.zeros(output_size)\n",
        "\n",
        "        self.act = relu\n",
        "        self.act_grad = relu_grad\n",
        "        \n",
        "        # Batch Normalization（1層目）の初期化\n",
        "        self.bn_input_size1 = None\n",
        "        self.gamma1 = np.ones(hidden_size)\n",
        "        self.beta1 = np.zeros(hidden_size)\n",
        "        self.momentum1 = 0.9\n",
        "        self.running_mean1 = None\n",
        "        self.running_var1 = None\n",
        "        self.batch_size1 = None\n",
        "        self.xc1 = None\n",
        "        self.std1 = None\n",
        "\n",
        "        # Batch Normalization（2層目）の初期化\n",
        "        self.bn_input_size2 = None\n",
        "        self.gamma2 = np.ones(hidden_size)\n",
        "        self.beta2 = np.zeros(hidden_size)\n",
        "        self.momentum2 = 0.9\n",
        "        self.running_mean2 = None\n",
        "        self.running_var2 = None\n",
        "        self.batch_size2 = None\n",
        "        self.xc2 = None\n",
        "        self.std2 = None\n",
        "\n",
        "        # Batch Normalization（3層目）の初期化\n",
        "        self.bn_input_size3 = None\n",
        "        self.gamma3 = np.ones(hidden_size)\n",
        "        self.beta3 = np.zeros(hidden_size)\n",
        "        self.momentum3 = 0.9\n",
        "        self.running_mean3 = None\n",
        "        self.running_var3 = None\n",
        "        self.batch_size3 = None\n",
        "        self.xc3 = None\n",
        "        self.std3 = None\n",
        "\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x, train_mode=True):\n",
        "        self.h1 = np.dot(x, self.w1) + self.b1\n",
        "\n",
        "        # Batch Normalization 1層目 =======\n",
        "        N, D = self.h1.shape\n",
        "        if self.running_mean1 is None:\n",
        "            self.running_mean1 = np.zeros(D)\n",
        "            self.running_var1 = np.zeros(D)\n",
        "\n",
        "        if train_mode:\n",
        "            mu1 = self.h1.mean(axis=0)\n",
        "            xc1 = self.h1 - mu1\n",
        "            var1 = np.mean(xc1**2, axis=0)\n",
        "            std1 = np.sqrt(var1 + 10e-7)\n",
        "            xn1 = xc1 / std1\n",
        "\n",
        "            self.batch_size1 = self.h1.shape[0]\n",
        "            self.xc1 = xc1\n",
        "            self.xn1 = xn1\n",
        "            self.std1 = std1\n",
        "            self.running_mean1 = self.momentum1 * self.running_mean1 + (1 - self.momentum1) * mu1\n",
        "            self.running_var1 = self.momentum1 * self.running_var1 + (1 - self.momentum1) * var1\n",
        "        else:\n",
        "            xc1 = self.h1 - self.running_mean1\n",
        "            xn1 = xc1 / ((np.sqrt(self.running_var1 + 10e-7)))\n",
        "        self.h2 = self.gamma1 * xn1 + self.beta1\n",
        "        # ================================\n",
        "\n",
        "        self.h3 = self.act(self.h2)\n",
        "        self.h4 = np.dot(self.h3, self.w2) + self.b2\n",
        "\n",
        "        # Batch Normalization 2層目 =======\n",
        "        N, D = self.h4.shape\n",
        "        if self.running_mean2 is None:\n",
        "            self.running_mean2 = np.zeros(D)\n",
        "            self.running_var2 = np.zeros(D)\n",
        "\n",
        "        if train_mode:\n",
        "            mu2 = self.h4.mean(axis=0)\n",
        "            xc2 = self.h4 - mu2\n",
        "            var2 = np.mean(xc2**2, axis=0)\n",
        "            std2 = np.sqrt(var2 + 10e-7)\n",
        "            xn2 = xc2 / std2\n",
        "\n",
        "            self.batch_size2 = self.h4.shape[0]\n",
        "            self.xc2 = xc2\n",
        "            self.xn2 = xn2\n",
        "            self.std2 = std2\n",
        "            self.running_mean2 = self.momentum2 * self.running_mean2 + (1 - self.momentum2) * mu2\n",
        "            self.running_var2 = self.momentum2 * self.running_var2 + (1 - self.momentum2) * var2\n",
        "        else:\n",
        "            xc2 = self.h4 - self.running_mean2\n",
        "            xn2 = xc2 / ((np.sqrt(self.running_var2 + 10e-7)))\n",
        "        self.h5 = self.gamma2 * xn2 + self.beta2\n",
        "        # ================================\n",
        "\n",
        "        self.h6 = self.act(self.h5)\n",
        "        self.h7 = np.dot(self.h6, self.w3) + self.b3\n",
        "\n",
        "        # Batch Normalization 3層目 =======\n",
        "        N, D = self.h7.shape\n",
        "        if self.running_mean3 is None:\n",
        "            self.running_mean3 = np.zeros(D)\n",
        "            self.running_var3 = np.zeros(D)\n",
        "\n",
        "        if train_mode:\n",
        "            mu3 = self.h7.mean(axis=0)\n",
        "            xc3 = self.h7 - mu3\n",
        "            var3 = np.mean(xc3**2, axis=0)\n",
        "            std3 = np.sqrt(var3 + 10e-7)\n",
        "            xn3 = xc3 / std3\n",
        "\n",
        "            self.batch_size3 = self.h7.shape[0]\n",
        "            self.xc3 = xc3\n",
        "            self.xn3 = xn3\n",
        "            self.std3 = std3\n",
        "            self.running_mean3 = self.momentum3 * self.running_mean3 + (1 - self.momentum3) * mu3\n",
        "            self.running_var3 = self.momentum3 * self.running_var3 + (1 - self.momentum3) * var3\n",
        "        else:\n",
        "            xc3 = self.h7 - self.running_mean3\n",
        "            xn3 = xc3 / ((np.sqrt(self.running_var3 + 10e-7)))\n",
        "        self.h8 = self.gamma3 * xn3 + self.beta3\n",
        "        # ================================\n",
        "\n",
        "        self.h9 = self.act(self.h8)\n",
        "        self.h10 = np.dot(self.h9, self.w4) + self.b4\n",
        "        self.y = softmax(self.h10)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "        self.grads = {}\n",
        "        \n",
        "        t = np.identity(10)[t]\n",
        "        dy = (self.y - t) / batch_size\n",
        "\n",
        "        self.grads['w4'] = np.dot(self.h9.T, dy)\n",
        "        self.grads['b4'] = np.sum(dy, axis=0)\n",
        "\n",
        "        d_h9 = np.dot(dy, self.w4.T)\n",
        "        d_h8 = self.act_grad(self.h8) * d_h9\n",
        "\n",
        "        # Batch Norm. 勾配　3層目 ==========\n",
        "        self.grads['beta3'] = d_h8.sum(axis=0)\n",
        "        self.grads['gamma3'] = np.sum(self.xn3 * d_h8, axis=0)\n",
        "        dxn3 = self.gamma3 * d_h8\n",
        "        dxc3 = dxn3 / self.std3\n",
        "        dstd3 = -np.sum((dxn3 * self.xc3) / (self.std3 * self.std3), axis=0)\n",
        "        dvar3 = 0.5 * dstd3 / self.std3\n",
        "        dxc3 += (2.0 / self.batch_size3) * self.xc3 * dvar3\n",
        "        dmu3 = np.sum(dxc3, axis=0)\n",
        "        d_h7 = dxc3 - dmu3 / self.batch_size3\n",
        "        # ================================\n",
        "\n",
        "        self.grads['w3'] = np.dot(self.h6.T, d_h7)\n",
        "        self.grads['b3'] = np.sum(d_h7, axis=0)\n",
        "\n",
        "        d_h6 = np.dot(d_h7, self.w3.T)\n",
        "        d_h5 = self.act_grad(self.h5) * d_h6\n",
        "\n",
        "        # Batch Norm. 勾配　2層目 ==========\n",
        "        self.grads['beta2'] = d_h5.sum(axis=0)\n",
        "        self.grads['gamma2'] = np.sum(self.xn2 * d_h5, axis=0)\n",
        "        dxn2 = self.gamma2 * d_h5\n",
        "        dxc2 = dxn2 / self.std2\n",
        "        dstd2 = -np.sum((dxn2 * self.xc2) / (self.std2 * self.std2), axis=0)\n",
        "        dvar2 = 0.5 * dstd2 / self.std2\n",
        "        dxc2 += (2.0 / self.batch_size2) * self.xc2 * dvar2\n",
        "        dmu2 = np.sum(dxc2, axis=0)\n",
        "        d_h4 = dxc2 - dmu2 / self.batch_size2\n",
        "        # ================================\n",
        "\n",
        "        self.grads['w2'] = np.dot(self.h3.T, d_h4)\n",
        "        self.grads['b2'] = np.sum(d_h4, axis=0)\n",
        "        \n",
        "        d_h3 = np.dot(d_h4, self.w2.T)\n",
        "        d_h2 = self.act_grad(self.h2) * d_h3\n",
        "\n",
        "        # Batch Norm. 勾配　1層目 ==========\n",
        "        self.grads['beta1'] = d_h2.sum(axis=0)\n",
        "        self.grads['gamma1'] = np.sum(self.xn1 * d_h2, axis=0)\n",
        "        dxn1 = self.gamma1 * d_h2\n",
        "        dxc1 = dxn1 / self.std1\n",
        "        dstd1 = -np.sum((dxn1 * self.xc1) / (self.std1 * self.std1), axis=0)\n",
        "        dvar1 = 0.5 * dstd1 / self.std1\n",
        "        dxc1 += (2.0 / self.batch_size1) * self.xc1 * dvar1\n",
        "        dmu1 = np.sum(dxc1, axis=0)\n",
        "        d_h1 = dxc1 - dmu1 / self.batch_size1\n",
        "        # ================================\n",
        "\n",
        "        self.grads['w1'] = np.dot(x.T, d_h1)\n",
        "        self.grads['b1'] = np.sum(d_h1, axis=0)\n",
        "        \n",
        "    def update_parameters(self, lr=0.1):\n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']  \n",
        "        self.w3 -= lr * self.grads['w3']\n",
        "        self.b3 -= lr * self.grads['b3']\n",
        "        self.gamma1 -= lr * self.grads['gamma1']\n",
        "        self.beta1  -= lr * self.grads['beta1']\n",
        "        self.gamma2 -= lr * self.grads['gamma2']\n",
        "        self.beta2  -= lr * self.grads['beta2']\n",
        "        self.gamma3 -= lr * self.grads['gamma3']\n",
        "        self.beta3  -= lr * self.grads['beta3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaRjNUOo3_sb"
      },
      "source": [
        "## 学習\n",
        "サンプル数を削減したMNISTデータセットと作成したネットワークを用いて，２つのネットワークの学習を行います．\n",
        "\n",
        "まず，学習およびネットワークに関するパラメータを設定します．\n",
        "学習サンプル数，テストサンプル数および，1回の誤差を算出するデータ数（ミニバッチサイズ）と学習エポック数を定義します．\n",
        "\n",
        "また，中間層と出力層のユニット数を定義します．\n",
        "ここでは，入力層のユニット数`input_size`を学習データの次元，中間層のユニット数`hidden_size`128，`output_size`を10とします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ADnmU-pAzx"
      },
      "source": [
        "# 学習途中の精度を確認するための関数\n",
        "def multiclass_classification_accuracy(pred, true):\n",
        "    clf_res = np.argmax(pred, axis=1)\n",
        "    return np.sum(clf_res == true).astype(np.float32)\n",
        "\n",
        "# 学習中の誤差を確認するための関数\n",
        "def cross_entropy(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "# 学習パラメータ\n",
        "num_train_data = x_train.shape[0]\n",
        "num_test_data = x_test.shape[0]\n",
        "batch_size = 100\n",
        "epoch_num = 50\n",
        "learning_rate = 0.01\n",
        "\n",
        "# ネットワークパラメータ\n",
        "input_size = x_train.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNJWammAxGLM"
      },
      "source": [
        "### Batch Norm.なしのモデルの学習\n",
        "\n",
        "Batch Norm.無しのネットワークモデルを学習します．\n",
        "上で定義したパラメータを使用して，ネットワーク`MLP`を初期化し，学習を行います．\n",
        "学習プログラムの詳細については前回までの資料を参照してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r108LKmi3_sc"
      },
      "source": [
        "model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "epoch_list = []\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    sum_accuracy, sum_loss = 0.0, 0.0\n",
        "    \n",
        "    perm = np.random.permutation(num_train_data)\n",
        "    for i in range(0, num_train_data, batch_size):\n",
        "        x_batch = x_train[perm[i:i+batch_size]]\n",
        "        y_batch = y_train[perm[i:i+batch_size]]\n",
        "        \n",
        "        y = model.forward(x_batch)\n",
        "        sum_accuracy += multiclass_classification_accuracy(y, y_batch)\n",
        "        sum_loss += cross_entropy(y, y_batch)\n",
        "\n",
        "        model.backward(x_batch, y_batch)\n",
        "        model.update_parameters(lr=learning_rate)\n",
        "\n",
        "        iteration += 1\n",
        "    \n",
        "    print(\"epoch: {} (iteration: {}), mean loss: {}, mean accuracy: {}\".format(epoch, iteration,\n",
        "                                                               sum_loss / num_train_data,\n",
        "                                                               sum_accuracy / num_train_data))\n",
        "    \n",
        "    test_correct_count = 0\n",
        "    for i in range(num_test_data):\n",
        "        input = x_test[i:i+1]\n",
        "        label = y_test[i:i+1]\n",
        "        y = model.forward(input)\n",
        "        test_correct_count += multiclass_classification_accuracy(y, label)\n",
        "\n",
        "    # 学習途中のlossと精度の保存\n",
        "    epoch_list.append(epoch)\n",
        "    train_loss_list.append(sum_loss / num_train_data)\n",
        "    train_accuracy_list.append(sum_accuracy / num_train_data)\n",
        "    test_accuracy_list.append(test_correct_count / num_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiU4YvwbxMsT"
      },
      "source": [
        "### Batch Norm.ありのモデルの学習\n",
        "\n",
        "次にDropoutありのネットワークモデルの学習を行います．\n",
        "\n",
        "テストデータの認識精度を計算する際は，Batch Normalizationの設定を変更するために，`train_mode=False`と指定することに注意してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wi1lr_6pjmh"
      },
      "source": [
        "model_bn = MLPBatchNorm(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "epoch_list_bn = []\n",
        "train_loss_list_bn = []\n",
        "train_accuracy_list_bn = []\n",
        "test_accuracy_list_bn = []\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    \n",
        "    sum_accuracy = 0.0\n",
        "    sum_loss = 0.0\n",
        "    \n",
        "    perm = np.random.permutation(num_train_data)\n",
        "    for i in range(0, num_train_data, batch_size):\n",
        "        x_batch = x_train[perm[i:i+batch_size]]\n",
        "        y_batch = y_train[perm[i:i+batch_size]]\n",
        "        \n",
        "        y = model_bn.forward(x_batch)\n",
        "        sum_accuracy += multiclass_classification_accuracy(y, y_batch)\n",
        "        sum_loss += cross_entropy(y, y_batch)\n",
        "        \n",
        "        model_bn.backward(x_batch, y_batch)\n",
        "        model_bn.update_parameters(lr=learning_rate)\n",
        "\n",
        "        iteration += 1\n",
        "    \n",
        "    print(\"epoch: {} (iteration: {}), mean loss: {}, mean accuracy: {}\".format(epoch, iteration,\n",
        "                                                               sum_loss / num_train_data,\n",
        "                                                               sum_accuracy / num_train_data))\n",
        "    \n",
        "    test_correct_count = 0\n",
        "    for i in range(num_test_data):\n",
        "        input = x_test[i:i+1]\n",
        "        label = y_test[i:i+1]\n",
        "        y = model_bn.forward(input, train_mode=False)\n",
        "        test_correct_count += multiclass_classification_accuracy(y, label)\n",
        "\n",
        "    # 学習途中のlossと精度の保存\n",
        "    epoch_list_bn.append(epoch)\n",
        "    train_loss_list_bn.append(sum_loss / num_train_data)\n",
        "    train_accuracy_list_bn.append(sum_accuracy / num_train_data)\n",
        "    test_accuracy_list_bn.append(test_correct_count / num_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLh9-EqIuxxU"
      },
      "source": [
        "## 学習推移のグラフ化\n",
        "\n",
        "最後に2つのネットワークの学習推移を一つのグラフにまとめてプロットすることで，違いを確認します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR3iv_WYp7l-"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(epoch_list, train_loss_list, label='train loss')\n",
        "plt.plot(epoch_list_bn, train_loss_list_bn, label='train loss (w/ bn)')\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"loss\")      # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epoch_list, train_accuracy_list, label='train accuracy')\n",
        "plt.plot(epoch_list, test_accuracy_list, label='test accuracy')\n",
        "plt.plot(epoch_list_bn, train_accuracy_list_bn, label='train accuracy (w/ bn)')\n",
        "plt.plot(epoch_list_bn, test_accuracy_list_bn, label='test accuracy (w/ bn)')\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"accuracy\")  # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}