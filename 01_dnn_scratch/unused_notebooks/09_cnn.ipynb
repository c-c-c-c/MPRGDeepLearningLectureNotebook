{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "08_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoYBRwoQ4CPO"
      },
      "source": [
        "# 08：CNNを用いた画像認識\n",
        "---\n",
        "\n",
        "## 目的\n",
        "畳み込みニューラルネットワーク (Convolutional Neural Network; CNN) を用いてCIFAR10データセットに対する物体認識を行う．\n",
        "\n",
        "\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．\n",
        "`pickle`はPythonのリストや辞書などのオブジェクトを保存・読み込みを行うためのライブラリです．今回はCIFAR-10データセットを読み込むために使用します・"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7cHWI44CPP"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgpU02Pg4CPS"
      },
      "source": [
        "## データセットの読み込み\n",
        "実験に使用するCIFAR-10データセットを読み込みます．\n",
        "\n",
        "まず，CIFAR-10データセットをダウンロードします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30lAJ-LS4CPU"
      },
      "source": [
        "# CIFAR-10データセットのダウンロード\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
        "!tar zxvf cifar-10-python.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXhCR13e4CPX"
      },
      "source": [
        "次に，ダウンロードしたデータセットを読み込みます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyQB7loc4CPY"
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def load_cifar10_train():\n",
        "    data = np.empty((0, 3 * 32 * 32))\n",
        "    label = np.empty(0)\n",
        "    \n",
        "    for i in range(1, 6):\n",
        "        tmp_data = unpickle(\"cifar-10-batches-py/data_batch_%d\" % i)\n",
        "        data = np.vstack((data, tmp_data[b'data']))\n",
        "        label = np.hstack((label, tmp_data[b'labels']))\n",
        "    data = data.reshape(-1, 3, 32, 32).astype(np.float32)\n",
        "    label = label.astype(np.int64)\n",
        "    return data, label\n",
        "\n",
        "def load_cifar10_test():\n",
        "    tmp_data = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    data = tmp_data[b'data']\n",
        "    label = np.array(tmp_data[b'labels'])\n",
        "    data = data.reshape(-1, 3, 32, 32).astype(np.float32)\n",
        "    label = label.astype(np.int64)\n",
        "    return data, label\n",
        "\n",
        "x_train, y_train = load_cifar10_train()\n",
        "x_test, y_test = load_cifar10_test()\n",
        "\n",
        "# 正規化（0~1）\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep_qaVu44CPb"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "次に，CNNを定義します．\n",
        "\n",
        "まずはじめに，ネットワークの定義に必要な関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ3OfRGA4CPe"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x.shape)\n",
        "    grad[x > 0] = 1\n",
        "    return grad\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOHODRmT4CPg"
      },
      "source": [
        "`im2col`およびその逆の変換の`col2im`も定義を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlAfEDzS4CPg"
      },
      "source": [
        "def im2col(input_image, kernel_h, kernel_w, stride=1, padding=0):\n",
        "    n, c, h, w = input_image.shape\n",
        "    \n",
        "    dst_h = (h + 2 * padding - kernel_h) // stride + 1\n",
        "    dst_w = (w + 2 * padding - kernel_w) // stride + 1\n",
        "    \n",
        "    image = np.pad(input_image, [(0,0), (0,0), (padding, padding), (padding, padding)], 'constant')\n",
        "    col = np.zeros((n, c, kernel_h, kernel_w, dst_h, dst_w))\n",
        "    \n",
        "    for y in range(kernel_h):\n",
        "        y_max = y + stride * dst_h\n",
        "        for x in range(kernel_w):\n",
        "            x_max = x + stride * dst_w\n",
        "            col[:, :, y, x, :, :] = image[:, :, y:y_max:stride, x:x_max:stride]\n",
        "    \n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(n * dst_h * dst_w, -1)\n",
        "    return col\n",
        "\n",
        "def col2im(col, input_shape, kernel_h, kernel_w, stride=1, padding=0):\n",
        "    n, c, h, w = input_shape\n",
        "    out_h = (h + 2 * padding - kernel_h) // stride + 1\n",
        "    out_w = (w + 2 * padding - kernel_w) // stride + 1\n",
        "    col = col.reshape(n, out_h, out_w, c, kernel_h, kernel_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((n, c, h + 2 * padding + stride - 1, w + 2 * padding + stride - 1))\n",
        "    for y in range(kernel_h):\n",
        "        y_max = y + stride * out_h\n",
        "        for x in range(kernel_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, padding:h + padding, padding:w + padding]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvXj4KPd4CPi"
      },
      "source": [
        "畳み込みおよびプーリングの処理は煩雑になってしまうため，関数として定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jE3eO_-4CPj"
      },
      "source": [
        "def conv(x, w, b, stride=1, padding=0):\n",
        "    FN, C, FH, FW = w.shape\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    out_h = 1 + int((H + 2 * padding - FH) / stride)\n",
        "    out_w = 1 + int((W + 2 * padding - FW) / stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, stride, padding)\n",
        "    col_w = w.reshape(FN, -1).T\n",
        "\n",
        "    out = np.dot(col, col_w) + b\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "    \n",
        "    return out, col, col_w\n",
        "\n",
        "def conv_grad(dout, x, col, col_w, w, b, stride=1, padding=0):\n",
        "    FN, C, FH, FW = w.shape\n",
        "    dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
        "    \n",
        "    grad_b = np.sum(dout, axis=0)\n",
        "    grad_w = np.dot(col.T, dout)\n",
        "    grad_w = grad_w.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "    \n",
        "    dcol = np.dot(dout, col_w.T)\n",
        "    dx = col2im(dcol, x.shape, FH, FW, stride, padding)\n",
        "\n",
        "    return dx, grad_w, grad_b\n",
        "    \n",
        "def maxpool(x, pool_size=2, stride=2, padding=0):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - pool_size) / stride)\n",
        "    out_w = int(1 + (W - pool_size) / stride)\n",
        "    \n",
        "    col = im2col(x, pool_size, pool_size, stride, padding)\n",
        "    col = col.reshape(-1, pool_size * pool_size)\n",
        "    \n",
        "    arg_max = np.argmax(col, axis=1)\n",
        "    out = np.max(col, axis=1)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out, arg_max\n",
        "\n",
        "def maxpool_grad(dout, x, arg_max, p_size=2, stride=2, padding=0):\n",
        "    dout = dout.transpose(0, 2, 3, 1)\n",
        "    pool_size = p_size * p_size\n",
        "\n",
        "    dmax = np.zeros((dout.size, pool_size))\n",
        "    dmax[np.arange(arg_max.size), arg_max.flatten()] = dout.flatten()\n",
        "    dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "\n",
        "    dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "    dx = col2im(dcol, x.shape, p_size, p_size, stride, padding)\n",
        "\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xxRWeSR4CPl"
      },
      "source": [
        "次に，上で定義した関数を用いてネットワークを定義します．\n",
        "ここでは，畳み込み層，中間層，出力層から構成されるCNNとします．\n",
        "\n",
        "入力画像のチャンネル数と，畳み込みのカーネルサイズ，畳み込みのカーネル数を引数として指定します．\n",
        "さらに，中間層，出力層のユニット数は引数として与え，それぞれ`hidden_size`, `output_size`とします．\n",
        "そして，`__init__`関数を用いて，ネットワークのパラメータを初期化します．\n",
        "`w1`, `w2`, `w3`は各層の重みで，`b1`, `b2`, `b3`はバイアスを表しています．\n",
        "重みは`randn`関数で，標準正規分布に従った乱数で生成した値を保有する配列を生成します．\n",
        "バイアスは`zeros`関数を用いて，要素が全て0の配列を生成します．\n",
        "\n",
        "そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n",
        "\n",
        "次に，`backward`関数ではパラメータの更新量を計算します．\n",
        "まず，ネットワークの出力結果と教師ラベルから，誤差`dy`を算出します．\n",
        "この時，教師ラベルをone-hotベクトルへ変換し，各ユニットの出力との差を取ることで，`dy`を計算しています．\n",
        "その後，連鎖律に基づいて，出力層から順番に勾配を計算していきます．\n",
        "このとき，パラメータの更新量を`self.grads`へ保存しておきます．\n",
        "\n",
        "最後に`update_parameters`関数で，更新量をもとにパラメータの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uudr8bll4CPl"
      },
      "source": [
        "class CNN:\n",
        "    \n",
        "    def __init__(self, n_channels=3, filter_size=3, num_kernel=64, hidden_size=128, output_size=10, w_std=0.01):\n",
        "        \n",
        "        # convolutional layer\n",
        "        self.w1 = w_std * np.random.randn(num_kernel, n_channels, filter_size, filter_size)\n",
        "        self.b1 = np.zeros(num_kernel)\n",
        "        # hidden layer\n",
        "        pooled_feature_size = int(num_kernel * (32 / 2) * (32 / 2))\n",
        "        self.w2 = w_std * np.random.randn(pooled_feature_size, hidden_size)\n",
        "        self.b2 = np.zeros(hidden_size)\n",
        "        # output layer\n",
        "        self.w3 = w_std * np.random.randn(hidden_size, output_size)\n",
        "        self.b3 = np.zeros(output_size)\n",
        "        # dict. for gradients\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h1, self.h1_col, self.h1_col_w = conv(x, self.w1, self.b1, stride=1, padding=1)\n",
        "        self.h2 = relu(self.h1)\n",
        "        self.h3, self.h3_argmax = maxpool(self.h2, pool_size=2, stride=2, padding=0)\n",
        "        self.h4 = np.dot(self.h3.reshape(self.h2.shape[0], -1), self.w2) + self.b2\n",
        "        self.h5 = relu(self.h4)\n",
        "        self.h6 = np.dot(self.h5, self.w3) + self.b3\n",
        "        self.y = softmax(self.h6)\n",
        "        return self.y\n",
        "        \n",
        "    def backward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # backward #####\n",
        "        self.grads = {}\n",
        "        \n",
        "        t = np.identity(10)[t]\n",
        "        \n",
        "        dy = (self.y - t) / batch_size\n",
        "        \n",
        "        # output layer\n",
        "        d_h5 = np.dot(dy, self.w3.T)\n",
        "        self.grads['w3'] = np.dot(self.h5.T, dy)\n",
        "        self.grads['b3'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        # relu\n",
        "        d_h4 = relu_grad(self.h4) * d_h5\n",
        "        \n",
        "        # hidden layer\n",
        "        d_h3 = np.dot(d_h4, self.w2.T)\n",
        "        self.grads['w2'] = np.dot(self.h3.T, d_h4).reshape(self.w2.shape)\n",
        "        self.grads['b2'] = np.sum(d_h4, axis=0)\n",
        "        \n",
        "        # maxpool\n",
        "        d_h3 = d_h3.reshape(self.h3.shape)\n",
        "        d_h2 = maxpool_grad(d_h3, self.h2, self.h3_argmax, p_size=2, stride=2, padding=0)\n",
        "\n",
        "        # relu\n",
        "        d_h1 = relu_grad(self.h1) * d_h2\n",
        "\n",
        "        # convolution\n",
        "        _, self.grads['w1'], self.grads['b1'] = conv_grad(d_h1, x, self.h1_col, self.h1_col_w, self.w1, self.b2, stride=1, padding=1)\n",
        "\n",
        "    def update_parameters(self, lr=0.1): \n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']\n",
        "        self.w3 -= lr * self.grads['w3']\n",
        "        self.b3 -= lr * self.grads['b3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p58_NRzJ4CPn"
      },
      "source": [
        "## ネットワークの作成と学習の準備\n",
        "\n",
        "読み込んだCIFAR10データセットと作成したネットワークを用いて，学習を行います．\n",
        "\n",
        "1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を10とします．\n",
        "\n",
        "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
        "各更新において，学習用データと教師データをそれぞれ`x_batch`と`y_batch`とします．\n",
        "学習モデルに`x_batch`を与えて，`h`を取得します．\n",
        "取得した`h`は精度および誤差を算出するための関数へと入力され，値を保存します．\n",
        "そして，誤差を`backward`関数で逆伝播し，`update_parameters`でネットワークの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbEJ-xH74CPn"
      },
      "source": [
        "model = CNN(n_channels=3, filter_size=3, num_kernel=64, hidden_size=256, output_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ituZSlEU4CPp"
      },
      "source": [
        "## 学習\n",
        "学習したネットワークを用いて，テストデータに対する認識律の確認を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7wCwLWr4CPq"
      },
      "source": [
        "def softmax_cross_entropy(x, t):\n",
        "    y = softmax(x)\n",
        "    return cross_entropy(y, t)\n",
        "\n",
        "def multiclass_classification_accuracy(pred, true):\n",
        "    clf_res = np.argmax(pred, axis=1)\n",
        "    return np.sum(clf_res == true).astype(np.float32)\n",
        "\n",
        "num_train_data = x_train.shape[0]\n",
        "batch_size = 100\n",
        "epoch_num = 10\n",
        "\n",
        "iteration = 1\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    sum_accuracy = 0.0\n",
        "    sum_loss= 0.0\n",
        "    \n",
        "    perm = np.random.permutation(num_train_data)\n",
        "    for i in range(0, num_train_data, batch_size):\n",
        "        x_batch = x_train[perm[i:i+batch_size]]\n",
        "        y_batch = y_train[perm[i:i+batch_size]]\n",
        "        \n",
        "        h = model.forward(x_batch)\n",
        "        sum_accuracy += multiclass_classification_accuracy(h, y_batch)\n",
        "        loss = softmax_cross_entropy(h, y_batch)\n",
        "        sum_loss += loss\n",
        "        \n",
        "        model.backward(x_batch, y_batch)\n",
        "        model.update_parameters(lr=0.1)\n",
        "        \n",
        "        if iteration % 100 == 0:\n",
        "            print(\"iteration: {}, loss: {}\".format(iteration, loss))\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed time: {}\".format(epoch,\n",
        "                                                                                 sum_loss / num_train_data,\n",
        "                                                                                 sum_accuracy / num_train_data,\n",
        "                                                                                 time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBh6HKN74CPs"
      },
      "source": [
        "## テスト\n",
        "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk1Eufsa4CPs"
      },
      "source": [
        "count = 0\n",
        "num_test_data = x_test.shape[0]\n",
        "\n",
        "for i in range(num_test_data):\n",
        "    x = np.array([x_test[i]], dtype=np.float32)\n",
        "    t = y_test[i]\n",
        "    y = model.forward(x)\n",
        "    pred = np.argmax(y.flatten())\n",
        "    \n",
        "    if pred == t:\n",
        "        count += 1\n",
        "\n",
        "print(\"test accuracy: {}\".format(count / num_test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4KESHAtVQrL"
      },
      "source": [
        "## GPUを用いたCNNの計算\n",
        "\n",
        "上のプログラムでは，Numpy配列（CPU）を使用してCNNを計算しており，計算に多くの時間を要していました．\n",
        "高速化するために，一般的にGPUを使用して計算が行われます．\n",
        "以下では，GPUを使用したCNNの学習の一例として，PyTorchを用いたネットワークの学習プログラムを紹介します．\n",
        "\n",
        "なお，ここではPyTorchに関する詳細は割愛しますので，中級編にて詳しい説明やより高度なプログラムを提供します．\n",
        "\n",
        "\n",
        "***\n",
        "### GPUの設定確認・変更\n",
        "\n",
        "以下ではPyTorchとGPUを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "上で定義したプログラムを全てPyTorchで記述すると以下のようになります．PyTorchをはじめとする深層学習のフレームワーク（TensorFlowなど）では，CNNや全結合層，プーリング，活性化関数などの一般的な演算を行うためのクラスがすでに用意されており，それらを呼び出すだけで使用することができます．\n",
        "そのほかにも，最適化手法や一般的な画像認識データセットを使うための関数やクラスも用意されており，以下のように簡単に定義することが可能です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VImPw4OVQzZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "# ネットワークの定義\n",
        "class CNNTorch(nn.Module):\n",
        "    def __init__(self, n_channels=3, filter_size=3, num_kernel=64, hidden_size=128, output_size=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(n_channels, num_kernel, filter_size, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        pooled_feature_size = int(num_kernel * (32 / 2) * (32 / 2))\n",
        "        self.fc1 = nn.Linear(pooled_feature_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.relu(self.conv1(x))\n",
        "        h = self.maxpool(h)\n",
        "        h = torch.flatten(h, start_dim=1)\n",
        "        h = self.relu(self.fc1(h))\n",
        "        h = self.fc2(h)\n",
        "        return h\n",
        "\n",
        "# ネットワーク・最適化手法の初期化\n",
        "pt_model = CNNTorch(n_channels=3, filter_size=3, num_kernel=64, hidden_size=128, output_size=10)\n",
        "pt_model = pt_model.cuda()\n",
        "optimizer = torch.optim.SGD(pt_model.parameters(), lr=0.01)\n",
        "\n",
        "# 学習用誤差関数の準備\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.cuda()\n",
        "\n",
        "# CIFAR10データセットの用意\n",
        "train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True,\n",
        "                                          transform=torchvision.transforms.ToTensor(), download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False,\n",
        "                                         transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 100\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "pt_model.train()\n",
        "\n",
        "# 学習\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for image, label in train_loader:\n",
        "        \n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        y = pt_model(image)\n",
        "\n",
        "        loss = criterion(y, label)\n",
        "        \n",
        "        pt_model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item()\n",
        "        \n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "        \n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch,\n",
        "                                                               sum_loss / n_iter,\n",
        "                                                               count.item() / len(train_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6KSQusZZNjW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}