{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "11_parameter_search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQEJLU_C3_sL"
      },
      "source": [
        "# 11：ハイパーパラメータの探索と検証データ\n",
        "---\n",
        "## 目的\n",
        "多層パーセプトロン (Multi Layer Perceptoron; MLP) を用いたMNISTデータセット文字認識を通じて，ハイパーパラメータの探索・検証および検証データの役割について理解する．\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vZoiRR03_sL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VIul2gL3_sO"
      },
      "source": [
        "## データセットのダウンロードと読み込み\n",
        "\n",
        "まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DDcpz6P3_sO"
      },
      "source": [
        "!wget -q http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXTzEns3_sQ"
      },
      "source": [
        "次に，ダウンロードしたファイルからデータを読み込みます．詳細は前回までのプログラムを確認してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGEMFDLI3_sR"
      },
      "source": [
        "# load images\n",
        "with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwZJgT3Q0Ix"
      },
      "source": [
        "## 検証データの作成\n",
        "\n",
        "ネットワークの学習やモデルの定義には多くのハイパーパラメータが存在します．例えば，\n",
        "* ネットワークのハイパーパラメータ\n",
        "  * 中間層の層数\n",
        "  * 中間層のユニット数\n",
        "  * Dropoutを適用するかどうか（Dropoutを用いる場所やdropout ratio）\n",
        "  * Batch Normalizationを適用するかどうか\n",
        "\n",
        "* 学習のハイパーパラメータ\n",
        "  * 学習率\n",
        "  * 学習回数\n",
        "  * ミニバッチサイズ\n",
        "\n",
        "などが挙げられます．\n",
        "\n",
        "最適なハイパーパラメータを決定するために使用するデータを**検証データ（validation data）**と呼びます．\n",
        "\n",
        "MNISTデータセットには学習データ（train data）とテストデータ（test data）しか存在しません．このように，専用の検証データが存在しないデータセットや個人が作成したオリジナルのデータセットでは，学習データの一部を検証データとして使用します．\n",
        "\n",
        "以下では，MNISTデータセットの学習データを分割し，学習および検証データを作成します．\n",
        "まず，検証データに使用するデータの割合を`validation_ratio`として定義します．\n",
        "今回は学習データの20%を検証データとして使用することとし，0.2と指定します．\n",
        "この割合に基づいて，学習データの20%となるサンプル数を`n_val`として計算します．\n",
        "\n",
        "その後，学習および検証データになるよう，データを分割します．\n",
        "\n",
        "実行すると，60000枚の学習サンプルの20%である12000枚が検証データ，残りの80%の48000枚が学習データになるよう分割されていることがわかります．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf2SPjJmQ0NK"
      },
      "source": [
        "validation_ratio = 0.2   # 検証に使用するデータの割合\n",
        "n_train_original = x_train.shape[0]\n",
        "n_val = int(n_train_original * validation_ratio)\n",
        "\n",
        "# 検証データ\n",
        "x_val = x_train[0:n_val]\n",
        "y_val = y_train[0:n_val]\n",
        "\n",
        "# 学習データ\n",
        "x_train = x_train[n_val:]\n",
        "y_train = y_train[n_val:]\n",
        "\n",
        "print(\"train      :\", x_train.shape, y_train.shape)\n",
        "print(\"validation :\", x_val.shape, y_val.shape)\n",
        "print(\"test       :\", x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlOaqBgd3_sV"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "次に，ニューラルネットワーク（多層パーセプトロン）を定義します．\n",
        "\n",
        "まずはじめに，ネットワークの定義に必要な関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVfJLTjw3_sV"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x.shape)\n",
        "    grad[x > 0] = 1\n",
        "    return grad\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        # x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    # x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkdkqRjq3_sX"
      },
      "source": [
        "次に，上で定義した関数を用いてネットワークを定義します．\n",
        "ここでは，入力層，中間層，出力層から構成される多層パーセプトロンとします．ネットワーク定義についての詳細は割愛しますので，前回までの資料を参照してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkujL9rz3_sX"
      },
      "source": [
        "class MLP:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, act_func='sigmoid', w_std=0.01):\n",
        "        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = w_std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.b2 = np.zeros(hidden_size)\n",
        "        self.w3 = w_std * np.random.randn(hidden_size, output_size)\n",
        "        self.b3 = np.zeros(output_size)\n",
        "\n",
        "        # 使用する活性化関数の選択\n",
        "        if act_func == 'sigmoid':\n",
        "            self.act = sigmoid\n",
        "            self.act_grad = sigmoid_grad\n",
        "        elif act_func == 'relu':\n",
        "            self.act = relu\n",
        "            self.act_grad = relu_grad\n",
        "        else:\n",
        "            print(\"ERROR\")\n",
        "\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h1 = np.dot(x, self.w1) + self.b1\n",
        "        self.h2 = self.act(self.h1)\n",
        "        self.h3 = np.dot(self.h2, self.w2) + self.b2\n",
        "        self.h4 = self.act(self.h3)\n",
        "        self.h5 = np.dot(self.h4, self.w3) + self.b3\n",
        "        self.y = softmax(self.h5)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "        self.grads = {}\n",
        "        \n",
        "        t = np.identity(10)[t]\n",
        "        dy = (self.y - t) / batch_size\n",
        "\n",
        "        self.grads['w3'] = np.dot(self.h4.T, dy)\n",
        "        self.grads['b3'] = np.sum(dy, axis=0)\n",
        "\n",
        "        d_h4 = np.dot(dy, self.w3.T)\n",
        "        d_h3 = self.act_grad(self.h3) * d_h4\n",
        "        self.grads['w2'] = np.dot(self.h2.T, d_h3)\n",
        "        self.grads['b2'] = np.sum(d_h3, axis=0)\n",
        "        \n",
        "        d_h2 = np.dot(d_h3, self.w2.T)\n",
        "        d_h1 = self.act_grad(self.h1) * d_h2\n",
        "        self.grads['w1'] = np.dot(x.T, d_h1)\n",
        "        self.grads['b1'] = np.sum(d_h1, axis=0)\n",
        "        \n",
        "    def update_parameters(self, lr=0.1):\n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']  \n",
        "        self.w3 -= lr * self.grads['w3']\n",
        "        self.b3 -= lr * self.grads['b3']  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZH7MDAX3_sZ"
      },
      "source": [
        "## 学習およびハイパーパラメータ探索の準備\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCiEl8ZJWS-G"
      },
      "source": [
        "\n",
        "### 探索時に共通するパラメータの設定\n",
        "\n",
        "まずはじめに，探索時に共通するパラメータを定義します．\n",
        "具体的には，入力層のユニット数`input_size`および出力層のユニット数`output_size`は，探索時には共通であるため，事前に定義しておきます．\n",
        "また，学習，検証データのサンプル数もすでに決まっているため，変数として定義しておきます．\n",
        "\n",
        "また，精度および誤差を算出するための関数（`multiclass_classification_accuracy`, `cross_entropy`）も共通して使用するため，ここで事前に定義を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkW6ljLN3_sa"
      },
      "source": [
        "input_size = x_train.shape[1]\n",
        "output_size = 10\n",
        "\n",
        "num_train_data = x_train.shape[0]\n",
        "num_val_data = x_val.shape[0]\n",
        "num_test_data = x_test.shape[0]\n",
        "\n",
        "# 学習途中の精度を確認するための関数\n",
        "def multiclass_classification_accuracy(pred, true):\n",
        "    clf_res = np.argmax(pred, axis=1)\n",
        "    return np.sum(clf_res == true).astype(np.float32)\n",
        "\n",
        "# 学習中の誤差を確認するための関数\n",
        "def cross_entropy(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnb7UhbJWRZD"
      },
      "source": [
        "### 探索するパラメータの設定\n",
        "\n",
        "次に，検証データを用いて最適な値を求めたいパラメータを定義します．\n",
        "\n",
        "ここでは，中間層のユニット数の最適な値を`[32, 64, 128, 256]`の中から求めるものとし，他のパラメータは固定します．\n",
        "\n",
        "また，パラメータの探索には複数回の学習を行う必要があり計算に多くの時間を要します．\n",
        "ここでは，実習中の計算時間削減のために，学習エポック数を20と小さい値に設定します．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjO2sUXzYNP6"
      },
      "source": [
        "hidden_size = 64\n",
        "hidden_list = [32, 64, 128, 256]\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "learning_rate = 0.01\n",
        "# learning_rate_list = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
        "\n",
        "epoch_num = 20\n",
        "\n",
        "learning_rate = 0.01\n",
        "activation_function = 'relu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaRjNUOo3_sb"
      },
      "source": [
        "## ハイパーパラメータ探索\n",
        "\n",
        "ハイパーパラメータ探索を行います．\n",
        "\n",
        "まず，各パラメータでの結果を保存するためのリスト`param_search_list`を作成します．\n",
        "\n",
        "次に，for文を用いて探索したいハイパーパラメータを一つづつ指定し，ネットワークの学習と検証データでの精度を求めます．\n",
        "ネットワークの学習プログラムは前回までのもの同様のため詳細は割愛します．\n",
        "\n",
        "学習が終了すると，探索したパラメータの値やその時の誤差および精度の推移のデータを辞書型オブジェクト`result`に格納し，それを`param_search_list`に保存します．\n",
        "\n",
        "これを繰り返すことで，各パラメータの値を用いた場合の精度や学習推移を確認比較することが可能となり，より精度の高いネットワークを構築するためのあたりをつけることができます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r108LKmi3_sc"
      },
      "source": [
        "param_search_list = []\n",
        "\n",
        "for hidden_size in hidden_list:\n",
        "    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, act_func=activation_function)\n",
        "\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    val_accuracy_list = []\n",
        "\n",
        "    iteration = 0\n",
        "    for epoch in range(1, epoch_num + 1):\n",
        "        sum_accuracy, sum_loss = 0.0, 0.0\n",
        "        \n",
        "        perm = np.random.permutation(num_train_data)\n",
        "        for i in range(0, num_train_data, batch_size):\n",
        "            x_batch = x_train[perm[i:i+batch_size]]\n",
        "            y_batch = y_train[perm[i:i+batch_size]]\n",
        "            \n",
        "            y = model.forward(x_batch)\n",
        "            sum_accuracy += multiclass_classification_accuracy(y, y_batch)\n",
        "            sum_loss += cross_entropy(y, y_batch)\n",
        "            \n",
        "            model.backward(x_batch, y_batch)\n",
        "            model.update_parameters(lr=learning_rate)\n",
        "\n",
        "            iteration += 1\n",
        "        \n",
        "        # 検証データでの精度の確認\n",
        "        val_correct_count = 0\n",
        "        for i in range(num_val_data):\n",
        "            input = x_val[i:i+1]\n",
        "            label = y_val[i:i+1]\n",
        "            y = model.forward(input)\n",
        "            \n",
        "            val_correct_count += multiclass_classification_accuracy(y, label)\n",
        "\n",
        "        # 学習途中のlossと精度の保存\n",
        "        epoch_list.append(epoch)\n",
        "        train_loss_list.append(sum_loss / num_train_data)\n",
        "        train_accuracy_list.append(sum_accuracy / num_train_data)\n",
        "        val_accuracy_list.append(val_correct_count / num_val_data)\n",
        "\n",
        "        # print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch,\n",
        "        #                                                           sum_loss / num_train_data,\n",
        "        #                                                           sum_accuracy / num_train_data))\n",
        "\n",
        "    # 探索した結果の保存\n",
        "    result = {'lr': learning_rate, 'n_hidden': hidden_size,\n",
        "              'val_acc': val_accuracy_list,\n",
        "              'train_acc': train_accuracy_list,\n",
        "              'train_loss': train_loss_list}\n",
        "    param_search_list.append(result)\n",
        "    print(result['lr'], result['n_hidden'], result['train_acc'][-1], result['val_acc'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY2Su0H2c-Rq"
      },
      "source": [
        "## 探索結果の確認\n",
        "\n",
        "`param_search_list`に保存しておいた各ハイパーパラメータに対する結果を確認します．\n",
        "\n",
        "まず，各結果の数値をprintします．\n",
        "その後，各パラメータでの学習推移（検証データ）をプロットし比較します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FTUmHXc-aF"
      },
      "source": [
        "# 学習結果の表示\n",
        "for ps in param_search_list:\n",
        "    print(\"lr:\", ps['lr'],\n",
        "          \"hidden size\", ps['n_hidden'],\n",
        "          \"train accuracy:\", ps['train_acc'][-1],\n",
        "          \"validation accuracy:\", ps['val_acc'][-1])\n",
        "\n",
        "# グラフプロット\n",
        "plt.figure()\n",
        "for ps in param_search_list:\n",
        "    plt.plot(ps['val_acc'], label='hidden=%d' % ps['n_hidden'])\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"accuracy\")  # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJiDB3Is3_sf"
      },
      "source": [
        "## 課題\n",
        "1. 他のパラメータについても探索をして最適な値を見つけよう\n",
        "2. 複数種類のパラメータの最適な組み合わせを求めよう"
      ]
    }
  ]
}