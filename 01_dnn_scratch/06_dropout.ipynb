{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"06_dropout.ipynb","provenance":[{"file_id":"https://github.com/machine-perception-robotics-group/GoogleColabNotebooks/blob/master/JDLA_lecture_notebook/05_neural_network_multinoulli.ipynb","timestamp":1600426882515}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VQEJLU_C3_sL"},"source":["# 06：正則化（Dropout）\n","\n","---\n","## 目的\n","多層パーセプトロン (Multi Layer Perceptoron; MLP) を用いたMNISTデータセットの認識において，ネットワークの正則化（Dropout）について理解する．\n","\n","## モジュールのインポート\n","プログラムの実行に必要なモジュールをインポートします．"]},{"cell_type":"code","metadata":{"id":"7vZoiRR03_sL"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import gzip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VIul2gL3_sO"},"source":["## データセットのダウンロードと読み込みと学習サンプルの削減\n","\n","\n","まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"]},{"cell_type":"code","metadata":{"id":"8DDcpz6P3_sO"},"source":["!wget -q http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RXTzEns3_sQ"},"source":["次に，ダウンロードしたファイルからデータを読み込みます．詳細は前回までのプログラムを確認してください．\n","\n","\n","ここで，学習データを削減します．\n","今回は1000サンプルになるように，先頭から1000個の学習データとラベルを取得します．"]},{"cell_type":"code","metadata":{"id":"qGEMFDLI3_sR"},"source":["# load images\n","with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n","    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n","x_train = x_train.reshape(-1, 784)\n","\n","with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n","    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n","x_test = x_test.reshape(-1, 784)\n","\n","with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n","    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n","\n","with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n","    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n","\n","# 学習サンプルの削減\n","x_train = x_train[0:1000, :]\n","y_train = y_train[0:1000]\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlOaqBgd3_sV"},"source":["## ネットワークモデルの定義\n","次に，ニューラルネットワーク（多層パーセプトロン）を定義します．\n","\n","まずはじめに，ネットワークの定義に必要な関数を定義します．"]},{"cell_type":"code","metadata":{"id":"cVfJLTjw3_sV"},"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_grad(x):\n","    grad = np.zeros(x.shape)\n","    grad[x > 0] = 1\n","    return grad\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","    else:\n","        return np.exp(x) / np.sum(np.exp(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PkdkqRjq3_sX"},"source":["次に，上で定義した関数を用いてネットワークを定義します．\n","ここでは，Dropoutを適用する場合としない場合の2種類の7層の多層パーセプトロンを定義します．\n","\n","まず，Dropoutを適用しないネットワーク`MLP`を定義します．詳細については前回までの資料を確認してください．\n"]},{"cell_type":"code","metadata":{"id":"VC4dXsRSPp6A"},"source":["class MLP:\n","\n","    def __init__(self, input_size, hidden_size, output_size, w_std=0.05):\n","        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n","        self.b1 = np.zeros(hidden_size)\n","        self.w2 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b2 = np.zeros(hidden_size)\n","        self.w3 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b3 = np.zeros(hidden_size)\n","        self.w4 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b4 = np.zeros(hidden_size)\n","        self.w5 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b5 = np.zeros(hidden_size)\n","        self.w6 = w_std * np.random.randn(hidden_size, output_size)\n","        self.b6 = np.zeros(output_size)\n","\n","        self.act = relu\n","        self.act_grad = relu_grad\n","\n","        self.grads = {}\n","\n","    def forward(self, x, train_mode=False):\n","        self.h1 = np.dot(x, self.w1) + self.b1\n","        self.h2 = self.act(self.h1)\n","        self.h3 = np.dot(self.h2, self.w2) + self.b2\n","        self.h4 = self.act(self.h3)\n","        self.h5 = np.dot(self.h4, self.w3) + self.b3\n","        self.h6 = self.act(self.h5)\n","        self.h7 = np.dot(self.h6, self.w4) + self.b4\n","        self.h8 = self.act(self.h7)\n","        self.h9 = np.dot(self.h8, self.w5) + self.b5\n","        self.h10 = self.act(self.h9)\n","        self.h11 = np.dot(self.h10, self.w6) + self.b6\n","        self.y = softmax(self.h11)\n","        return self.y\n","\n","    def backward(self, x, t):\n","        batch_size = x.shape[0]\n","        self.grads = {}\n","        \n","        t = np.identity(10)[t]\n","        dy = (self.y - t) / batch_size\n","        self.grads['w6'] = np.dot(self.h10.T, dy)\n","        self.grads['b6'] = np.sum(dy, axis=0)\n","\n","        d_h10 = np.dot(dy, self.w6.T)\n","        d_h9 = self.act_grad(self.h9) * d_h10\n","        self.grads['w5'] = np.dot(self.h8.T, d_h9)\n","        self.grads['b5'] = np.sum(d_h9, axis=0)\n","        \n","        d_h8 = np.dot(d_h9, self.w5.T)\n","        d_h7 = self.act_grad(self.h7) * d_h8\n","        self.grads['w4'] = np.dot(self.h6.T, d_h7)\n","        self.grads['b4'] = np.sum(d_h7, axis=0)\n","\n","        d_h6 = np.dot(d_h7, self.w4.T)\n","        d_h5 = self.act_grad(self.h5) * d_h6\n","        self.grads['w3'] = np.dot(self.h4.T, d_h5)\n","        self.grads['b3'] = np.sum(d_h5, axis=0)\n","\n","        d_h4 = np.dot(d_h5, self.w3.T)\n","        d_h3 = self.act_grad(self.h3) * d_h4\n","        self.grads['w2'] = np.dot(self.h2.T, d_h3)\n","        self.grads['b2'] = np.sum(d_h3, axis=0)\n","\n","        d_h2 = np.dot(d_h3, self.w2.T)\n","        d_h1 = self.act_grad(self.h1) * d_h2\n","        self.grads['w1'] = np.dot(x.T, d_h1)\n","        self.grads['b1'] = np.sum(d_h1, axis=0)\n","        \n","    def update_parameters(self, lr=0.1):\n","        self.w1 -= lr * self.grads['w1']\n","        self.b1 -= lr * self.grads['b1']\n","        self.w2 -= lr * self.grads['w2']\n","        self.b2 -= lr * self.grads['b2']  \n","        self.w3 -= lr * self.grads['w3']\n","        self.b3 -= lr * self.grads['b3']\n","        self.w4 -= lr * self.grads['w4']\n","        self.b4 -= lr * self.grads['b4']  \n","        self.w5 -= lr * self.grads['w5']\n","        self.b5 -= lr * self.grads['b5']  \n","        self.w6 -= lr * self.grads['w6']\n","        self.b6 -= lr * self.grads['b6'] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7alocwTLXh3"},"source":["次に，Dropoutを適用する場合のネットワーク`MLPDropout`を定義します．\n","\n","\n","`__init__`関数でネットワークの初期化を行う際に，`dropout_ratio`という変数を定義します．この変数によってdropoutを適用する割合(`0 ~ 1`)を指定します．\n","\n","そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n","ここでは最終層手前の全結合層後にdropoutを適用します．\n","まず，Dropoutへ入力されるデータの配列のサイズを`_batch_size`, `_feature_dims`として取得します．\n","そして，入力データの次元と同じサイズの乱数で初期化された配列を`np.random.rand`で生成し，`dropout_ratio`を閾値として，マスク`dropout_mask`を作成します．\n","この時，`dropout_mask`の各要素には閾値処理により`[True, False]`が格納されています．\n","そのマスクを入力データと掛けることで，`False`の部分の値が0となったデータを出力します．\n","\n","次に，`backward`関数ではパラメータの更新量を計算します．\n","Dropout部分では，上の`forward`関数を計算する際に保存しておいた`self.dropout_mask`と勾配を掛け合わせることで，Dropoutを考慮した勾配を計算することができます．"]},{"cell_type":"code","metadata":{"id":"C2xSRwq4Ym7y"},"source":["class MLPDropout:\n","\n","    def __init__(self, input_size, hidden_size, output_size, dropout_ratio=0.5, w_std=0.05):\n","        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n","        self.b1 = np.zeros(hidden_size)\n","        self.w2 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b2 = np.zeros(hidden_size)\n","        self.w3 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b3 = np.zeros(hidden_size)\n","        self.w4 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b4 = np.zeros(hidden_size)\n","        self.w5 = w_std * np.random.randn(hidden_size, hidden_size)\n","        self.b5 = np.zeros(hidden_size)\n","        self.w6 = w_std * np.random.randn(hidden_size, output_size)\n","        self.b6 = np.zeros(output_size)\n","\n","        self.act = relu\n","        self.act_grad = relu_grad\n","\n","        # dropoutのパラメータ\n","        self.dropout_ratio = dropout_ratio\n","\n","        self.grads = {}\n","\n","    def forward(self, x, train_mode=True):\n","        self.h1 = np.dot(x, self.w1) + self.b1\n","        self.h2 = self.act(self.h1)\n","        self.h3 = np.dot(self.h2, self.w2) + self.b2\n","        self.h4 = self.act(self.h3)\n","        self.h5 = np.dot(self.h4, self.w3) + self.b3\n","        self.h6 = self.act(self.h5)\n","        self.h7 = np.dot(self.h6, self.w4) + self.b4\n","        self.h8 = self.act(self.h7)\n","        self.h9 = np.dot(self.h8, self.w5) + self.b5\n","        self.h10 = self.act(self.h9)\n","\n","        # dropoutの適用 =============\n","        if train_mode:\n","          self.dropout_mask = np.random.rand(self.h10.shape[1]) > self.dropout_ratio\n","          self.h_d = self.h10 * self.dropout_mask\n","        else:\n","          self.h_d = self.h10\n","        # ==========================\n","\n","        self.h11 = np.dot(self.h_d, self.w6) + self.b6\n","        self.y = softmax(self.h11)\n","        return self.y\n","\n","    def backward(self, x, t):\n","        batch_size = x.shape[0]\n","        self.grads = {}\n","        \n","        t = np.identity(10)[t]\n","        dy = (self.y - t) / batch_size\n","        self.grads['w6'] = np.dot(self.h10.T, dy)\n","        self.grads['b6'] = np.sum(dy, axis=0)\n","\n","        d_h_d = np.dot(dy, self.w6.T)\n","\n","        # dropout部分の勾配計算 ======\n","        d_h10 = d_h_d * self.dropout_mask\n","        # ==========================\n","      \n","        d_h9 = self.act_grad(self.h9) * d_h10\n","        self.grads['w5'] = np.dot(self.h8.T, d_h9)\n","        self.grads['b5'] = np.sum(d_h9, axis=0)\n","        \n","        d_h8 = np.dot(d_h9, self.w5.T)\n","        d_h7 = self.act_grad(self.h7) * d_h8\n","        self.grads['w4'] = np.dot(self.h6.T, d_h7)\n","        self.grads['b4'] = np.sum(d_h7, axis=0)\n","\n","        d_h6 = np.dot(d_h7, self.w4.T)\n","        d_h5 = self.act_grad(self.h5) * d_h6\n","        self.grads['w3'] = np.dot(self.h4.T, d_h5)\n","        self.grads['b3'] = np.sum(d_h5, axis=0)\n","\n","        d_h4 = np.dot(d_h5, self.w3.T)\n","        d_h3 = self.act_grad(self.h3) * d_h4\n","        self.grads['w2'] = np.dot(self.h2.T, d_h3)\n","        self.grads['b2'] = np.sum(d_h3, axis=0)\n","\n","        d_h2 = np.dot(d_h3, self.w2.T)\n","        d_h1 = self.act_grad(self.h1) * d_h2\n","        self.grads['w1'] = np.dot(x.T, d_h1)\n","        self.grads['b1'] = np.sum(d_h1, axis=0)\n","        \n","    def update_parameters(self, lr=0.1):\n","        self.w1 -= lr * self.grads['w1']\n","        self.b1 -= lr * self.grads['b1']\n","        self.w2 -= lr * self.grads['w2']\n","        self.b2 -= lr * self.grads['b2']  \n","        self.w3 -= lr * self.grads['w3']\n","        self.b3 -= lr * self.grads['b3']\n","        self.w4 -= lr * self.grads['w4']\n","        self.b4 -= lr * self.grads['b4']  \n","        self.w5 -= lr * self.grads['w5']\n","        self.b5 -= lr * self.grads['b5']  \n","        self.w6 -= lr * self.grads['w6']\n","        self.b6 -= lr * self.grads['b6'] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HaRjNUOo3_sb"},"source":["## 学習\n","サンプル数を削減したMNISTデータセットと作成したネットワークを用いて，２つのネットワークの学習を行います．\n","\n","まず，学習およびネットワークに関するパラメータを設定します．\n","\n","1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を100とします．\n","\n","また，中間層と出力層のユニット数を定義します．\n","ここでは，入力層のユニット数`input_size`を学習データの次元，中間層のユニット数`hidden_size`100`output_size`を10とします．"]},{"cell_type":"code","metadata":{"id":"_habzfG4fjk8"},"source":["# 学習途中の精度を確認するための関数\n","def multiclass_classification_accuracy(pred, true):\n","    clf_res = np.argmax(pred, axis=1)\n","    return np.sum(clf_res == true).astype(np.float32)\n","\n","# 学習中の誤差を確認するための関数\n","def cross_entropy(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","# 学習パラメータ\n","num_train_data = x_train.shape[0]\n","num_test_data = x_test.shape[0]\n","batch_size = 100\n","epoch_num = 100\n","learning_rate = 0.01\n","\n","# ネットワークのパラメータ\n","input_size = x_train.shape[1]\n","hidden_size = 100\n","output_size = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZchI6GMkotD"},"source":["### Dropout無しのモデルの学習\n","\n","Dropout無しのネットワークモデルを学習します．\n","上で定義したパラメータを使用して，ネットワーク`MLP`を初期化し，学習を行います．\n","学習プログラムの詳細については前回までの資料を参照してください．"]},{"cell_type":"code","metadata":{"id":"r108LKmi3_sc"},"source":["model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n","\n","epoch_list = []\n","train_loss_list = []\n","train_accuracy_list = []\n","test_accuracy_list = []\n","\n","iteration = 0\n","for epoch in range(1, epoch_num + 1):\n","    sum_accuracy, sum_loss = 0.0, 0.0\n","    perm = np.random.permutation(num_train_data)\n","    for i in range(0, num_train_data, batch_size):\n","        x_batch = x_train[perm[i:i+batch_size]]\n","        y_batch = y_train[perm[i:i+batch_size]]\n","        \n","        y = model.forward(x_batch)\n","        sum_accuracy += multiclass_classification_accuracy(y, y_batch)\n","        sum_loss += cross_entropy(y, y_batch)\n","        \n","        model.backward(x_batch, y_batch)\n","        model.update_parameters(lr=learning_rate)\n","        iteration += 1\n","\n","    # テストデータに対する認識精度の計算\n","    test_correct_count = 0\n","    for i in range(num_test_data):\n","        input = x_test[i:i+1]\n","        label = y_test[i:i+1]\n","        y = model.forward(input)\n","        test_correct_count += multiclass_classification_accuracy(y, label)\n","\n","    # 学習途中のlossと精度の保存\n","    epoch_list.append(epoch)\n","    train_loss_list.append(sum_loss / num_train_data)\n","    train_accuracy_list.append(sum_accuracy / num_train_data)\n","    test_accuracy_list.append(test_correct_count / num_test_data)\n","\n","    print(\"epoch: {}, train loss: {}, train accuracy: {}, test accuracy: {}\".format(epoch,\n","                                                               sum_loss / num_train_data,\n","                                                               sum_accuracy / num_train_data,\n","                                                               test_correct_count / num_test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2Vs2N4Qksl_"},"source":["### Dropoutありのモデルの学習\n","\n","次にDropoutありのネットワークモデルの学習を行います．\n","この時，Dropoutの割合`dropout_ratio`を`0.5`に設定します．\n","\n","テストデータの認識精度を計算する際は，Dropoutの適用を行わないようにするために，\n","`train_mode=False`と指定することに注意してください．"]},{"cell_type":"code","metadata":{"id":"Gwd7lFWqfqsi"},"source":["model_dout = MLPDropout(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dropout_ratio=0.5)\n","\n","epoch_list_dout = []\n","train_loss_list_dout = []\n","train_accuracy_list_dout = []\n","test_accuracy_list_dout = []\n","\n","iteration = 0\n","for epoch in range(1, epoch_num + 1):\n","    sum_accuracy, sum_loss = 0.0, 0.0\n","    perm = np.random.permutation(num_train_data)\n","    for i in range(0, num_train_data, batch_size):\n","        x_batch = x_train[perm[i:i+batch_size]]\n","        y_batch = y_train[perm[i:i+batch_size]]\n","        \n","        y = model_dout.forward(x_batch)\n","        sum_accuracy += multiclass_classification_accuracy(y, y_batch)\n","        sum_loss += cross_entropy(y, y_batch)\n","        \n","        model_dout.backward(x_batch, y_batch)\n","        model_dout.update_parameters(lr=learning_rate)\n","        iteration += 1\n","\n","    # テストデータに対する認識精度の計算\n","    test_correct_count = 0\n","    for i in range(num_test_data):\n","        input = x_test[i:i+1]\n","        label = y_test[i:i+1]\n","        y = model_dout.forward(input, train_mode=False)\n","        test_correct_count += multiclass_classification_accuracy(y, label)\n","\n","    # 学習途中のlossと精度の保存\n","    epoch_list_dout.append(epoch)\n","    train_loss_list_dout.append(sum_loss / num_train_data)\n","    train_accuracy_list_dout.append(sum_accuracy / num_train_data)\n","    test_accuracy_list_dout.append(test_correct_count / num_test_data)\n","\n","    print(\"epoch: {}, train loss: {}, train accuracy: {}, test accuracy: {}\".format(epoch,\n","                                                               sum_loss / num_train_data,\n","                                                               sum_accuracy / num_train_data,\n","                                                               test_correct_count / num_test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMXfb7CgkY2_"},"source":["## 学習推移のグラフ化\n","\n","最後に2つのネットワークの学習推移を一つのグラフにまとめてプロットすることで，違いを確認します．\n"]},{"cell_type":"code","metadata":{"id":"614eAQzhgQHI"},"source":["plt.figure()\n","plt.plot(epoch_list, train_loss_list, label='train loss')\n","plt.plot(epoch_list_dout, train_loss_list_dout, label='train loss (w/ dout)')\n","plt.xlabel(\"epoch\")     # x軸ラベル\n","plt.ylabel(\"loss\")      # y軸ラベル\n","plt.legend()            # 凡例\n","plt.show()\n","\n","plt.figure()\n","plt.plot(epoch_list, train_accuracy_list, label='train accuracy')\n","plt.plot(epoch_list, test_accuracy_list, label='test accuracy')\n","plt.plot(epoch_list_dout, train_accuracy_list_dout, label='train accuracy (w/ dropout)')\n","plt.plot(epoch_list_dout, test_accuracy_list_dout, label='test accuracy (w/ dout)')\n","plt.xlabel(\"epoch\")     # x軸ラベル\n","plt.ylabel(\"accuracy\")  # y軸ラベル\n","plt.legend()            # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJiDB3Is3_sf"},"source":["## 課題\n","1. ネットワークの中間層のユニット数を変更して認識性能の変化を確認しましょう\n","\n","2. Dropoutを適用する場所を変更してその結果をかくにんしましょう"]}]}