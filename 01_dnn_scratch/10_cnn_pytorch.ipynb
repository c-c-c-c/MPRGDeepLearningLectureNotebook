{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"10_cnn_pytorch.ipynb","provenance":[{"file_id":"https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/01_cnn_dataloader_augmentation.ipynb","timestamp":1606185447796},{"file_id":"https://github.com/machine-perception-robotics-group/GoogleColabNotebooks/blob/master/MLDL_lecture_notebooks/10-B_objecct_classification_with_cifar10_whitening_augmentation.ipynb","timestamp":1598574332404}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wJU2RPpSvlQT"},"source":["# 10：PyTorchを用いたCNNによる画像認識\n","\n","\n","---\n","## 目的\n","PyTorchを用いてMNIST Datasetに対する文字認識を行う．\n","\n","また，ここではGPUを用いたネットワークの計算を行う．\n"]},{"cell_type":"markdown","metadata":{"id":"5rQGfxWYK_4O"},"source":["## 準備\n","\n","### Google Colaboratoryの設定確認・変更\n","本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n","**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n"]},{"cell_type":"markdown","metadata":{"id":"Xo4jjpmwvle1"},"source":["## モジュールのインポート\n","はじめに必要なモジュールをインポートする．\n","\n","### GPUの確認\n","GPUを使用した計算が可能かどうかを確認します．\n","\n","`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n","Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．\n"]},{"cell_type":"code","metadata":{"id":"iCeaCulfvlao"},"source":["# モジュールのインポート\n","import os\n","from time import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary\n","\n","import gzip\n","from random import randint\n","\n","# GPUの確認\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nbdiIyZId5i"},"source":["## データセットのダウンロードと読み込みと学習サンプルの削減\n","\n","\n","まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"]},{"cell_type":"code","metadata":{"id":"N1LbRsqxIfoF"},"source":["!wget -q http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n","!wget -q http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5R2ghpzIqyf"},"source":["次に，ダウンロードしたファイルからデータを読み込みます．詳細は前回までのプログラムを確認してください．\n","\n","今回は2次元の画像データとしてMNISTデータセットを扱うため，\n","データを`(チャンネル, 縦，横)`の形に並べ替えます．"]},{"cell_type":"code","metadata":{"id":"JlChw880IqDC"},"source":["# load images\n","with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n","    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n","x_train = x_train.reshape(-1, 784)\n","\n","with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n","    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n","x_test = x_test.reshape(-1, 784)\n","\n","with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n","    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n","\n","with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n","    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n","\n","x_train = x_train.reshape(-1, 1, 28, 28)\n","x_test = x_test.reshape(-1, 1, 28, 28)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgDd3iX2zmSV"},"source":["## ネットワークモデルの定義\n","畳み込みニューラルネットワークを定義します．\n","\n","ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．\n","\n","１層目の畳み込み層は入力チャンネル数が１，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．２層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．１つ目の全結合層は入力ユニット数は不定とし，出力は1024としています．次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．これらの各層の構成を`__init__`関数で定義します．\n","\n","次に，`forward`関数では，定義した層を接続して処理するように記述します．`forward`関数の引数xは入力データです．それを`__init__`関数で定義したconv1に与え，その出力を活性化関数であるrelu関数に与えます．そして，その出力をmax_pooling_2dに与えて，プーリング処理結果をhとして出力します．hはconv2に与えられて畳み込み処理とプーリング処理を行います．そして，出力hをl1に与えて全結合層の処理を行います．最終的にl3の全結合層の処理を行った出力hを戻り値としています．"]},{"cell_type":"code","metadata":{"id":"TNHnp_YczmY3"},"source":["class CNN(nn.Module):\n","    def __init__(self, n_channels=1, filter_size=3, num_kernel=64, hidden_size=128):\n","        super().__init__()\n","        self.conv = nn.Conv2d(n_channels, num_kernel, kernel_size=filter_size, stride=1, padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.l1 = nn.Linear(int(28/2) * int(28/2) * num_kernel, hidden_size)\n","        self.l2 = nn.Linear(hidden_size, 10)\n","    \n","    def forward(self, x):\n","        h = self.relu(self.conv(x))\n","        h = self.pool(h)\n","        h = h.view(h.size()[0], -1)\n","        h = self.relu(self.l1(h))\n","        h = self.l2(h)\n","        return h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dwuvfouzmd7"},"source":["## ネットワークの作成\n","上のプログラムで定義したネットワークを作成します．\n","\n","CNNクラスを呼び出して，ネットワークモデルを定義します． また，GPUを使う場合（use_cuda == True）には，ネットワークモデルをGPUメモリ上に配置します． これにより，GPUを用いた演算が可能となります．\n","\n","学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します． また，学習率を0.01，モーメンタムを0.9として引数に与えます．\n","\n","最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．\n"]},{"cell_type":"code","metadata":{"id":"23m79Eq-zmjl"},"source":["model = CNN()\n","if use_cuda:\n","    model.cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# モデルの情報を表示\n","torchsummary.summary(model, (1, 28, 28))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUNa9Xe79vAG"},"source":["## 学習\n","読み込んだMNISTデータセットと作成したネットワークを用いて，学習を行います．\n","\n","1回の誤差を算出するデータ数（ミニバッチサイズ）を64，学習エポック数を10とします．\n","\n","次にデータローダーを定義します．\n","データローダーでは，上で読み込んだデータセット（`train_data`）を用いて，for文で指定したミニバッチサイズでデータを読み込むオブジェクトを作成します．\n","この時，`shuffle=True`と設定することで，読み込むデータを毎回ランダムに指定します．\n","\n","次に，誤差関数を設定します．\n","今回は，分類問題をあつかうため，クロスエントロピー誤差を計算するための`CrossEntropyLoss`を`criterion`として定義します．\n","\n","学習を開始します．\n","\n","各更新において，学習用データと教師データをそれぞれ`image`と`label`とします．\n","学習モデルにimageを与えて各クラスの確率yを取得します．\n","各クラスの確率yと教師ラベルtとの誤差を`criterion`で算出します．\n","また，認識精度も算出します．\n","そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．"]},{"cell_type":"code","metadata":{"id":"68RE3RTa76-W"},"source":["# ミニバッチサイズ・エポック数の設定\n","batch_size = 100\n","epoch_num = 10\n","num_train_data = x_train.shape[0]\n","n_iter = num_train_data / batch_size\n","\n","# 誤差関数の設定\n","criterion = nn.CrossEntropyLoss()\n","if use_cuda:\n","    criterion.cuda()\n","\n","# ネットワークを学習モードへ変更\n","model.train()\n","\n","iteration = 1\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","    \n","    perm = np.random.permutation(num_train_data)\n","    for i in range(0, num_train_data, batch_size):\n","        x_batch = x_train[perm[i:i+batch_size]]\n","        y_batch = y_train[perm[i:i+batch_size]]\n","\n","        x_batch = torch.from_numpy(x_batch).type(torch.float32)\n","        y_batch = torch.from_numpy(y_batch).type(torch.int64)\n","\n","        if use_cuda:\n","            x_batch = x_batch.cuda()\n","            y_batch = y_batch.cuda()\n","\n","        y = model(x_batch)\n","\n","        loss = criterion(y, y_batch)\n","        \n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sum_loss += loss.item()\n","        \n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == y_batch)\n","        \n","    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n","                                                                                 sum_loss / n_iter,\n","                                                                                 count.item() / num_train_data,\n","                                                                                 time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"119eIrSmzmw6"},"source":["## テスト\n","学習したネットワークモデルを用いて評価を行います．"]},{"cell_type":"code","metadata":{"id":"yoYVMRGLzm1I"},"source":["# ネットワークを評価モードへ変更\n","model.eval()\n","\n","# 評価の実行\n","count = 0\n","num_test_data = x_test.shape[0]\n","\n","# 勾配計算なしで順伝播計算を行うためのフラグ\n","with torch.no_grad():\n","    for i in range(num_test_data):\n","        x = np.array([x_test[i]], dtype=np.float32)\n","        t = y_test[i]\n","\n","        x = torch.from_numpy(x).type(torch.float32)\n","\n","        if use_cuda:\n","            x = x.cuda()\n","\n","        y = model.forward(x)\n","        pred = torch.argmax(y)\n","        \n","        if pred == t:\n","            count += 1\n","\n","print(\"test accuracy: {}\".format(count / num_test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gzl4N5rC4j5u"},"source":["## 課題\n","1. ネットワーク構造を変えて実験しましょう． \n","     * まず，1層目の畳み込み層のフィルタ数を32にしましょう．また，2層目の畳み込み層のフィルタ数を64にしましょう．\n","    * 次に，中間層のユニット数を2048にしましょう．\n","   \n","\n","\n","2. 最適化の方法をAdamに変えて実験しましょう．\n","\n","\n","\n","3. エポック数やミニバッチサイズを変えて実験しましょう．\n","    * まず，ミニバッチサイズを128にしましょう．\n","    * 次に，エポック数を50にしましょう．"]},{"cell_type":"code","metadata":{"id":"MjS-WptIg9Pf"},"source":[""],"execution_count":null,"outputs":[]}]}