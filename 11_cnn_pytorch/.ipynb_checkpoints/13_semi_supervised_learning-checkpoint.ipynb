{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/13_semi_supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4x9rmDiUTI1"
   },
   "source": [
    "# 13. 半教師付き学習\n",
    "\n",
    "DNNは一般的に，大量の教師ラベル付きデータを用いて学習を行うことで高い認識性能を発揮しています．しかし，教師ラベルは人手によって付与されるため，データ数に比例して人的コストが増えてしまいます．また，問題設定によっては画像に対して1ピクセル毎のラベル付けが必要な場合やデータ自体を集めることが困難な場合もあります．これらのことから期待される問題設定やタスクに対して理想的なデータセット（大量の教師ラベル付きデータ）を用意するのは多くの場合困難です．このような問題を解決する学習方法の１つとして「半教師付き学習」があります．\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/cw3nd3iq0ggb01g/sup.png\" width = 57%>\n",
    "\n",
    "半教師付き学習（Semi-supervised Learning）は，教師ラベル付きデータと教師ラベルなしデータの両方を含むデータセットを利用した学習方法です．問題設定として一般的に，教師ラベル付きデータと比較して大量の教師ラベルなしデータが用意されています．\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/wljsvzdlb9f9ivn/semi_sup.png\" width = 57%>\n",
    "\n",
    "半教師付き学習として様々な学習方法が提案されてますが，ここではConsistency Regularizationという枠組みについて紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPyzEOSCUTI1"
   },
   "source": [
    "# Consistency Regularization\n",
    "\n",
    "Consistency Regularizationは，教師ラベルなしデータに対する出力が一貫性を持つように学習をします．ここで，一貫性とは同一画像に対してノイズなどによって出力が左右されないことを表します．\n",
    "ネットワークはノイズや幾何学変換などに頑健な特徴抽出器を獲得するため，教師ラベルを利用しないものの正解率の向上に寄与します．\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/40zfvwq2eksud9k/CR.png\" width = 57%>\n",
    "\n",
    "## 学習方法\n",
    "Consistency Regularizationは，ネットワークに入力されたデータが教師ラベルを持つか否かによって学習の流れが異なります．\n",
    "\n",
    "### 教師ラベル付きデータ\n",
    "1. ネットワークにデータを入力\n",
    "2. ネットワークが出力した確率分布と教師ラベルからCross Entropyを計算\n",
    "\n",
    "### 教師ラベルなしデータ\n",
    "1. データに対して摂動（ノイズやデータ増幅）を付与\n",
    "2. データ（または摂動を付与したデータ）と摂動を付与したデータをネットワークに入力\n",
    "3. ネットワークが出力した2つの確率分布の相違度を計算\n",
    "\n",
    "ネットワークは，「教師ラベル付きデータから算出したCross Entropy」と「教師ラベルなしデータから算出した確率分布の相違度」を損失関数として学習が行われます．\n",
    "確率分布の相違度を小さくするように学習が行われるため，摂動に左右されないネットワークを獲得します．\n",
    "学習の流れは多くの場合共通ですが，摂動の内容や摂動の付与方法，確率分布間の相違度の計算方法などによって以下のような様々な手法が提案されています．\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/t862ozembfzb1vi/semi_sup_list.png\" width = 65%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vaz2xHBVUTI1"
   },
   "source": [
    "# 畳み込みニューラルネットワークの半教師付き学習\n",
    "クラス分類問題において，半教師付き学習によってネットワークを学習します．\\\n",
    "以降では，教師ラベル付きデータを教師付きデータ，教師ラベルなしデータを教師なしデータと呼びます．\n",
    "\n",
    "プログラムの構成は以下の通りになります．\n",
    "1. データセットの定義　　　：教師付きデータのデータセットと教師なしデータのデータセットを定義します．\n",
    "2. ネットワークの定義　　　：学習を行うネットワークを定義します，\n",
    "3. 教師付き学習による評価　：教師付きデータセットとネットワークを用いて教師付き学習を行います．\n",
    "4. 半教師付き学習による評価：教師付きデータセット，教師なしデータセットとネットワークを用いて半教師付き学習を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK1A84wlAedD"
   },
   "source": [
    "## モジュールの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6jjnSOmeTtzw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#import torchsummary\n",
    "from torch.utils.data import Sampler\n",
    "from torch.utils.data import SubsetRandomSampler, Subset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25j5APz8F_Tf"
   },
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jqF0puazTMK"
   },
   "source": [
    "### データ増幅の定義\n",
    "ここでは，教師付きデータに対するデータ増幅，教師なしデータに摂動として付与するデータ増幅，評価用データに対するデータ増幅の3つを定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ND3_3dOWUTI1"
   },
   "outputs": [],
   "source": [
    "# 教師付きデータに対するデータ増幅\n",
    "transform_A = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# 教師なしデータに対するデータ増幅（摂動）\n",
    "transform_B = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.Pad(4, padding_mode=\"reflect\"), \n",
    "    transforms.RandomCrop(32), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# 評価用データに対するデータ増幅\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWfN6vvFUTI1"
   },
   "source": [
    "### ベースとなるデータセットの定義\n",
    "半教師付き学習手法の評価には，一般的に教師付き学習に用いるベンチマークデータセットが利用されます．\\\n",
    "データセットとして，一般物体認識用データセットであるCIFAR-10を用います．\\\n",
    "CIFAR-10は，10クラス（飛行機，自動車，鳥，猫，鹿，犬，カエル，馬，船，トラック）の画像から構成されるデータセットです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au8la3afUTI1",
    "outputId": "d0db0396-a2dd-424d-e6c3-5bef56bb255a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 学習用データ\n",
    "total_trainset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=True,  download=True, transform=transform_A)\n",
    "unsup_trainset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=True,  download=True, transform=None)\n",
    "\n",
    "# 評価用データ\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./dataset/CIFAR-10\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x993wPPUTI2"
   },
   "source": [
    "### データセットの分割\n",
    "先程定義したデータセットは全て教師付きデータであるため，まず初めにデータセットを教師付きデータと教師なしデータに分割します．\n",
    "ここでは，教師付きデータと教師なしデータに割り振るデータのidのみを取得します．\n",
    "教師なしデータの数はStratifiedShuffleSplitの引数「test_size」で決定し，教師付きデータの数は全体のデータ数からtest_sizeとして指定したデータ数の差分となります．\n",
    "今回は，教師付きデータ100枚，教師なしデータ49,900枚とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L5LEgWDCUTI2"
   },
   "outputs": [],
   "source": [
    "# StratifiedShuffleSplit：データをシャッフルして分割\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=49000, random_state=0)\n",
    "\n",
    "# list(range(len(total_trainset)))：データidのリスト，total_trainset.targets：データidに対応するラベル\n",
    "sss = sss.split(list(range(len(total_trainset))), total_trainset.targets)\n",
    "\n",
    "# 教師付きデータと教師なしデータのデータidを取得\n",
    "label_idx, unlabel_idx = next(sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR6aeccsUTI2"
   },
   "source": [
    "### Datasetの定義\n",
    "Consistency Regularizationを行うには，教師なしデータ１枚につき，摂動を施した２枚の画像を取得する必要があります．\\\n",
    "データセットからデータを取得するアルゴリズムはDatasetクラスにより制御します．今回は自作のDatasetクラスを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "40mubfsSwG6D"
   },
   "outputs": [],
   "source": [
    "# 自作 Dataset\n",
    "class UnsupervisedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform_1, transform_2):\n",
    "        self.dataset = dataset          # データセット\n",
    "        self.transform_1 = transform_1  # 摂動１\n",
    "        self.transform_2 = transform_2  # 摂動２\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.dataset[index]  # データセットからデータを取得\n",
    "\n",
    "        # 同一画像から2つの画像を作成\n",
    "        img1 = self.transform_1(img)  # データに摂動１を適用\n",
    "        img2 = self.transform_2(img)  # データに摂動２を適用\n",
    "\n",
    "        return img1, img2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # このクラスに対してlen()を行った際にデータセットのデータ数を返すように設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyqubj8WUTI2"
   },
   "source": [
    "### データセットの定義\n",
    "「データセットの分割」で取得したデータidを用いて教師付きデータセットと教師なしデータセットを定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7QD0dF51wG4M"
   },
   "outputs": [],
   "source": [
    "# 教師付きデータセットを作成（データ増幅はtotal_trainsetで設定したものを実行）\n",
    "trainset = Subset(total_trainset, label_idx)\n",
    "train_labels          = [total_trainset.targets[idx] for idx in label_idx]\n",
    "trainset.train_labels = train_labels\n",
    "\n",
    "# 教師なしデータセットを作成\n",
    "otherset = Subset(unsup_trainset, unlabel_idx)\n",
    "otherset = UnsupervisedDataset(otherset, transform_B, transform_B)  # 第１引数：教師なしデータ，第２引数：摂動１，第３引数：摂動２"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06QrpYgE6X3J",
    "outputId": "500bee32-f7a3-4cc9-c5e4-b299f8315f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数\n",
      "教師付きデータ： 1000\n",
      "教師なしデータ： 49000\n"
     ]
    }
   ],
   "source": [
    "print(\"データ数\")\n",
    "print(\"教師付きデータ：\", len(trainset))\n",
    "print(\"教師なしデータ：\", len(otherset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ymegZQeUTI3"
   },
   "source": [
    "### Samplerの定義\n",
    "ネットワークの学習時にデータセットから学習用データをランダムに取得する必要があります．\n",
    "今回は，教師付きデータが少ないため，ミニバッチ内の教師付きデータ数を小さく設定します．\n",
    "そのため，教師付きデータを各クラスからバランスよく取得することを考えます．\n",
    "データセットの中から取得するデータを決定するアルゴリズムはSamplerクラスにより制御します．\n",
    "今回は自作のSamplerクラスを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8FiDCrBgJM8S"
   },
   "outputs": [],
   "source": [
    "# 自作 Sampler\n",
    "class StratifiedSampler(Sampler):\n",
    "    def __init__(self, labels):\n",
    "        self.idx_by_lb = defaultdict(list)\n",
    "        for idx, lb in enumerate(labels):\n",
    "            self.idx_by_lb[lb].append(idx)\n",
    "\n",
    "        self.size = len(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        songs_list = []\n",
    "        artists_list = []\n",
    "        for lb, v in self.idx_by_lb.items():\n",
    "            for idx in v:\n",
    "                songs_list.append(idx)\n",
    "                artists_list.append(lb)\n",
    "\n",
    "        shuffled = spotifyShuffle(songs_list, artists_list)\n",
    "        return iter(shuffled)\n",
    "\n",
    "\n",
    "def fisherYatesShuffle(arr):\n",
    "    for i in range(len(arr)-1, 0, -1):\n",
    "        j = random.randint(0, i)\n",
    "        arr[i], arr[j] = arr[j], arr[i]\n",
    "    return arr\n",
    "\n",
    "\n",
    "def spotifyShuffle(songs_list, artists_list):\n",
    "    artist2songs = defaultdict(list)\n",
    "    for artist, song in zip(artists_list, songs_list):\n",
    "        artist2songs[artist].append(song)\n",
    "    songList = []\n",
    "    songsLocs = []\n",
    "    for artist, songs in artist2songs.items():\n",
    "        songs = fisherYatesShuffle(songs)\n",
    "        songList += songs\n",
    "        songsLocs += get_locs(len(songs))\n",
    "    return [songList[idx] for idx in argsort(songsLocs)]\n",
    "\n",
    "\n",
    "def get_locs(n):\n",
    "    percent = 1. / n\n",
    "    locs = [percent * random.random()]\n",
    "    last = locs[0]\n",
    "    for i in range(n - 1):\n",
    "        value = last + percent * random.uniform(0.8, 1.2)\n",
    "        locs.append(value)\n",
    "        last = value\n",
    "    return locs\n",
    "\n",
    "\n",
    "def argsort(seq):\n",
    "    return [i for i, j in sorted(enumerate(seq), key=lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgDSZ48PUTI3"
   },
   "source": [
    "### Dataloaderの定義\n",
    "今回はバッチサイズを512（ラベルありデータ10枚，ラベルなしデータ251枚 × 摂動により2倍）とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uin-5_AhwmnD"
   },
   "outputs": [],
   "source": [
    "# 教師付きデータセット用 Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=False,   # Samplerを利用するためFalse\n",
    "                                           num_workers=16, \n",
    "                                           pin_memory=True,\n",
    "                                           sampler=StratifiedSampler(trainset.train_labels),  # 自作のSampler\n",
    "                                           drop_last=True)\n",
    "\n",
    "# 教師なしデータセット用 Dataloader\n",
    "unsup_loader = torch.utils.data.DataLoader(otherset, \n",
    "                                           batch_size=21,\n",
    "                                           shuffle=True,    # Samplerを利用しないためTure\n",
    "                                           num_workers=16, \n",
    "                                           pin_memory=True, \n",
    "                                           drop_last=True)\n",
    "\n",
    "# 評価データ用 Dataloader\n",
    "test_loader  = torch.utils.data.DataLoader(testset, \n",
    "                                           batch_size=100,\n",
    "                                           shuffle=False, \n",
    "                                           num_workers=16, \n",
    "                                           pin_memory=True, \n",
    "                                           drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCU0WUHXUTI3"
   },
   "source": [
    "### Samplerによるデータ取得の確認\n",
    "ここでは先程定義した自作Samplerクラスによってdataloaderからクラスのバランスよく教師付きデータが取得されているのかを確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qcgtPcCUTI3",
    "outputId": "147744a8-27ba-4370-8c9c-ca412ab28672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 3 4 5 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "tmp = train_loader.__iter__()\n",
    "for _ in range(5):\n",
    "    data1, label1 = tmp.next()      # 教師付きデータのDataloaderからデータと教師ラベルを取得\n",
    "    print(np.sort(label1.numpy()))  # 教師ラベルをソートして表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ysGfkjygN8G"
   },
   "source": [
    "### データの可視化\n",
    "ここでは先程定義した摂動と自作Datasetクラスによって１つの教師なしデータに対してどのような摂動が与えられたのか（視覚的にどう違うのか）を確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "zy3f6dD_UTI3",
    "outputId": "6efcf9e6-5b34-46fd-e684-23d5f7cfed91"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = unsup_loader.__iter__()\n",
    "data1, data2 = tmp.next()      # 教師なしデータのDataloaderから摂動を付与した２つのデータを取得\n",
    "\n",
    "# データの可視化\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "ax1.imshow(data1[0].permute(1,2,0))\n",
    "ax2.imshow(data2[0].permute(1,2,0))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwXJszqqkYE3"
   },
   "source": [
    "## ネットワークの定義\n",
    "畳み込みニューラルネットワークの定義をします．\\\n",
    "ここでは，11.knowledge_distillationや12.deep_mutual_learningで使用したネットワークと同様のものを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mj8-_UzOTtz9"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, widen_factor=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16*widen_factor, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16*widen_factor, 32*widen_factor, kernel_size=3, stride=1, padding=1)\n",
    "        self.l1 = nn.Linear(8*8*32*widen_factor, 1024*widen_factor)\n",
    "        self.l2 = nn.Linear(1024*widen_factor, 1024*widen_factor)\n",
    "        self.l3 = nn.Linear(1024*widen_factor, 10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.pool(self.act(self.conv1(x)))\n",
    "        h = self.pool(self.act(self.conv2(h)))\n",
    "        h = h.view(h.size()[0], -1)\n",
    "        h = self.act(self.l1(h))\n",
    "        h = self.act(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxg52VoDUTI3"
   },
   "source": [
    "## 教師付き学習による学習と評価\n",
    "半教師付き学習による学習結果と比較をするために，教師付きデータのみを用いた教師付き学習を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqK5QGdyUTI3"
   },
   "source": [
    "### 学習条件の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1gNzw4nwUTI3"
   },
   "outputs": [],
   "source": [
    "# エポック数の設定\n",
    "NUM_EPOCH = 25\n",
    "\n",
    "# ネットワークの用意\n",
    "net_sup = CNN(widen_factor=1).cuda()\n",
    "\n",
    "# オプティマイザの設定\n",
    "optimizer = torch.optim.SGD(net_sup.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEaYI-DyUTI3"
   },
   "source": [
    "### 学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNAzb4D3UTI3",
    "outputId": "cc5bbf7d-0875-4593-ec9b-fce19f3c8f8d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1,  mean loss: 2.181,  mean accuracy: 0.18,  elapsed_time :2.05\n",
      "epoch: 2,  mean loss: 1.905,  mean accuracy: 0.32,  elapsed_time :3.89\n",
      "epoch: 3,  mean loss: 1.658,  mean accuracy: 0.4,  elapsed_time :5.99\n",
      "epoch: 4,  mean loss: 1.473,  mean accuracy: 0.47,  elapsed_time :8.07\n",
      "epoch: 5,  mean loss: 1.249,  mean accuracy: 0.56,  elapsed_time :10.1\n",
      "test accuracy: 0.3823\n",
      "epoch: 6,  mean loss: 1.001,  mean accuracy: 0.66,  elapsed_time :13.96\n",
      "epoch: 7,  mean loss: 0.73,  mean accuracy: 0.74,  elapsed_time :15.88\n",
      "epoch: 8,  mean loss: 0.569,  mean accuracy: 0.82,  elapsed_time :17.77\n",
      "epoch: 9,  mean loss: 0.479,  mean accuracy: 0.84,  elapsed_time :19.87\n",
      "epoch: 10,  mean loss: 0.352,  mean accuracy: 0.89,  elapsed_time :21.91\n",
      "test accuracy: 0.3736\n",
      "epoch: 11,  mean loss: 0.336,  mean accuracy: 0.89,  elapsed_time :25.72\n",
      "epoch: 12,  mean loss: 0.198,  mean accuracy: 0.94,  elapsed_time :27.8\n",
      "epoch: 13,  mean loss: 0.226,  mean accuracy: 0.93,  elapsed_time :29.83\n",
      "epoch: 14,  mean loss: 0.255,  mean accuracy: 0.93,  elapsed_time :31.91\n",
      "epoch: 15,  mean loss: 0.318,  mean accuracy: 0.9,  elapsed_time :33.96\n",
      "test accuracy: 0.3374\n",
      "epoch: 16,  mean loss: 0.173,  mean accuracy: 0.94,  elapsed_time :37.78\n",
      "epoch: 17,  mean loss: 0.188,  mean accuracy: 0.94,  elapsed_time :39.83\n",
      "epoch: 18,  mean loss: 0.095,  mean accuracy: 0.97,  elapsed_time :41.56\n",
      "epoch: 19,  mean loss: 0.113,  mean accuracy: 0.97,  elapsed_time :43.51\n",
      "epoch: 20,  mean loss: 0.129,  mean accuracy: 0.97,  elapsed_time :45.59\n",
      "test accuracy: 0.3476\n",
      "epoch: 21,  mean loss: 0.058,  mean accuracy: 0.98,  elapsed_time :48.82\n",
      "epoch: 22,  mean loss: 0.031,  mean accuracy: 0.99,  elapsed_time :50.9\n",
      "epoch: 23,  mean loss: 0.013,  mean accuracy: 1.0,  elapsed_time :52.96\n",
      "epoch: 24,  mean loss: 0.0,  mean accuracy: 1.0,  elapsed_time :55.02\n",
      "epoch: 25,  mean loss: 0.0,  mean accuracy: 1.0,  elapsed_time :57.13\n",
      "test accuracy: 0.3884\n"
     ]
    }
   ],
   "source": [
    "# ネットワークを学習モードへ変更\n",
    "net_sup.train()\n",
    "\n",
    "start = time()\n",
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    # ネットワークの学習 ----------------------------------------------------------\n",
    "    # ネットワークを学習モードへ変更\n",
    "    net_sup.train()\n",
    "\n",
    "    # ログ用の設定\n",
    "    sum_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for image, label in train_loader:\n",
    "        # 学習データをGPUへ\n",
    "        image = image.cuda()\n",
    "        label = label.cuda()\n",
    "        \n",
    "        # ネットワークへ学習データを入力\n",
    "        logits = net_sup(image)\n",
    "        \n",
    "        # 損失の計算(教師付きデータ)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        \n",
    "        # パラメータの更新\n",
    "        net_sup.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ログ用に損失と精度の取得\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        pred   = torch.argmax(logits, dim=1)\n",
    "        count += torch.sum(pred == label)\n",
    "        \n",
    "    # ログの表示\n",
    "    print(f\"epoch: {epoch},\\\n",
    "  mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
    "  mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
    "  elapsed_time :{round(time()-start, 2)}\")\n",
    "\n",
    "    # ネットワークの評価 ----------------------------------------------------------\n",
    "    if epoch%5 == 0:  # 5 epoch毎に評価\n",
    "        # ネットワークを評価モードへ変更\n",
    "        net_sup.eval()\n",
    "\n",
    "        # 評価の実行\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for image, label in test_loader:\n",
    "                \n",
    "                image = image.cuda()\n",
    "                label = label.cuda()\n",
    "                \n",
    "                logits = net_sup(image)\n",
    "                \n",
    "                pred   = torch.argmax(logits, dim=1)\n",
    "                count += torch.sum(pred == label)\n",
    "\n",
    "        # 評価結果の表示\n",
    "        print(f\"test accuracy: {count.item()/len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW3VIvsZTt0G"
   },
   "source": [
    "## 半教師付き学習による学習と評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIJe2NQu_L4e"
   },
   "source": [
    "### 確率分布の相違度（教師なしデータに対する損失設計）\n",
    "今回は「教師なしデータから算出した確率分布の相違度」としてKL-divergenceを使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XdYjccMqy5kv"
   },
   "outputs": [],
   "source": [
    "# 教師なしデータに対する損失関数（KL-divergence）の定義\n",
    "def kl_divergence(logits_1, logits_2):\n",
    "    softmax_1 = F.softmax(logits_1, dim=1)\n",
    "    softmax_2 = F.softmax(logits_2, dim=1)\n",
    "    kl = (softmax_2 * torch.log((softmax_2 / (softmax_1+1e-10)) + 1e-10)).sum(dim=1)\n",
    "    return kl.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9husGpYUTI3"
   },
   "source": [
    "### 学習条件の設定\n",
    "教師付きデータに対する損失式と教師なしデータに対する損失式の設計が異なるため，教師なしデータに対する損失式に重み付けをすることで勾配の調整をします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "B3n7ec4JUTI3"
   },
   "outputs": [],
   "source": [
    "# エポック数の設定\n",
    "NUM_EPOCH = 100\n",
    "\n",
    "# 教師なし損失に対する重み付け\n",
    "w_unlabel = 400\n",
    "\n",
    "# ネットワークの用意\n",
    "net = CNN(widen_factor=1).cuda()\n",
    "\n",
    "# オプティマイザの設定\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvH9lACsUTI3"
   },
   "source": [
    "### 学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPUL3binTt0H",
    "outputId": "49d0babd-b4c3-41f3-e58f-98115aff16b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1,  mean loss: 2.302,  label loss: 2.29,  unlabel loss: 0.012,  mean accuracy: 0.17,  elapsed_time :3.69\n",
      "epoch: 2,  mean loss: 2.283,  label loss: 2.265,  unlabel loss: 0.018,  mean accuracy: 0.24,  elapsed_time :7.56\n",
      "epoch: 3,  mean loss: 2.275,  label loss: 2.248,  unlabel loss: 0.027,  mean accuracy: 0.28,  elapsed_time :11.58\n",
      "epoch: 4,  mean loss: 2.271,  label loss: 2.242,  unlabel loss: 0.03,  mean accuracy: 0.32,  elapsed_time :15.74\n",
      "epoch: 5,  mean loss: 2.266,  label loss: 2.234,  unlabel loss: 0.032,  mean accuracy: 0.33,  elapsed_time :19.77\n",
      "test accuracy: 0.28\n",
      "epoch: 6,  mean loss: 2.268,  label loss: 2.233,  unlabel loss: 0.035,  mean accuracy: 0.31,  elapsed_time :25.42\n",
      "epoch: 7,  mean loss: 2.262,  label loss: 2.224,  unlabel loss: 0.039,  mean accuracy: 0.35,  elapsed_time :29.36\n",
      "epoch: 8,  mean loss: 2.258,  label loss: 2.217,  unlabel loss: 0.041,  mean accuracy: 0.35,  elapsed_time :33.35\n",
      "epoch: 9,  mean loss: 2.257,  label loss: 2.214,  unlabel loss: 0.043,  mean accuracy: 0.37,  elapsed_time :37.17\n",
      "epoch: 10,  mean loss: 2.255,  label loss: 2.212,  unlabel loss: 0.043,  mean accuracy: 0.35,  elapsed_time :40.7\n",
      "test accuracy: 0.33\n",
      "epoch: 11,  mean loss: 2.26,  label loss: 2.214,  unlabel loss: 0.045,  mean accuracy: 0.32,  elapsed_time :46.43\n",
      "epoch: 12,  mean loss: 2.259,  label loss: 2.219,  unlabel loss: 0.04,  mean accuracy: 0.34,  elapsed_time :50.35\n",
      "epoch: 13,  mean loss: 2.249,  label loss: 2.202,  unlabel loss: 0.047,  mean accuracy: 0.39,  elapsed_time :54.35\n",
      "epoch: 14,  mean loss: 2.251,  label loss: 2.198,  unlabel loss: 0.053,  mean accuracy: 0.39,  elapsed_time :58.29\n",
      "epoch: 15,  mean loss: 2.246,  label loss: 2.196,  unlabel loss: 0.05,  mean accuracy: 0.37,  elapsed_time :61.99\n",
      "test accuracy: 0.34\n",
      "epoch: 16,  mean loss: 2.246,  label loss: 2.195,  unlabel loss: 0.051,  mean accuracy: 0.39,  elapsed_time :67.68\n",
      "epoch: 17,  mean loss: 2.243,  label loss: 2.19,  unlabel loss: 0.053,  mean accuracy: 0.37,  elapsed_time :71.66\n",
      "epoch: 18,  mean loss: 2.237,  label loss: 2.181,  unlabel loss: 0.056,  mean accuracy: 0.4,  elapsed_time :75.58\n",
      "epoch: 19,  mean loss: 2.243,  label loss: 2.182,  unlabel loss: 0.061,  mean accuracy: 0.37,  elapsed_time :79.23\n",
      "epoch: 20,  mean loss: 2.237,  label loss: 2.182,  unlabel loss: 0.056,  mean accuracy: 0.4,  elapsed_time :83.27\n",
      "test accuracy: 0.3\n",
      "epoch: 21,  mean loss: 2.239,  label loss: 2.177,  unlabel loss: 0.062,  mean accuracy: 0.4,  elapsed_time :88.65\n",
      "epoch: 22,  mean loss: 2.228,  label loss: 2.166,  unlabel loss: 0.062,  mean accuracy: 0.44,  elapsed_time :92.61\n",
      "epoch: 23,  mean loss: 2.247,  label loss: 2.189,  unlabel loss: 0.058,  mean accuracy: 0.4,  elapsed_time :96.59\n",
      "epoch: 24,  mean loss: 2.239,  label loss: 2.178,  unlabel loss: 0.06,  mean accuracy: 0.42,  elapsed_time :100.48\n",
      "epoch: 25,  mean loss: 2.233,  label loss: 2.171,  unlabel loss: 0.062,  mean accuracy: 0.44,  elapsed_time :104.25\n",
      "test accuracy: 0.35\n",
      "epoch: 26,  mean loss: 2.229,  label loss: 2.166,  unlabel loss: 0.063,  mean accuracy: 0.43,  elapsed_time :109.52\n",
      "epoch: 27,  mean loss: 2.229,  label loss: 2.164,  unlabel loss: 0.065,  mean accuracy: 0.46,  elapsed_time :113.46\n",
      "epoch: 28,  mean loss: 2.23,  label loss: 2.162,  unlabel loss: 0.068,  mean accuracy: 0.42,  elapsed_time :117.44\n",
      "epoch: 29,  mean loss: 2.229,  label loss: 2.159,  unlabel loss: 0.07,  mean accuracy: 0.47,  elapsed_time :121.41\n",
      "epoch: 30,  mean loss: 2.224,  label loss: 2.158,  unlabel loss: 0.066,  mean accuracy: 0.45,  elapsed_time :125.17\n",
      "test accuracy: 0.4\n",
      "epoch: 31,  mean loss: 2.225,  label loss: 2.15,  unlabel loss: 0.075,  mean accuracy: 0.45,  elapsed_time :130.92\n",
      "epoch: 32,  mean loss: 2.227,  label loss: 2.16,  unlabel loss: 0.067,  mean accuracy: 0.45,  elapsed_time :134.89\n",
      "epoch: 33,  mean loss: 2.226,  label loss: 2.153,  unlabel loss: 0.073,  mean accuracy: 0.45,  elapsed_time :138.83\n",
      "epoch: 34,  mean loss: 2.218,  label loss: 2.149,  unlabel loss: 0.069,  mean accuracy: 0.47,  elapsed_time :142.82\n",
      "epoch: 35,  mean loss: 2.216,  label loss: 2.136,  unlabel loss: 0.079,  mean accuracy: 0.49,  elapsed_time :146.71\n",
      "test accuracy: 0.4\n",
      "epoch: 36,  mean loss: 2.216,  label loss: 2.14,  unlabel loss: 0.076,  mean accuracy: 0.48,  elapsed_time :151.86\n",
      "epoch: 37,  mean loss: 2.222,  label loss: 2.147,  unlabel loss: 0.075,  mean accuracy: 0.46,  elapsed_time :155.77\n",
      "epoch: 38,  mean loss: 2.212,  label loss: 2.134,  unlabel loss: 0.078,  mean accuracy: 0.48,  elapsed_time :159.56\n",
      "epoch: 39,  mean loss: 2.234,  label loss: 2.157,  unlabel loss: 0.077,  mean accuracy: 0.44,  elapsed_time :163.51\n",
      "epoch: 40,  mean loss: 2.221,  label loss: 2.145,  unlabel loss: 0.076,  mean accuracy: 0.5,  elapsed_time :167.45\n",
      "test accuracy: 0.4\n",
      "epoch: 41,  mean loss: 2.213,  label loss: 2.14,  unlabel loss: 0.074,  mean accuracy: 0.49,  elapsed_time :173.29\n",
      "epoch: 42,  mean loss: 2.207,  label loss: 2.129,  unlabel loss: 0.079,  mean accuracy: 0.5,  elapsed_time :177.25\n",
      "epoch: 43,  mean loss: 2.219,  label loss: 2.134,  unlabel loss: 0.086,  mean accuracy: 0.49,  elapsed_time :181.25\n",
      "epoch: 44,  mean loss: 2.205,  label loss: 2.122,  unlabel loss: 0.083,  mean accuracy: 0.51,  elapsed_time :185.18\n",
      "epoch: 45,  mean loss: 2.213,  label loss: 2.133,  unlabel loss: 0.079,  mean accuracy: 0.51,  elapsed_time :189.1\n",
      "test accuracy: 0.42\n",
      "epoch: 46,  mean loss: 2.21,  label loss: 2.12,  unlabel loss: 0.089,  mean accuracy: 0.54,  elapsed_time :194.79\n",
      "epoch: 47,  mean loss: 2.21,  label loss: 2.126,  unlabel loss: 0.083,  mean accuracy: 0.52,  elapsed_time :198.77\n",
      "epoch: 48,  mean loss: 2.212,  label loss: 2.128,  unlabel loss: 0.085,  mean accuracy: 0.51,  elapsed_time :202.77\n",
      "epoch: 49,  mean loss: 2.205,  label loss: 2.122,  unlabel loss: 0.083,  mean accuracy: 0.53,  elapsed_time :206.78\n",
      "epoch: 50,  mean loss: 2.203,  label loss: 2.113,  unlabel loss: 0.09,  mean accuracy: 0.51,  elapsed_time :210.66\n",
      "test accuracy: 0.36\n",
      "epoch: 51,  mean loss: 2.201,  label loss: 2.11,  unlabel loss: 0.091,  mean accuracy: 0.55,  elapsed_time :216.45\n",
      "epoch: 52,  mean loss: 2.205,  label loss: 2.12,  unlabel loss: 0.085,  mean accuracy: 0.51,  elapsed_time :220.39\n",
      "epoch: 53,  mean loss: 2.191,  label loss: 2.099,  unlabel loss: 0.092,  mean accuracy: 0.56,  elapsed_time :224.15\n",
      "epoch: 54,  mean loss: 2.195,  label loss: 2.097,  unlabel loss: 0.097,  mean accuracy: 0.56,  elapsed_time :228.22\n",
      "epoch: 55,  mean loss: 2.197,  label loss: 2.107,  unlabel loss: 0.091,  mean accuracy: 0.57,  elapsed_time :232.16\n",
      "test accuracy: 0.39\n",
      "epoch: 56,  mean loss: 2.196,  label loss: 2.098,  unlabel loss: 0.098,  mean accuracy: 0.58,  elapsed_time :237.88\n",
      "epoch: 57,  mean loss: 2.194,  label loss: 2.101,  unlabel loss: 0.093,  mean accuracy: 0.56,  elapsed_time :241.82\n",
      "epoch: 58,  mean loss: 2.191,  label loss: 2.087,  unlabel loss: 0.104,  mean accuracy: 0.58,  elapsed_time :245.24\n",
      "epoch: 59,  mean loss: 2.184,  label loss: 2.088,  unlabel loss: 0.097,  mean accuracy: 0.58,  elapsed_time :248.85\n",
      "epoch: 60,  mean loss: 2.19,  label loss: 2.094,  unlabel loss: 0.096,  mean accuracy: 0.59,  elapsed_time :252.81\n",
      "test accuracy: 0.41\n",
      "epoch: 61,  mean loss: 2.188,  label loss: 2.09,  unlabel loss: 0.098,  mean accuracy: 0.6,  elapsed_time :258.55\n",
      "epoch: 62,  mean loss: 2.172,  label loss: 2.068,  unlabel loss: 0.104,  mean accuracy: 0.6,  elapsed_time :262.51\n",
      "epoch: 63,  mean loss: 2.227,  label loss: 2.141,  unlabel loss: 0.086,  mean accuracy: 0.45,  elapsed_time :266.47\n",
      "epoch: 64,  mean loss: 2.222,  label loss: 2.151,  unlabel loss: 0.07,  mean accuracy: 0.45,  elapsed_time :270.45\n",
      "epoch: 65,  mean loss: 2.206,  label loss: 2.12,  unlabel loss: 0.086,  mean accuracy: 0.52,  elapsed_time :274.41\n",
      "test accuracy: 0.4\n",
      "epoch: 66,  mean loss: 2.202,  label loss: 2.115,  unlabel loss: 0.087,  mean accuracy: 0.54,  elapsed_time :280.15\n",
      "epoch: 67,  mean loss: 2.188,  label loss: 2.091,  unlabel loss: 0.096,  mean accuracy: 0.57,  elapsed_time :283.78\n",
      "epoch: 68,  mean loss: 2.184,  label loss: 2.078,  unlabel loss: 0.106,  mean accuracy: 0.59,  elapsed_time :287.7\n",
      "epoch: 69,  mean loss: 2.193,  label loss: 2.1,  unlabel loss: 0.093,  mean accuracy: 0.55,  elapsed_time :291.59\n",
      "epoch: 70,  mean loss: 2.245,  label loss: 2.154,  unlabel loss: 0.091,  mean accuracy: 0.46,  elapsed_time :295.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.23\n",
      "epoch: 71,  mean loss: 2.269,  label loss: 2.234,  unlabel loss: 0.035,  mean accuracy: 0.29,  elapsed_time :301.14\n",
      "epoch: 72,  mean loss: 2.24,  label loss: 2.179,  unlabel loss: 0.061,  mean accuracy: 0.43,  elapsed_time :304.75\n",
      "epoch: 73,  mean loss: 2.223,  label loss: 2.149,  unlabel loss: 0.074,  mean accuracy: 0.48,  elapsed_time :308.39\n",
      "epoch: 74,  mean loss: 2.209,  label loss: 2.13,  unlabel loss: 0.079,  mean accuracy: 0.55,  elapsed_time :312.22\n",
      "epoch: 75,  mean loss: 2.217,  label loss: 2.118,  unlabel loss: 0.1,  mean accuracy: 0.52,  elapsed_time :316.19\n",
      "test accuracy: 0.21\n",
      "epoch: 76,  mean loss: 2.245,  label loss: 2.193,  unlabel loss: 0.052,  mean accuracy: 0.38,  elapsed_time :321.89\n",
      "epoch: 77,  mean loss: 2.217,  label loss: 2.137,  unlabel loss: 0.08,  mean accuracy: 0.5,  elapsed_time :325.9\n",
      "epoch: 78,  mean loss: 2.204,  label loss: 2.113,  unlabel loss: 0.091,  mean accuracy: 0.55,  elapsed_time :329.93\n",
      "epoch: 79,  mean loss: 2.189,  label loss: 2.094,  unlabel loss: 0.095,  mean accuracy: 0.56,  elapsed_time :333.83\n",
      "epoch: 80,  mean loss: 2.196,  label loss: 2.109,  unlabel loss: 0.087,  mean accuracy: 0.6,  elapsed_time :337.79\n",
      "test accuracy: 0.41\n",
      "epoch: 81,  mean loss: 2.184,  label loss: 2.081,  unlabel loss: 0.103,  mean accuracy: 0.6,  elapsed_time :343.47\n",
      "epoch: 82,  mean loss: 2.179,  label loss: 2.071,  unlabel loss: 0.108,  mean accuracy: 0.59,  elapsed_time :347.05\n",
      "epoch: 83,  mean loss: 2.168,  label loss: 2.066,  unlabel loss: 0.102,  mean accuracy: 0.63,  elapsed_time :351.0\n",
      "epoch: 84,  mean loss: 2.209,  label loss: 2.112,  unlabel loss: 0.097,  mean accuracy: 0.51,  elapsed_time :354.7\n",
      "epoch: 85,  mean loss: 2.204,  label loss: 2.114,  unlabel loss: 0.09,  mean accuracy: 0.52,  elapsed_time :358.64\n",
      "test accuracy: 0.37\n",
      "epoch: 86,  mean loss: 2.184,  label loss: 2.093,  unlabel loss: 0.091,  mean accuracy: 0.57,  elapsed_time :364.36\n",
      "epoch: 87,  mean loss: 2.162,  label loss: 2.06,  unlabel loss: 0.102,  mean accuracy: 0.59,  elapsed_time :368.28\n",
      "epoch: 88,  mean loss: 2.167,  label loss: 2.072,  unlabel loss: 0.096,  mean accuracy: 0.58,  elapsed_time :372.03\n",
      "epoch: 89,  mean loss: 2.243,  label loss: 2.167,  unlabel loss: 0.075,  mean accuracy: 0.4,  elapsed_time :376.02\n",
      "epoch: 90,  mean loss: 2.253,  label loss: 2.184,  unlabel loss: 0.069,  mean accuracy: 0.35,  elapsed_time :379.97\n",
      "test accuracy: 0.1\n",
      "epoch: 91,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :385.72\n",
      "epoch: 92,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :389.74\n",
      "epoch: 93,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :393.66\n",
      "epoch: 94,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :397.6\n",
      "epoch: 95,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :401.54\n",
      "test accuracy: 0.1\n",
      "epoch: 96,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :407.34\n",
      "epoch: 97,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :411.35\n",
      "epoch: 98,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :415.27\n",
      "epoch: 99,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.1,  elapsed_time :419.25\n",
      "epoch: 100,  mean loss: 2.303,  label loss: 2.303,  unlabel loss: 0.0,  mean accuracy: 0.09,  elapsed_time :423.3\n",
      "test accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "# ネットワークを学習モードへ変更\n",
    "net.train()\n",
    "\n",
    "start = time()\n",
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "    # ネットワークの学習 ----------------------------------------------------------\n",
    "    # ネットワークを学習モードへ変更\n",
    "    net.train()\n",
    "\n",
    "    # ログ用の設定\n",
    "    sum_loss_sup   = 0.0\n",
    "    sum_loss_unsup = 0.0\n",
    "    sum_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # 教師なし用 Dataloaderの設定\n",
    "    iter_u = iter(unsup_loader)\n",
    "    \n",
    "    for img, label in train_loader:  # 教師付きデータの取得\n",
    "        # 教師なしデータの取得\n",
    "        try:\n",
    "            unlabel_img1, unlabel_img2 = next(iter_u)\n",
    "        except StopIteration:\n",
    "            iter_u = iter(unsup_loader)\n",
    "            unlabel_img1, unlabel_img2 = next(iter_u)\n",
    "\n",
    "        # 学習データをGPUへ\n",
    "        image = torch.cat([img, unlabel_img1, unlabel_img2]).cuda()  # 教師付きデータと教師なしデータを1つに\n",
    "        label = label.cuda()\n",
    "        \n",
    "        # ネットワークへ学習データを入力\n",
    "        logits_all = net(image)\n",
    "        \n",
    "        # 損失の計算(教師付きデータ) 交差エントロピー損失\n",
    "        logits = logits_all[:len(img)]   # 教師付きデータに対する出力を抽出\n",
    "        loss_sup = F.cross_entropy(logits, label)\n",
    "\n",
    "        # 損失の計算(教師なしデータ) KL-divergence\n",
    "        logits_unsup = logits_all[len(img):]   # 教師なしデータに対する出力を抽出\n",
    "        logits1, logits2 = torch.chunk(logits_unsup, 2)\n",
    "        # KL－divergenceは距離の公理を満たさないため，(logits1 -> logits2)と(logits2 -> logits1)の平均値を損失として利用\n",
    "        loss_unsup = (kl_divergence(logits1, logits2)+kl_divergence(logits2, logits1))/2  # KL-divergence\n",
    "        #loss_unsup = F.mse_loss(logits2, logits1)   # 平均2乗誤差 (出力,ラベル)\n",
    "\n",
    "        # 最終的な損失を計算\n",
    "        loss = loss_sup + w_unlabel*loss_unsup\n",
    "        \n",
    "        # パラメータの更新\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ログ用に各損失と精度の取得\n",
    "        sum_loss += loss.item()               # 損失の合計値\n",
    "        sum_loss_sup   += loss_sup.item()     # 教師付きデータに対する損失\n",
    "        sum_loss_unsup += loss_unsup.item()   # 教師なしデータに対する損失\n",
    "        \n",
    "        pred   = torch.argmax(logits, dim=1)  # 値の最も高いクラスの抽出\n",
    "        count += torch.sum(pred == label)     # 正解したデータの数をカウント\n",
    "        \n",
    "    # ログの表示\n",
    "    print( f\"epoch: {epoch},\\\n",
    "  mean loss: {round(sum_loss/len(train_loader), 3)},\\\n",
    "  label loss: {round(sum_loss_sup/len(train_loader), 3)},\\\n",
    "  unlabel loss: {round(w_unlabel*sum_loss_unsup/len(train_loader), 3)},\\\n",
    "  mean accuracy: {round(count.item()/len(train_loader.dataset), 2)},\\\n",
    "  elapsed_time :{round(time()-start, 2)}\" )\n",
    "\n",
    "    # ネットワークの評価 ----------------------------------------------------------\n",
    "    if (epoch%5 == 0) or (epoch == NUM_EPOCH):  # 5 epoch毎に評価\n",
    "        # ネットワークを評価モードへ変更\n",
    "        net.eval()\n",
    "\n",
    "        # 評価の実行\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for image, label in test_loader:\n",
    "                \n",
    "                image = image.cuda()\n",
    "                label = label.cuda()\n",
    "                \n",
    "                logits = net(image)\n",
    "                \n",
    "                pred   = torch.argmax(logits, dim=1)\n",
    "                count += torch.sum(pred == label)\n",
    "\n",
    "        # 評価結果の表示\n",
    "        print(f\"test accuracy: {round(count.item()/len(test_loader.dataset), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQPab9W0Tt0y"
   },
   "source": [
    "# 課題\n",
    "1. 摂動の内容（transform_B）を変更して精度がどのように変化するのか確認しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KSuyl0jhcjV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "13_semi_supervised_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
