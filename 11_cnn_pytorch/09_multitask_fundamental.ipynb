{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "マルチタスク学習（基礎編） ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMdhh1cV6BLek3nDEwisjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/09_multitask_fundamental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lolcIPKZznbJ"
      },
      "source": [
        "# 目的\n",
        "マルチタスク学習の基礎を理解する．また，回帰タスクと識別タスクを同時に解くネットワークを実際に作成し，マルチタスク学習の性能を確認する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeIM5FMrjYq9"
      },
      "source": [
        "# マルチタスク学習（MTL）\n",
        "MTLとは，単一のニューラルネットワークで複数のタスクを解くことです．\n",
        "例えば，[1]では歩行者の画像を入力して，歩行者の頭頂部と足先の座標，身体と顔の向き，性別，傘の有無を出力します．タスクごとに個別のネットワークを用いる場合と比べ，推定時の計算コストが少なく高速な推定ができるほか，各タスクがお互いの精度向上に貢献しあうことが知られています．\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td><img src=\"http://www.mprg.cs.chubu.ac.jp/Tutorial/MPRG_Lecture/02_09/fukui1.png\"></td>\n",
        "<td><img src=\"http://www.mprg.cs.chubu.ac.jp/Tutorial/MPRG_Lecture/02_09/fukui2.png\"></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "具体的には，ネットワークの出力ユニットを複数設け，それらの誤差関数を同時に最適化することで実現できます．出力ユニットは一般的に，[1]のようにネットワークの最終層で複数タスクの結果を同時に出力する方法や，ネットワークを途中で分岐させて各タスクの結果を出力する方法があります．基礎編では前者，応用編では後者のネットワークを取り扱います．誤差関数は，以下のように2つ以上のタスクの誤差を足します．このとき，誤差のオーダー（桁数）が大きく異る場合は，桁数の大きい誤差のタスクを重点的に学習してしまい，もう一方のタスクはあまり学習されないことがあります．これを解決するため，いずれかまたは全ての誤差に重み($\\lambda$)をかけることがあります．\n",
        "\n",
        "$$\n",
        "L\\left(\\cdot\\right)= \\lambda_a L_{\\mathrm{taskA}}\\left(\\hat{y}_{i}, y_{i}\\right) + \\lambda_b L_{\\mathrm{taskB}}\\left(\\hat{z}_{i}, z_{i}\\right)\n",
        "$$\n",
        "\n",
        "実は，近年多く提案されているモダンなディープニューラルネットワークのほとんどが，MTLの考え方に基づいています．例えば，物体検出を行うFaster R-CNN[2]は，物体の領域を検出するための誤差$L_{reg}$と，対象領域の物体クラスを識別するための誤差$L_{cls}$を同時に最適化しています．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmGAFXVOjVAB"
      },
      "source": [
        "# 準備\n",
        "## Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います． GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV5TaEvNHdlv"
      },
      "source": [
        "# 使用するデータセット\n",
        "今回は，分類タスクと回帰タスクを同時に行うことができるMulti-Task Facial Landmark (MTFL) dataset[3]を用います．MTFLは，12995人の著名な人物の顔画像から構成されているデータセットです．教師ラベルとして，\n",
        "\n",
        "- 各器官点の座標（両目，鼻，口角）\n",
        "- 性別（男，女）\n",
        "- 笑顔の判定（笑顔，真顔）\n",
        "- 眼鏡の有無（あり，なし）\n",
        "- 頭の向き（左横顔，左，正面，右，右横顔）\n",
        "\n",
        "が含まれています．\n",
        "\n",
        "<img src=\"http://www.mprg.cs.chubu.ac.jp/Tutorial/MPRG_Lecture/02_09/mtfl.jpg\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4f5QyLuHgQ0"
      },
      "source": [
        "# モジュールのインポートとGPUの確認\n",
        "はじめに，必要なモジュールをインポートしたのち，GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です． `False`となっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvhOviHHy5AI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e5a068-b60c-4c70-acb6-7a53a79472f9"
      },
      "source": [
        "# モジュールのインポート\n",
        "import os\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8piZ5BcDICYL"
      },
      "source": [
        "# データセットの読み込み\n",
        "データセットの読み込みは，他のチュートリアルと同様に行います．\n",
        "まず，`MTFLdataset`というクラスを作成します． この際，`torch.utils.data.Dataset`クラスを継承します．\n",
        "\n",
        "## 1. `__init__()`\n",
        "`self.__init__(self, ...)`でMTFLデータセットのダウンロードと読み込みを行います． まず，`urllib`を用いてwebからデータをダウンロードします． その後，ダウンロードした圧縮ファイルを`zipfile`を用いて解凍します． ※ 解凍したファイルを左側の「ファイル」一覧から確認して見ましょう．\n",
        "\n",
        "次に，用意するデータが学習用データ (`train`) か評価用データ (`test`) かを指定し，読み込むデータリスト (`training.txt`または`testing.txt`) を`self.data_list`に格納します．データリストのテキストファイルは下記のフォーマットになっています．\n",
        "\n",
        "```\n",
        " #image path #x1...x5,y1..y5 #gender #smile #wearing glasses #head pose\n",
        "\n",
        "--x1...x5,y1...y5: 各器官点の座標． 1: left eye, 2: right eye, 3: nose, 4: left mouth corner, 5: right mouth corner\n",
        "--gender: 性別． 1: male, 2: female\n",
        "--smile: 笑顔の判定． 1: smiling, 2: not smiling\n",
        "--glasses: 眼鏡の有無． 1: wearing glasses, 2: not wearing glasses.\n",
        "--head pose: 頭の向き． 1: left profile, 2: left, 3: frontal, 4: right, 5: right profile\n",
        "```\n",
        "\n",
        "このデータを格納するために，`pandas.read_csv()`を用います．この関数は本来，コンマで区切られたファイル（CSVファイル）を読み込むためのものですが，`sep=\" \"`を指定することで，半角スペースで区切られたファイルも読み込むことができます．また，1文字目に余分な半角スペースが入っているため`skipinitialspace=True`で無視するように指定し，最終行には無駄なデータ（半角スペースひとつだけ）があるため，`skipfooter=1`で最終行を無視するように指定します．\n",
        "\n",
        "\n",
        "## 2. `__len__()`\n",
        "`__len__(self)`ではデータセットのサンプル数を返すように定義します． 今回は，`self.data`に格納されている画像データの枚数を返す様に定義します． （学習用データでは10,000枚，評価用データは2,995枚）\n",
        "\n",
        "## 3. `__getitem__()`\n",
        "`__getitem__(self, item)`では，`item`で指定した番号のサンプルを読み込んで返すように定義を行います． まず，`item`番目の画像のパスを取得し，OpenCVの`imread`で読み込みます．pandasで読み込んだcsvは，`.iloc[]`を用いることでnumpyの配列のようにアクセスできます．また，パスの区切り文字はスラッシュとバックスラッシュが混ざっていることがあるため，ここではUnix（スラッシュ）に統一します．パスの文字列に`.replace('\\\\', '/')`をつけてバックスラッシュをスラッシュに置換します．\n",
        "\n",
        "その後，画像を`40×40`にリサイズし，画像データの画素値を0~1の範囲の値になる様に正規化を行い，画像データの配列を`[channel, height, width]`となる様に配列操作を行います．\n",
        "\n",
        "最後に，画像データと対応する教師ラベルを取得します．今回は鼻の座標と性別のデータのみ必要であるため，対象のデータを抽出してnumpyの配列にします．また，座標のデータは画像サイズで割り，0~1に正規化しておきます．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX6VxcQxKft0"
      },
      "source": [
        "class MTFLdataset(torch.utils.data.Dataset):\n",
        "    base_folder = 'mtfl'\n",
        "    url = \"http://mmlab.ie.cuhk.edu.hk/projects/TCDCN/data/MTFL.zip\"\n",
        "    filename = \"MTFL.zip\"\n",
        "\n",
        "    def __init__(self, root, train=True, download=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.download = download\n",
        "\n",
        "        # MTFLデータのダウンロード\n",
        "        if download and not(os.path.exists(os.path.join(self.root, self.filename))):\n",
        "            print(\"Downloading dataset...\")\n",
        "            urllib.request.urlretrieve(self.url, os.path.join(self.root, self.filename))\n",
        "            with zipfile.ZipFile(os.path.join(self.root, self.filename), 'r') as zipf:\n",
        "                zipf.extractall(path=os.path.join(self.root, self.base_folder))\n",
        "\n",
        "        # 学習，評価データの判定\n",
        "        if self.train:\n",
        "            self.data_list = pandas.read_csv(os.path.join(self.root, self.base_folder, \"training.txt\"), sep=\" \", header=None, skipinitialspace=True, skipfooter=1, engine=\"python\", \n",
        "                                          names=[\"#image path\", \"#x1\",\"#x2\",\"#x3\",\"#x4\",\"#x5\",\"#y1\",\"#y2\",\"#y3\"\n",
        "                                                    ,\"#y4\",\"#y5\",\"#gender\",\" #smile\", \"#wearing glasses\", \"#head pose\"])\n",
        "        else:\n",
        "            self.data_list = pandas.read_csv(os.path.join(self.root, self.base_folder, \"testing.txt\"), sep=\" \", header=None, skipinitialspace=True, skipfooter=1, engine=\"python\", \n",
        "                                          names=[\"#image path\", \"#x1\",\"#x2\",\"#x3\",\"#x4\",\"#x5\",\"#y1\",\"#y2\",\"#y3\"\n",
        "                                                    ,\"#y4\",\"#y5\",\"#gender\",\" #smile\", \"#wearing glasses\", \"#head pose\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img = cv2.imread(os.path.join(self.root, self.base_folder, self.data_list.iloc[item, 0]).replace('\\\\', '/'))\n",
        "        original_height, original_width, _ = img.shape\n",
        "        img = cv2.resize(img, (40, 40))\n",
        "        #cv2_imshow(img)\n",
        "\n",
        "        # データの正規化（0~255）\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "        # 画像の配列を入れ替え\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        # 教師ラベルの読み込み・正規化\n",
        "        target_csv = self.data_list.iloc[item]\n",
        "        #target = np.asarray([target_csv[\"#x3\"], target_csv[\"#y3\"], target_csv[\"#gender\"]], dtype=np.float32)\n",
        "        #target[0] = target[0] / float(original_width)\n",
        "        #target[1] = target[1] / float(original_height)\n",
        "        target1 = np.asarray([target_csv[\"#x3\"], target_csv[\"#y3\"]], dtype=np.float32)\n",
        "        target1[0] = target1[0] / float(original_width)\n",
        "        target1[1] = target1[1] / float(original_height)\n",
        "        target2 = np.asarray(target_csv[\"#gender\"], dtype=np.long) - 1\n",
        "\n",
        "        return img, target1, target2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqK2a2Qxn10I"
      },
      "source": [
        "上で定義したデータセットクラスを用いて，MTFLデータセットを読み込みます．\n",
        "\n",
        "また，読み込んだデータセットクラスの情報を表示します． まず，各データセットが保有しているサンプル数を表示します． データセットクラスに`len()`を適用すると，上で定義した`__len__()`メソッドが呼ばれ，サンプル数を返します．\n",
        "\n",
        "次に，`train_data`のとある1サンプルを読み込みます． `train_data[10]`とすることで，上で定義した`__getitem__()`メソッドが呼ばれ，引数の`item`に`10`が与えられ，10番目のサンプルを返します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtkII3hjbiLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e7f307-3d6a-4741-fcf7-9930cbc33d73"
      },
      "source": [
        "train_data = MTFLdataset(root=\"./\", train=True, download=True)\n",
        "test_data = MTFLdataset(root=\"./\", train=False, download=True)\n",
        "\n",
        "# サンプル数の表示\n",
        "print(\"# Training set:\", len(train_data))\n",
        "print(\"# Test set:\", len(test_data))\n",
        "\n",
        "# とあるサンプルの読み込み\n",
        "img, label1, label2 = train_data[10]\n",
        "print(img)\n",
        "print(label1)\n",
        "print(label2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "# Training set: 10000\n",
            "# Test set: 2995\n",
            "[[[0.6117647  0.61960787 0.63529414 ... 0.39607844 0.39215687 0.39215687]\n",
            "  [0.60784316 0.6156863  0.6313726  ... 0.39607844 0.38039216 0.38039216]\n",
            "  [0.6156863  0.62352943 0.6392157  ... 0.39607844 0.3882353  0.3882353 ]\n",
            "  ...\n",
            "  [0.16862746 0.05098039 0.08235294 ... 0.25490198 0.27058825 0.22352941]\n",
            "  [0.0627451  0.03921569 0.05098039 ... 0.27450982 0.2627451  0.23529412]\n",
            "  [0.27058825 0.09411765 0.04705882 ... 0.25882354 0.22352941 0.23137255]]\n",
            "\n",
            " [[0.61960787 0.6313726  0.64705884 ... 0.4117647  0.40784314 0.40784314]\n",
            "  [0.61960787 0.627451   0.6431373  ... 0.4117647  0.39607844 0.39607844]\n",
            "  [0.62352943 0.63529414 0.6509804  ... 0.4117647  0.40392157 0.40392157]\n",
            "  ...\n",
            "  [0.13725491 0.03529412 0.07450981 ... 0.1882353  0.20392157 0.14901961]\n",
            "  [0.06666667 0.04313726 0.05882353 ... 0.2        0.19607843 0.16470589]\n",
            "  [0.20392157 0.05098039 0.02352941 ... 0.19215687 0.15294118 0.16470589]]\n",
            "\n",
            " [[0.6117647  0.62352943 0.6392157  ... 0.39215687 0.3882353  0.3882353 ]\n",
            "  [0.6117647  0.61960787 0.63529414 ... 0.39215687 0.3764706  0.3764706 ]\n",
            "  [0.61960787 0.627451   0.6431373  ... 0.39215687 0.38431373 0.38431373]\n",
            "  ...\n",
            "  [0.13725491 0.03137255 0.07058824 ... 0.1764706  0.19215687 0.13725491]\n",
            "  [0.05882353 0.03529412 0.05098039 ... 0.1882353  0.18431373 0.15294118]\n",
            "  [0.20784314 0.05098039 0.01960784 ... 0.18039216 0.14509805 0.15294118]]]\n",
            "[0.511 0.533]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIZfJ4frt1yt"
      },
      "source": [
        "# ネットワークモデルの定義\n",
        "今回用いる畳み込みニューラルネットワーク (CNN) を定義します．CNNは，[3]で用いられているものを参考に，畳み込み層4層，全結合層3層で構成します．\n",
        "\n",
        "1層目の畳み込み層は入力チャンネル数が3，出力する特徴マップ数が16，畳み込むフィルタサイズが5x5です．2層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が48，畳み込むフィルタサイズは3x3です．同様に，3, 4層目の畳み込み層も準備します．また，各畳み込み層にはプーリングを適用しますが，プーリングは重みを持たず毎回同じ処理となるため，1つだけ定義します．1つ目の全結合層は入力ユニット数は2x2x64とし，出力は100としています．次の全結合層は入力が100，出力が24，最後の全結合層，つまり出力層は入力が24，出力が4です．出力ユニットは，0-1番目が鼻の座標 (x, y) ，2-3番目が性別 (male, female) に対応します．これらの各層の構成を`__init__`関数で定義します．\n",
        "\n",
        "次に，`forward`関数では，定義した層を接続して処理するように記述します．`forward`関数の引数xは入力データです．それを`__init__`関数で定義した`conv1`に与え，その出力を`pool`に与えて，畳み込みとプーリングの処理を行います．プーリング処理結果はhとして出力し，`conv2`に与えられて次の層の処理を行います．これを繰り返して，すべての層が順番に処理されるように接続します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mht-H0q9uY-m"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(16, 48, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(48, 64, kernel_size=3)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(2 * 2 * 64,  100)\n",
        "        self.fc2 = nn.Linear(100, 24)\n",
        "        self.fc3 = nn.Linear(24, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.pool(self.conv1(x))\n",
        "        h = self.pool(self.conv2(h))\n",
        "        h = self.pool(self.conv3(h))\n",
        "        h = self.conv4(h)\n",
        "        h = h.view(h.size()[0], -1)\n",
        "        h = self.fc1(h)\n",
        "        h = self.fc2(h)\n",
        "        h = self.fc3(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2SMh4fY7q3z"
      },
      "source": [
        "# ネットワークの作成\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "\n",
        "CNNクラスを呼び出して，ネットワークモデルを定義します． また，GPUを使う場合 (`use_cuda == True`) には，ネットワークモデルをGPUメモリ上に配置します． これにより，GPUを用いた演算が可能となります．\n",
        "\n",
        "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します． また，学習率 (`lr`) を0.01，モーメンタム (`momentum`) を0.9として引数に与えます．\n",
        "\n",
        "最後に，定義したネットワークの詳細情報を`torchsummary.summary()`関数を用いて表示します．畳み込みと全結合層には`Param #`にいくつかの値が存在しますが，これが重みパラメタの数となります．マックスプーリングは単に特徴マップのサイズを削減するだけなので，パラメタは存在しません．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oQFsizkwBR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cb9c8f-523f-46da-c631-5eb3cc71ece0"
      },
      "source": [
        "model = CNN()\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 40, 40))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 36, 36]           1,216\n",
            "         MaxPool2d-2           [-1, 16, 18, 18]               0\n",
            "            Conv2d-3           [-1, 48, 16, 16]           6,960\n",
            "         MaxPool2d-4             [-1, 48, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 6, 6]          27,712\n",
            "         MaxPool2d-6             [-1, 64, 3, 3]               0\n",
            "            Conv2d-7             [-1, 64, 2, 2]          16,448\n",
            "            Linear-8                  [-1, 100]          25,700\n",
            "            Linear-9                   [-1, 24]           2,424\n",
            "           Linear-10                    [-1, 4]             100\n",
            "================================================================\n",
            "Total params: 80,560\n",
            "Trainable params: 80,560\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 0.34\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.67\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpieuKiB8ZPI"
      },
      "source": [
        "# 学習\n",
        "読み込んだMTFLデータセットと作成したネットワークを用いて，学習を行います．\n",
        "\n",
        "1回の誤差を算出するデータ数（ミニバッチサイズ）を64，学習エポック数を10とします．\n",
        "\n",
        "次にデータローダーを定義します．データローダーでは，上で読み込んだデータセット（`train_data`）を用いて，`for`文で指定したミニバッチサイズでデータを読み込むオブジェクトを作成します．この時，`shuffle=True`と設定することで，読み込むデータを毎回ランダムに指定します．\n",
        "\n",
        "次に，誤差関数を設定します．今回はマルチタスク学習であるため，2つの誤差関数を定義します．回帰タスクを解くための二乗誤差を計算するための`MSELoss`を`criterion1`として，分類タスクを解くためのクロスエントロピー誤差を計算するための`CrossEntropyLoss`を`criterion2`として定義します．\n",
        "\n",
        "そして，学習を開始します．誤差を二乗誤差とクロスエントロピー誤差と合計したものをそれぞれ表示するためにカウンターを初期化しておきます．\n",
        "\n",
        "各更新において，学習用データと教師データをそれぞれ`image`と`label1`, `label2`とします．学習モデルに`image`をネットワークの出力`y`を取得します．`y`の中身は0-1番目が鼻の座標 (x, y) ，2-3番目が性別 (male, female) の確率を示しています．これらを対応する各教師データ (`label1`, `label2`) と対応する各誤差関数 (`criterion1`, `criterion2`) に入力して，誤差を計算します．誤差の逆伝播は1つの誤差値でしか行えないため，2つの誤差を単純に足します．はじめに述べたように，このとき誤差に重み値 (`lambda_1`, `lambda_2`)をかけます．今回はどちらも1として，重みを与えないようにします．足して1つにした誤差を`backward`関数で逆伝播し，ネットワークの更新を行います．認識精度も同時に計算して，`print`関数で学習経過における誤差や認識精度を表示します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqxrO4Ctwx1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798ac5d4-1122-41bb-8d68-3c5f80f8a770"
      },
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 64\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "lambda_1 = 1.\n",
        "lambda_2 = 1.\n",
        "if use_cuda:\n",
        "    criterion1.cuda()\n",
        "    criterion2.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_reg_loss = 0.0\n",
        "    sum_cls_loss = 0.0\n",
        "    sum_all_loss = 0.0\n",
        "    cls_count = 0\n",
        "    reg_count = 0\n",
        "    num_data = 0\n",
        "    \n",
        "    for image, label1, label2 in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label1 = label1.cuda()\n",
        "            label2 = label2.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "\n",
        "        reg_loss = criterion1(y[:, :2], label1[:, :2])\n",
        "        cls_loss = criterion2(y[:, 2:], label2)\n",
        "        all_loss = lambda_1 * reg_loss + lambda_2 * cls_loss \n",
        "\n",
        "        model.zero_grad()\n",
        "        all_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        sum_reg_loss += reg_loss.item()\n",
        "        sum_cls_loss += cls_loss.item()\n",
        "        sum_all_loss += all_loss.item()\n",
        "        norm = torch.sqrt(torch.pow(y[:, 0] - label1[:, 0], 2) + torch.pow(y[:, 1] - label1[:, 1], 2))\n",
        "        reg_count += torch.sum(norm <= 0.1)\n",
        "        cls_pred = torch.argmax(y[:, 2:], dim=1)\n",
        "        cls_count += torch.sum(cls_pred == label2)\n",
        "        num_data += len(image)\n",
        "        \n",
        "    print(\"epoch: {}, mean loss: {}, mean loss(reg): {}, mean loss(cls): {}, mean accuracy(reg): {}, mean accuracy(cls): {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_all_loss / n_iter,\n",
        "                                                                                 sum_reg_loss / n_iter,\n",
        "                                                                                 sum_cls_loss / n_iter,\n",
        "                                                                                 reg_count.item() / float(num_data),\n",
        "                                                                                 cls_count.item() / float(num_data),\n",
        "                                                                                 time() - start))\n",
        "print(\"Done.\")\n",
        "print(\"mean accuracy(reg): {} ({}/{})\".format(reg_count.item() / float(num_data), reg_count.item(), num_data))\n",
        "print(\"mean accuracy(cls): {} ({}/{})\".format(cls_count.item() / float(num_data), cls_count.item(), num_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, mean loss: 0.6915680828094483, mean loss(reg): 0.008234763135015965, mean loss(cls): 0.6833333187103271, mean accuracy(reg): 0.6467, mean accuracy(cls): 0.5807, elapsed_time :15.013142585754395\n",
            "epoch: 2, mean loss: 0.6847536476135254, mean loss(reg): 0.0035068423613905907, mean loss(cls): 0.6812468070983887, mean accuracy(reg): 0.7379, mean accuracy(cls): 0.5807, elapsed_time :29.599194288253784\n",
            "epoch: 3, mean loss: 0.6740172958374023, mean loss(reg): 0.0035903671249747277, mean loss(cls): 0.6704269275665283, mean accuracy(reg): 0.7422, mean accuracy(cls): 0.5928, elapsed_time :44.36145043373108\n",
            "epoch: 4, mean loss: 0.6276863939285279, mean loss(reg): 0.0029930346116423608, mean loss(cls): 0.6246933580398559, mean accuracy(reg): 0.7905, mean accuracy(cls): 0.6519, elapsed_time :59.15302515029907\n",
            "epoch: 5, mean loss: 0.5452123704910279, mean loss(reg): 0.0023453194729983807, mean loss(cls): 0.542867052078247, mean accuracy(reg): 0.8655, mean accuracy(cls): 0.7274, elapsed_time :73.81962561607361\n",
            "epoch: 6, mean loss: 0.5004093328475953, mean loss(reg): 0.0020318745613098145, mean loss(cls): 0.4983774583816528, mean accuracy(reg): 0.9, mean accuracy(cls): 0.7552, elapsed_time :88.45127820968628\n",
            "epoch: 7, mean loss: 0.453954443359375, mean loss(reg): 0.0019888098917901515, mean loss(cls): 0.45196563167572024, mean accuracy(reg): 0.9069, mean accuracy(cls): 0.7914, elapsed_time :103.09829521179199\n",
            "epoch: 8, mean loss: 0.40374208574295045, mean loss(reg): 0.001942459625005722, mean loss(cls): 0.4017996269226074, mean accuracy(reg): 0.9094, mean accuracy(cls): 0.8163, elapsed_time :117.69937539100647\n",
            "epoch: 9, mean loss: 0.358224555683136, mean loss(reg): 0.0018582462601363658, mean loss(cls): 0.3563663088798523, mean accuracy(reg): 0.9164, mean accuracy(cls): 0.8417, elapsed_time :132.3053867816925\n",
            "epoch: 10, mean loss: 0.31981089487075803, mean loss(reg): 0.0018189822152256967, mean loss(cls): 0.3179919125556946, mean accuracy(reg): 0.9233, mean accuracy(cls): 0.8608, elapsed_time :146.8651430606842\n",
            "Done.\n",
            "mean accuracy(reg): 0.9233 (9233/10000)\n",
            "mean accuracy(cls): 0.8608 (8608/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFHjwCjPAdL1"
      },
      "source": [
        "# テスト\n",
        "学習したネットワークモデルを用いて評価（テスト）を行います．テストは100枚ずつ行うため，`batch_size`は100とします．データをシャッフルする必要はないため，`shuffle=False`とします．学習時と同様に，`for`文で指定したミニバッチサイズでデータを読み込むオブジェクトを作成します．\n",
        "\n",
        "すべての画像（2,995枚）でテストが終わったら，最終的な精度を表示します．回帰タスクが75〜80%，分類タスクが60〜70%程度の性能になるはずです．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYCMRlgfPCTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1510db3c-cbec-4ece-f51d-2c30f66fadcd"
      },
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "cls_count = 0\n",
        "reg_count = 0\n",
        "num_data = 0\n",
        "with torch.no_grad():\n",
        "    for image, label1, label2 in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label1 = label1.cuda()\n",
        "            label2 = label2.cuda()\n",
        "            \n",
        "        y = model(image)\n",
        "\n",
        "        norm = torch.sqrt(torch.pow(y[:, 0] - label1[:, 0], 2) + torch.pow(y[:, 1] - label1[:, 1], 2))\n",
        "        reg_count += torch.sum(norm <= 0.1)\n",
        "        cls_pred = torch.argmax(y[:, 2:], dim=1)\n",
        "        cls_count += torch.sum(cls_pred == label2)\n",
        "        num_data += len(image)\n",
        "\n",
        "print(\"test accuracy(reg): {}({}/{})\".format(reg_count.item() / float(num_data), reg_count.item(), num_data))\n",
        "print(\"test accuracy(cls): {}({}/{})\".format(cls_count.item() / float(num_data), cls_count.item(), num_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy(reg): 0.7549248747913189(2261/2995)\n",
            "test accuracy(cls): 0.66110183639399(1980/2995)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaLksT-IAg4T"
      },
      "source": [
        "# マルチタスクにしない場合との比較\n",
        "マルチタスク学習を行わず，各タスクをそれぞれ別のネットワークで学習した場合と比較します．ネットワークは最終層のユニットを2つにして，`single_CNN`とします．そして，ネットワークとオプティマイザを個別に定義し，以降すべての学習処理を2つの異なるネットワークで同時に進めます．学習が終了したら，そのまま評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAeURbY_WzvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c6ad81-64b6-4d08-de94-50ad53583205"
      },
      "source": [
        "class single_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(16, 48, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(48, 64, kernel_size=3)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(2 * 2 * 64,  100)\n",
        "        self.fc2 = nn.Linear(100, 24)\n",
        "        self.fc3 = nn.Linear(24, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.pool(self.conv1(x))\n",
        "        h = self.pool(self.conv2(h))\n",
        "        h = self.pool(self.conv3(h))\n",
        "        h = self.conv4(h)\n",
        "        h = h.view(h.size()[0], -1)\n",
        "        h = self.fc1(h)\n",
        "        h = self.fc2(h)\n",
        "        h = self.fc3(h)\n",
        "        return h\n",
        "\n",
        "model_reg = single_CNN()\n",
        "model_cls = single_CNN()\n",
        "if use_cuda:\n",
        "    model_reg.cuda()\n",
        "    model_cls.cuda()\n",
        "\n",
        "optimizer_reg = torch.optim.SGD(model_reg.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_cls = torch.optim.SGD(model_cls.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "## TRAIN\n",
        "\n",
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 64\n",
        "epoch_num = 10\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion_reg = nn.MSELoss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion_reg.cuda()\n",
        "    criterion_cls.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model_reg.train()\n",
        "model_cls.train()\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_reg_loss = 0.0\n",
        "    sum_cls_loss = 0.0\n",
        "    cls_count = 0\n",
        "    reg_count = 0\n",
        "    num_data = 0\n",
        "    \n",
        "    for image, label1, label2 in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label1 = label1.cuda()\n",
        "            label2 = label2.cuda()\n",
        "\n",
        "        y_reg = model_reg(image)\n",
        "        y_cls = model_cls(image)\n",
        "\n",
        "        reg_loss = criterion_reg(y_reg, label1)\n",
        "        cls_loss = criterion_cls(y_cls, label2)\n",
        "\n",
        "        model_reg.zero_grad()\n",
        "        model_cls.zero_grad()\n",
        "        reg_loss.backward()\n",
        "        cls_loss.backward()\n",
        "        optimizer_reg.step()\n",
        "        optimizer_cls.step()\n",
        "        \n",
        "        sum_reg_loss += reg_loss.item()\n",
        "        sum_cls_loss += cls_loss.item()\n",
        "        norm = torch.sqrt(torch.pow(y_reg[:, 0] - label1[:, 0], 2) + torch.pow(y_reg[:, 1] - label1[:, 1], 2))\n",
        "        reg_count += torch.sum(norm <= 0.1)\n",
        "        cls_pred = torch.argmax(y_cls, dim=1)\n",
        "        cls_count += torch.sum(cls_pred == label2)\n",
        "        num_data += len(image)\n",
        "        \n",
        "    print(\"epoch: {}, mean loss(reg): {}, mean loss(cls): {}, mean accuracy(reg): {}, mean accuracy(cls): {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_reg_loss / n_iter,\n",
        "                                                                                 sum_cls_loss / n_iter,\n",
        "                                                                                 reg_count.item() / float(num_data),\n",
        "                                                                                 cls_count.item() / float(num_data),\n",
        "                                                                                 time() - start))\n",
        "print(\"Done.\")\n",
        "print(\"mean accuracy(reg): {} ({}/{})\".format(reg_count.item() / float(num_data), reg_count.item(), num_data))\n",
        "print(\"mean accuracy(cls): {} ({}/{})\".format(cls_count.item() / float(num_data), cls_count.item(), num_data))\n",
        "\n",
        "\n",
        "## TEST\n",
        "\n",
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model_reg.eval()\n",
        "model_cls.eval()\n",
        "\n",
        "# 評価の実行\n",
        "cls_count = 0\n",
        "reg_count = 0\n",
        "num_data = 0\n",
        "with torch.no_grad():\n",
        "    for image, label1, label2 in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label1 = label1.cuda()\n",
        "            label2 = label2.cuda()\n",
        "            \n",
        "        y_reg = model_reg(image)\n",
        "        y_cls = model_cls(image)\n",
        "\n",
        "        norm = torch.sqrt(torch.pow(y_reg[:, 0] - label1[:, 0], 2) + torch.pow(y_reg[:, 1] - label1[:, 1], 2))\n",
        "        reg_count += torch.sum(norm <= 0.1)\n",
        "        cls_pred = torch.argmax(y_cls, dim=1)\n",
        "        cls_count += torch.sum(cls_pred == label2)\n",
        "        num_data += len(image)\n",
        "\n",
        "print(\"test accuracy(reg): {}({}/{})\".format(reg_count.item() / float(num_data), reg_count.item(), num_data))\n",
        "print(\"test accuracy(cls): {}({}/{})\".format(cls_count.item() / float(num_data), cls_count.item(), num_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, mean loss(reg): 0.014211838310956956, mean loss(cls): 0.6854059036254883, mean accuracy(reg): 0.6256, mean accuracy(cls): 0.5717, elapsed_time :15.328052997589111\n",
            "epoch: 2, mean loss(reg): 0.003456924858689308, mean loss(cls): 0.6822478080749512, mean accuracy(reg): 0.7416, mean accuracy(cls): 0.5807, elapsed_time :30.6910502910614\n",
            "epoch: 3, mean loss(reg): 0.0034310488805174826, mean loss(cls): 0.6771477939605713, mean accuracy(reg): 0.7431, mean accuracy(cls): 0.5813, elapsed_time :45.93756127357483\n",
            "epoch: 4, mean loss(reg): 0.003406249889731407, mean loss(cls): 0.6354801582336426, mean accuracy(reg): 0.745, mean accuracy(cls): 0.6417, elapsed_time :61.151575803756714\n",
            "epoch: 5, mean loss(reg): 0.00337755616158247, mean loss(cls): 0.5532261325836182, mean accuracy(reg): 0.7461, mean accuracy(cls): 0.7159, elapsed_time :76.2879695892334\n",
            "epoch: 6, mean loss(reg): 0.0033528373032808306, mean loss(cls): 0.512042798614502, mean accuracy(reg): 0.7493, mean accuracy(cls): 0.7492, elapsed_time :91.49462604522705\n",
            "epoch: 7, mean loss(reg): 0.003320234875380993, mean loss(cls): 0.4632656532287598, mean accuracy(reg): 0.7505, mean accuracy(cls): 0.7826, elapsed_time :106.83730578422546\n",
            "epoch: 8, mean loss(reg): 0.0032819809034466746, mean loss(cls): 0.4143001749038696, mean accuracy(reg): 0.7564, mean accuracy(cls): 0.8114, elapsed_time :121.96818232536316\n",
            "epoch: 9, mean loss(reg): 0.0032467488944530488, mean loss(cls): 0.37206010417938234, mean accuracy(reg): 0.7598, mean accuracy(cls): 0.8352, elapsed_time :137.1325581073761\n",
            "epoch: 10, mean loss(reg): 0.003191791853308678, mean loss(cls): 0.3285276418685913, mean accuracy(reg): 0.7643, mean accuracy(cls): 0.8548, elapsed_time :152.33425998687744\n",
            "Done.\n",
            "mean accuracy(reg): 0.7643 (7643/10000)\n",
            "mean accuracy(cls): 0.8548 (8548/10000)\n",
            "test accuracy(reg): 0.7342237061769616(2199/2995)\n",
            "test accuracy(cls): 0.657762938230384(1970/2995)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhpWq5GfAoP-"
      },
      "source": [
        "# 課題\n",
        "1. 鼻の位置・性別以外でも学習・テストしてみよう．\n",
        "2. 誤差に重みをかけて，精度が上がるのか確かめてみよう．\n",
        "\n",
        "## ヒント\n",
        "1. `MTFLdataset`の，以下の部分を変更します．\n",
        "```\n",
        "        target1 = np.asarray([target_csv[\"#x3\"], target_csv[\"#y3\"]], dtype=np.float32)\n",
        "        target2 = np.asarray(target_csv[\"#gender\"], dtype=np.long) - 1\n",
        "```\n",
        "\n",
        "2. 通常，「誤差が早く収束するタスク」「誤差の値が小さいタスク」の誤差に少数（`0.001`など）の重みをかけて調整します．ただし，例外も多いため，実験的に見つける事がほとんどです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srD6wAvv55N7"
      },
      "source": [
        "# 参考文献\n",
        "- [1] H. Fukui, T. Yamashita, Y. Yamauchi, H. Fujiyoshi, and H. Murase, \"Training of CNN with Heterogeneous Learning for Multiple Pedestrian Attributes Recognition Using Rarity Rate\", IEICE TRANSACTIONS on Information and Systems, Vol.E101-D, No.5, pp.1222-1231, 2018.\n",
        "- [2] S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\". Advances in Neural Information Processing Systems, pp. 91-99, 2015.\n",
        "- [3] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, \"Facial Landmark Detection by Deep Multi-task Learning\", in Proceedings of European Conference on Computer Vision, 2014."
      ]
    }
  ]
}